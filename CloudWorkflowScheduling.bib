% This file was created with JabRef 2.10b2.
% Encoding: UTF8


@InProceedings{Catalyuerek2011,
  Title                    = {Integrated Data Placement and Task Assignment for Scientific Workflows in Clouds},
  Author                   = {\c{C}ataly\"{u}rek, \"{U}mit V. and Kaya, Kamer and U\c{c}ar, Bora},
  Booktitle                = {Proceedings of the Fourth International Workshop on Data-intensive Distributed Computing},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Pages                    = {45--54},
  Publisher                = {ACM},
  Series                   = {DIDC '11},

  Acmid                    = {1996022},
  Doi                      = {10.1145/1996014.1996022},
  File                     = {:PDF/Catalyuerek2011.pdf:PDF},
  ISBN                     = {978-1-4503-0704-8},
  Keywords                 = {cloud computing, data placement, hypergraph partitioning, scientific workflows, task assignment},
  Location                 = {San Jose, California, USA},
  Numpages                 = {10}
}

@Article{Abrishami2012,
  Title                    = {Deadline-constrained workflow scheduling in software as a service Cloud },
  Author                   = {S. Abrishami and M. Naghibzadeh},
  Journal                  = {Scientia Iranica },
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {680--689},
  Volume                   = {19},

  Abstract                 = {The advent of Cloud computing as a new model of service provisioning in distributed systems, encourages researchers to investigate its benefits and drawbacks in executing scientific applications such as workflows. In this model, the users request for available services according to their desired Quality of Service, and they are charged on a pay-per-use basis. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS of the user as well as minimizing the cost of workflow execution. In this paper, we propose a new QoS-based workflow scheduling algorithm based on a novel concept called Partial Critical Paths (PCP), which tries to minimize the cost of workflow execution while meeting a user-defined deadline. This algorithm recursively schedules the partial critical paths ending at previously scheduled tasks. The simulation results show that the performance of our algorithm is very promising.},
  Doi                      = {10.1016/j.scient.2011.11.047},
  File                     = {:PDF/Abrishami2012.pdf:PDF},
  ISSN                     = {1026-3098},
  Keywords                 = {Cloud computing},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1026309812000351}
}

@Article{Abrishami2013,
  Title                    = {Deadline-constrained workflow scheduling algorithms for Infrastructure as a Service Clouds },
  Author                   = {Saeid Abrishami and Mahmoud Naghibzadeh and Dick H.J. Epema},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2013},
  Note                     = {Including Special section: AIRCC-NetCoM 2009 and Special section: Clouds and Service-Oriented Architectures },
  Number                   = {1},
  Pages                    = {158--169},
  Volume                   = {29},

  Abstract                 = {The advent of Cloud computing as a new model of service provisioning in distributed systems encourages researchers to investigate its benefits and drawbacks on executing scientific applications such as workflows. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS requirements of the user as well as minimizing the cost of workflow execution. We have previously designed and analyzed a two-phase scheduling algorithm for utility Grids, called Partial Critical Paths (PCP), which aims to minimize the cost of workflow execution while meeting a user-defined deadline. However, we believe Clouds are different from utility Grids in three ways: on-demand resource provisioning, homogeneous networks, and the pay-as-you-go pricing model. In this paper, we adapt the \{PCP\} algorithm for the Cloud environment and propose two workflow scheduling algorithms: a one-phase algorithm which is called IaaS Cloud Partial Critical Paths (IC-PCP), and a two-phase algorithm which is called IaaS Cloud Partial Critical Paths with Deadline Distribution (IC-PCPD2). Both algorithms have a polynomial time complexity which make them suitable options for scheduling large workflows. The simulation results show that both algorithms have a promising performance, with IC-PCP performing better than IC-PCPD2 in most cases. },
  Doi                      = {10.1016/j.future.2012.05.004},
  File                     = {:PDF/Abrishami2013.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Cloud computing;IaaS Clouds;Grid computing;Workflow scheduling;QoS-based scheduling },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X12001008}
}

@Electronic{AWS2014,
  Title                    = {Amazon EC2 Pricing},
  Author                   = {Amazon Web Services, Inc},
  Url                      = {http://goo.gl/yKb41s},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/yKb41s}
}

@Article{Arabnejad2014,
  Title                    = {A Budget Constrained Scheduling Algorithm for Workflow Applications},
  Author                   = {Arabnejad, Hamid and Barbosa, JorgeG.},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {665--679},
  Volume                   = {12},

  Doi                      = {10.1007/s10723-014-9294-7},
  File                     = {:PDF/Arabnejad2014.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Utility computing; Deadline; Quality of Service; Planning Success Rate},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@Article{Armbrust2010,
  Title                    = {A View of Cloud Computing},
  Author                   = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
  Journal                  = {Commun. ACM},
  Year                     = {2010},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {50--58},
  Volume                   = {53},

  Acmid                    = {1721672},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1721654.1721672},
  File                     = {:PDF/Armbrust2010.pdf:PDF},
  ISSN                     = {0001-0782},
  Issue_date               = {April 2010},
  Numpages                 = {9},
  Publisher                = {ACM}
}

@InProceedings{Barrett2011,
  Title                    = {A Learning Architecture for Scheduling Workflow Applications in the Cloud},
  Author                   = {Barrett, E. and Howley, E. and Duggan, J.},
  Booktitle                = {Web Services (ECOWS), 2011 Ninth IEEE European Conference on},
  Year                     = {2011},
  Month                    = sep,
  Pages                    = {83--90},

  Abstract                 = {The scheduling of workflow applications involves the mapping of individual workflow tasks to computational resources, based on a range of functional and non-functional quality of service requirements. Workflow applications such as scientific workflows often require extensive computational processing and generate significant amounts of experimental data. The emergence of cloud computing has introduced a utility-type market model, where computational resources of varying capacities can be procured on demand, in a pay-per-use fashion. In workflow based applications dependencies exist amongst tasks which requires the generation of schedules in accordance with defined precedence constraints. These constraints pose a difficult planning problem, where tasks must be scheduled for execution only once all their parent tasks have completed. In general the two most important objectives of workflow schedulers are the minimisation of both cost and make span. The cost of workflow execution consists of both computational costs incurred from processing individual tasks, and data transmission costs. With scientific workflows potentially large amounts of data must be transferred between compute and storage sites. This paper proposes a novel cloud workflow scheduling approach which employs a Markov Decision Process to optimally guide the workflow execution process depending on environmental state. In addition the system employs a genetic algorithm to evolve workflow schedules. The overall architecture is presented, and initial results indicate the potential of this approach for developing viable workflow schedules on the Cloud.},
  Doi                      = {10.1109/ECOWS.2011.27},
  File                     = {:PDF/Barrett2011.pdf:PDF},
  Keywords                 = {Markov processes;cloud computing;genetic algorithms;minimisation;quality of service;scheduling;software architecture;storage management;workflow management software;Markov decision process;cloud computing;cloud workflow scheduling;computational costs;computational processing;computational resources;data transmission costs;environmental state;genetic algorithm;learning architecture;minimisation;pay-per-use fashion;precedence constraints;quality of service requirements;schedules;scheduling workflow applications;scientific workflows;storage sites;utility-type market model;workflow based applications dependency;workflow execution process;workflow schedulers;workflow tasks;Biological cells;Genetic algorithms;Markov processes;Optimal scheduling;Processor scheduling;Schedules;Scheduling;Bayesian Model Learning;Genetic Algorithm;Markov Decision Process;Workflow Scheduling}
}

@Article{Benoit2013,
  Title                    = {A Survey of Pipelined Workflow Scheduling: Models and Algorithms},
  Author                   = {Benoit, Anne and \c{C}ataly\"{u}rek, \"{U}mit V. and Robert, Yves and Saule, Erik},
  Journal                  = {ACM Comput. Surv.},
  Year                     = {2013},

  Month                    = aug,
  Number                   = {4},
  Pages                    = {50:1--50:36},
  Volume                   = {45},

  Acmid                    = {2501664},
  Address                  = {New York, NY, USA},
  Articleno                = {50},
  Doi                      = {10.1145/2501654.2501664},
  File                     = {:PDF/Benoit2013.pdf:PDF},
  ISSN                     = {0360-0300},
  Issue_date               = {August 2013},
  Keywords                 = {Workflow programming, algorithms, distributed systems, filter-stream programming, latency, models, parallel systems, pipeline, scheduling, throughput},
  Numpages                 = {36},
  Publisher                = {ACM}
}

@InProceedings{Bessai2012,
  Title                    = {Bi-criteria Workflow Tasks Allocation and Scheduling in Cloud Computing Environments},
  Author                   = {Bessai, K. and Youcef, S. and Oulamara, A. and Godart, Claude and Nurcan, S.},
  Booktitle                = {Cloud Computing (CLOUD), 2012 IEEE 5\textsuperscript{th} International Conference on},
  Year                     = {2012},
  Month                    = jun,
  Pages                    = {638--645},

  Abstract                 = {Although there are few efficient algorithms in the literature for scientific workflow tasks allocation and scheduling for heterogeneous resources such as those proposed in grid computing context, they usually require a bounded number of computer resources that cannot be applied in Cloud computing environment. Indeed, unlike grid, elastic computing, such asAmazon's EC2, allows users to allocate and release compute resources on-demand and pay only for what they use. Therefore, it is reasonable to assume that the number of resources is infinite. This feature of Clouds has been called âillusion of infiniteresourcesâ. However, despite the proven benefits of using Cloud to run scientific workflows, users lack guidance for choosing between multiple offering while taking into account several objectives which are often conflicting. On the other side, the workflow tasks allocation and scheduling have been shown to be NP-complete problems. Thus, it is convenient to use heuristic rather than deterministic algorithm. The objective of this paper is to design an allocation strategy for Cloud computing platform. More precisely, we propose three complementary bi-criteria approaches for scheduling workflows on distributed Cloud resources, taking into account the overall execution time and the cost incurred by using a set of resources.},
  Doi                      = {10.1109/CLOUD.2012.83},
  File                     = {:PDF/Bessai2012.pdf:PDF},
  ISSN                     = {2159-6182},
  Keywords                 = {cloud computing;grid computing;optimisation;scheduling;workflow management software;NP-complete problems;allocation strategy;bi-criteria workflow tasks allocation;cloud computing environments;distributed Cloud resources;elastic computing;grid computing context;heterogeneous resources;workflow task scheduling;Cloud computing;Computational modeling;Processor scheduling;Resource management;Schedules;Scheduling;Virtual machining}
}

@InProceedings{Bharambe2013,
  Title                    = {Pareto optimization for multiobjective matching of geospatial ontologies},
  Author                   = {Bharambe, U. and Durbha, S.S. and Kurte, K. and Younan, N.H. and King, R.L.},
  Booktitle                = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {1159-1162},

  Abstract                 = {Geospatial information is different than conventional information. Harmonization is needed for interoperability and seamless access to data. Ontology matching is an emerging solution to achieve this harmonization. The input data of the Geospatial ontologies vary from the conventional ontologies and hence it is conceptualized in a different manner. There are two major obstacles for geoinformation fusion: heterogeneity and uncertainty. Heterogeneity is more prevalent and uncertainty is an unavoidable entity in geospatial domain. This paper explores a novel multi-objective algorithm for geospatial ontology matching. It uses Pareto ranking to sort the probable solution and derives the pareto front. This pareto front is used further to find the best match.},
  Doi                      = {10.1109/IGARSS.2013.6721371},
  File                     = {:PDF/Bharambe2013.pdf:PDF},
  ISSN                     = {2153-6996},
  Keywords                 = {Pareto optimisation;geophysics computing;ontologies (artificial intelligence);open systems;pattern matching;sensor fusion;Pareto front;Pareto optimization;Pareto ranking;geoinformation fusion;geospatial domain;geospatial information;geospatial ontology matching;harmonization;input data;interoperability;multiobjective algorithm;Atmospheric waves;Geospatial analysis;Interoperability;Ontologies;Pareto optimization;Semantics;Interoperability;Ontology Matching;Pareto Front;Pareto Ranking}
}

@InProceedings{Bharathi2008,
  Title                    = {Characterization of scientific workflows},
  Author                   = {Bharathi, S. and Chervenak, A. and Deelman, E. and Mehta, G. and Mei-Hui Su and Vahi, K.},
  Booktitle                = {Workflows in Support of Large-Scale Science, 2008. WORKS 2008. Third Workshop on},
  Year                     = {2008},
  Month                    = nov,
  Pages                    = {1--10},

  Abstract                 = {Researchers working on the planning, scheduling and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. We describe basic workflow structures that are composed into complex workflows by scientific communities. We provide a characterization of workflows from five diverse scientific applications, describing their composition and data and computational requirements. We also describe the effect of the size of the input datasets on the structure and execution profiles of these workflows. Finally, we describe a workflow generator that produces synthetic, parameterizable workflows that closely resemble the workflows that we characterize. We make these workflows available to the community to be used as benchmarks for evaluating various workflow systems and scheduling algorithms.},
  Doi                      = {10.1109/WORKS.2008.4723958},
  File                     = {:PDF/Bharathi2008.pdf:PDF},
  Keywords                 = {data analysis;data structures;natural sciences computing;scheduling;workflow management software;computational requirement;data requirement;scientific workflow characterization;scientific workflow planning;scientific workflow scheduling;workflow execution profile;Astronomy;Biology;Character generation;Earthquakes;Geometry;Libraries;Performance analysis;Physics;Scheduling algorithm;Workflow management software}
}

@Article{Birman2009,
  Title                    = {Toward a Cloud Computing Research Agenda},
  Author                   = {Birman, Ken and Chockler, Gregory and van Renesse, Robbert},
  Journal                  = {SIGACT News},
  Year                     = {2009},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {68--80},
  Volume                   = {40},

  Acmid                    = {1556172},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1556154.1556172},
  File                     = {:PDF/Birman2009.pdf:PDF},
  ISSN                     = {0163-5700},
  Issue_date               = {June 2009},
  Numpages                 = {13},
  Publisher                = {ACM}
}

@Article{Bittencourt2011,
  Title                    = {HCOC: a cost optimization algorithm for workflow scheduling in hybrid clouds},
  Author                   = {Bittencourt, LuizFernando and Madeira, EdmundoRobertoMauro},
  Journal                  = {Journal of Internet Services and Applications},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {207--227},
  Volume                   = {2},

  Doi                      = {10.1007/s13174-011-0032-0},
  File                     = {:PDF/Bittencourt2011.pdf:PDF},
  ISSN                     = {1867-4828},
  Keywords                 = {Workflow; Scheduling; DAG; Cloud computing},
  Language                 = {English},
  Publisher                = {Springer-Verlag}
}

@InProceedings{Bittencourt2012,
  Title                    = {Impact of communication uncertainties on workflow scheduling in hybrid clouds},
  Author                   = {Bittencourt, L.F. and Madeira, E.R.M. and da Fonseca, N.L.S.},
  Booktitle                = {Global Communications Conference (GLOBECOM), 2012 IEEE},
  Year                     = {2012},
  Month                    = dec,
  Pages                    = {1623--1628},

  Abstract                 = {The so-called hybrid cloud is the composition of an infrastructure that comprises private resources as well as public resources leased from public clouds. Hybrid clouds can be utilized for the execution of applications composed of dependent jobs, usually modeled as workflows. In this scenario, a scheduler must distribute the components of the workflow onto available resources considering the communication demands and the available bandwidth in network links. However, such information can be imprecise, and consequently decisions on resource allocation can be ineffective. In this paper, we evaluate scheduling algorithms in the face of imprecise information on the availability of communication channels. Results showed that schedules are negatively affected by the unforeseen variations in bandwidth during the execution of the application.},
  Doi                      = {10.1109/GLOCOM.2012.6503346},
  File                     = {:PDF/Bittencourt2012.pdf:PDF},
  ISSN                     = {1930-529X},
  Keywords                 = {cloud computing;resource allocation;scheduling;telecommunication channels;workflow management software;communication channels;communication uncertainties;hybrid clouds;network links;private resources;public clouds;public resources;resource allocation;scheduling algorithms;workflow scheduling}
}

@InCollection{Cai2013,
  Title                    = {Critical Path-Based Iterative Heuristic for Workflow Scheduling in Utility and Cloud Computing},
  Author                   = {Cai, Zhicheng and Li, Xiaoping and Gupta, JatinderN.D.},
  Booktitle                = {Service-Oriented Computing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2013},
  Editor                   = {Basu, Samik and Pautasso, Cesare and Zhang, Liang and Fu, Xiang},
  Pages                    = {207--221},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8274},

  Doi                      = {10.1007/978-3-642-45005-1_15},
  File                     = {:PDF/Cai2013.pdf:PDF},
  ISBN                     = {978-3-642-45004-4},
  Keywords                 = {Cloud computing; workflow scheduling; utility computing; critical path; dynamic programming; multi-stage decision process},
  Language                 = {English}
}

@InProceedings{Cao2013,
  Title                    = {Energy-Aware Workflow Job Scheduling for Green Clouds},
  Author                   = {Fei Cao and Zhu, M.M.},
  Booktitle                = {Green Computing and Communications (GreenCom), 2013 IEEE and Internet of Things (iThings/CPSCom), IEEE International Conference on and IEEE Cyber, Physical and Social Computing},
  Year                     = {2013},
  Month                    = aug,
  Pages                    = {232--239},

  Abstract                 = {With the increasing deployment of many data centers and computer servers around the globe, the energy cost on running the computing, communication and cooling together with the amount of CO2 emissions have increased dramatically. In order to maintain sustainable Cloud computing with ever-increasing problem scale, we design and develop energy-aware scientific workflow scheduling algorithm to minimize energy consumption and CO2 emission without sacrificing Quality of Service (QoS) such as response time specified in Service Level Agreement (SLA). The underlying available computing capacity and network bandwidth is represented as time-dependent because of the dual operation modes of on-demand and reservation instances supported by many commercial Cloud data centers. The Dynamic Voltage and Frequency Scaling (DVFS) is utilized to lower the CPU frequencies of virtual machines as long as the finishing time is still before the specified deadline. Our resource provision and allocation algorithm aims to meet the response time requirement and minimize the Virtual Machine (VM) overhead for reduced energy consumption. The consolidated VM reuse can lead to higher resource utilization rate for higher system throughput. The effectiveness of our algorithm is evaluated under various performance metrics and experimental scenarios using software adapted from open source CloudSim simulator. The simulation results show that our algorithm is able to achieve an average up to 30% of energy savings.},
  Doi                      = {10.1109/GreenCom-iThings-CPSCom.2013.58},
  File                     = {:PDF/Cao2013.pdf:PDF},
  Keywords                 = {air pollution;cloud computing;computer centres;contracts;energy consumption;green computing;performance evaluation;power aware computing;public domain software;quality of service;resource allocation;scheduling;sustainable development;virtual machines;workflow management software;CPU frequency;DVFS;QoS;SLA;VM overhead;allocation algorithm;carbon dioxide emissions;cloud data centers;computer servers;computing capacity;consolidated VM reuse;deployment;dual operation modes;dynamic voltage and frequency scaling;energy consumption;energy cost;energy-aware workflow job scheduling;finishing time;green clouds;higher system throughput;network bandwidth;open source CloudSim simulator;performance metric evaluation;quality of service;reservation instances;resource provision;resource utilization rate;response time requirement;service level agreement;sustainable cloud computing;virtual machine overhead;virtual machines;Clouds;Computational modeling;Cooling;Energy consumption;Green products;Resource management;Servers}
}

@Article{Chen2009,
  Title                    = {An Ant Colony Optimization Approach to a Grid Workflow Scheduling Problem With Various QoS Requirements},
  Author                   = {Wei-Neng Chen and Jun Zhang},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2009},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {29--43},
  Volume                   = {39},

  Abstract                 = {Grid computing is increasingly considered as a promising next-generation computational platform that supports wide-area parallel and distributed computing. In grid environments, applications are always regarded as workflows. The problem of scheduling workflows in terms of certain quality of service (QoS) requirements is challenging and it significantly influences the performance of grids. By now, there have been some algorithms for grid workflow scheduling, but most of them can only tackle the problems with a single QoS parameter or with small-scale workflows. In this frame, this paper aims at proposing an ant colony optimization (ACO) algorithm to schedule large-scale workflows with various QoS parameters. This algorithm enables users to specify their QoS preferences as well as define the minimum QoS thresholds for a certain application. The objective of this algorithm is to find a solution that meets all QoS constraints and optimizes the user-preferred QoS parameter. Based on the characteristics of workflow scheduling, we design seven new heuristics for the ACO approach and propose an adaptive scheme that allows artificial ants to select heuristics based on pheromone values. Experiments are done in ten workflow applications with at most 120 tasks, and the results demonstrate the effectiveness of the proposed algorithm.},
  Doi                      = {10.1109/TSMCC.2008.2001722},
  File                     = {:PDF/Chen2009.pdf:PDF},
  ISSN                     = {1094-6977},
  Keywords                 = {grid computing;optimisation;quality of service;scheduling;QoS;ant colony optimization;grid computing;grid workflow scheduling problem;quality of service;wide-area distributed computing;wide-area parallel computing;Ant colony optimization (ACO);grid computing;workflow scheduling}
}

@InProceedings{Chopra2013,
  Title                    = {Deadline and cost based workflow scheduling in hybrid cloud},
  Author                   = {Chopra, N. and Singh, S.},
  Booktitle                = {Advances in Computing, Communications and Informatics (ICACCI), 2013 International Conference on},
  Year                     = {2013},
  Month                    = aug,
  Pages                    = {840--846},

  Abstract                 = {Cloud computing provides on demand resources for compute and storage requirements. Private cloud is a good option for cost saving for executing workflow applications but when the resources in private cloud are not enough to meet storage and compute requirements of an application then public clouds are the option left. While public clouds charge users on pay-per-use basis, private clouds are owned by users and can be utilized with no charge. When a public cloud and a private cloud is merged, we get a hybrid cloud. In hybrid cloud, task scheduling is a complex process as jobs can be allocated resources either from private cloud or from public cloud. Deadline based scheduling is the main focus in many of the workflow applications. Proposed algorithm does cost optimization by deciding which resources should be taken on lease from public cloud to complete the workflow execution within deadline. In the proposed work, we have developed a level based scheduling algorithm which executes tasks level wise and it uses the concept of sub-deadline which is helpful in finding best resources on public cloud for cost saving and also completes workflow execution within deadlines. Performance analysis and comparison of the proposed algorithm with min-min approach is also presented.},
  Doi                      = {10.1109/ICACCI.2013.6637285},
  File                     = {:PDF/Chopra2013.pdf:PDF},
  Keywords                 = {cloud computing;performance evaluation;processor scheduling;resource allocation;storage management;cloud computing;complex process;cost based workflow scheduling;deadline based workflow scheduling;hybrid cloud;level based scheduling algorithm;level wise task execution;min-min approach;on demand resources;pay-per-use basis charge;performance analysis;private cloud;resource allocation;storage requirements;subdeadline concept;task scheduling;workflow applications;Cloud computing;Hardware;Organizations;Schedules;Scheduling;Scheduling algorithms;Cloud Computing;DAG;Hybrid cloud;Private cloud;Public cloud;Workflow Scheduling}
}

@Book{Coello2004,
  Title                    = {Applications of multi-objective evolutionary algorithms},
  Author                   = {Coello, Carlos A Coello and Lamont, Gary B},
  Publisher                = {World Scientific},
  Year                     = {2004},
  Volume                   = {1}
}

@Book{Coello2007,
  Title                    = {Evolutionary algorithms for solving multi-objective problems},
  Author                   = {Coello, Carlos Coello and Lamont, Gary B and Van Veldhuizen, David A},
  Publisher                = {Springer Science \& Business Media},
  Year                     = {2007}
}

@Electronic{Computing2013,
  Title                    = {Back to the Future: 1.21 petaFLOPS(RPeak), 156,000-core CycleCloud HPC runs 264 years of Materials Science | Cycle Computing},
  Author                   = {Cycle Computing},
  Month                    = nov,
  Url                      = {http://www.cyclecomputing.com/blog/back-to-the-future-121-petaflopsrpeak-156000-core-cyclecloud-hpc-runs-264-years-of-materials-science/},
  Year                     = {2013},

  Bdsk-url-1               = {http://goo.gl/59ItjU}
}

@Book{Deb2001,
  Title                    = {Multi-objective optimization using evolutionary algorithms},
  Author                   = {Kalyanmoy Deb},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2001},
  Series                   = {Wiley Interscience Series in Systems and Optimization},
  Volume                   = {16}
}

@Article{Deb2002,
  Title                    = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
  Author                   = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {2002},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {182--197},
  Volume                   = {6},

  Abstract                 = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
  Doi                      = {10.1109/4235.996017},
  File                     = {:PDF/Deb2002.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {Pareto distribution;computational complexity;constraint theory;convergence;genetic algorithms;operations research;simulation;sorting;NSGA-II;Nondominated Sorting Genetic Algorithm II;Pareto-archived evolution strategy;Pareto-optimal front;algorithm performance;computational complexity;constrained multi-objective problems;constraint handling;convergence;dominance definition;fast elitist multi-objective genetic algorithm;mating pool;multi-criterion decision making;multi-objective evolutionary algorithm;multi-objective optimization;nondominated sharing;nonlinear problem;objectives;parent/offspring population combination;population size;selection operator;simulation;solution fitness;solution spread;strength-Pareto evolutionary algorithm;Associate members;Computational complexity;Computational modeling;Constraint optimization;Decision making;Diversity reception;Evolutionary computation;Genetic algorithms;Sorting;Testing}
}

@Article{Deelman2005,
  Title                    = {Pegasus: A framework for mapping complex scientific workflows onto distributed systems},
  Author                   = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G Bruce and Good, John and others},
  Journal                  = {Scientific Programming},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {219--237},
  Volume                   = {13},

  File                     = {:PDF/Deelman2005.pdf:PDF},
  Publisher                = {Hindawi Publishing Corporation}
}

@Article{Delimitrou2013,
  Title                    = {QoS-Aware Scheduling in Heterogeneous Datacenters with Paragon},
  Author                   = {Delimitrou, Christina and Kozyrakis, Christos},
  Journal                  = {ACM Trans. Comput. Syst.},
  Year                     = {2013},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {12:1--12:34},
  Volume                   = {31},

  Acmid                    = {2556583},
  Address                  = {New York, NY, USA},
  Articleno                = {12},
  Doi                      = {10.1145/2556583},
  File                     = {:PDF/Delimitrou2013.pdf:PDF},
  ISSN                     = {0734-2071},
  Issue_date               = {December 2013},
  Keywords                 = {Datacenter, QoS, cloud computing, heterogeneity, interference, resource-efficiency, scheduling},
  Numpages                 = {34},
  Publisher                = {ACM}
}

@InProceedings{Dongarra2007,
  Title                    = {Bi-objective Scheduling Algorithms for Optimizing Makespan and Reliability on Heterogeneous Systems},
  Author                   = {Dongarra, Jack J. and Jeannot, Emmanuel and Saule, Erik and Shi, Zhiao},
  Booktitle                = {Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {280--288},
  Publisher                = {ACM},
  Series                   = {SPAA '07},

  Acmid                    = {1248423},
  Doi                      = {10.1145/1248377.1248423},
  File                     = {:PDF/Dongarra2007.pdf:PDF},
  ISBN                     = {978-1-59593-667-7},
  Keywords                 = {DAG, pareto-curve, reliability, scheduling},
  Location                 = {San Diego, California, USA},
  Numpages                 = {9}
}

@InProceedings{Durillo2013,
  Title                    = {Multi-objective Workflow Scheduling: An Analysis of the Energy Efficiency and Makespan Tradeoff},
  Author                   = {Durillo, J.J. and Nae, V. and Prodan, R.},
  Booktitle                = {Cluster, Cloud and Grid Computing (CCGrid), 2013 13\textsuperscript{th} IEEE/ACM International Symposium on},
  Year                     = {2013},
  Month                    = may,
  Pages                    = {203--210},

  Abstract                 = {While in the past scheduling algorithms were almost exclusively targeted at optimizing applications' make span, today they must simultaneously optimise several goals. Among these goals, energy efficiency is receiving increasing attention for environmental and financial reasons. In contrast to related work that optimises energy consumption as a single objective function, we reformulate in this paper the problem as a bi-objective optimisation by considering both make span and energy as goals. We study the potential benefits of using a Pareto-based workflow scheduling algorithm called MOHEFT using realistic energy consumption and performance models for task executions. We analyse the tradeoff solutions computed by MOHET for different workflows (different in shapes and sizes) in different execution scenarios (different resources in terms of energy consumption). The obtained results show that our bi-objective approach found in some cases schedules that reduce the energy consumption up to 85% with only 3.3% of make span concessions.},
  Doi                      = {10.1109/CCGrid.2013.62},
  File                     = {:PDF/Durillo2013.pdf:PDF},
  Keywords                 = {Pareto optimisation;environmental factors;natural sciences computing;power aware computing;scheduling;MOHEFT;Pareto-based workflow scheduling algorithm;application makespan optimization;biobjective optimisation;energy efficiency;environmental reasons;financial reasons;makespan concessions;makespan tradeoff;multiobjective workflow scheduling;Computational modeling;Energy consumption;Energy measurement;Multicore processing;Optimization;Schedules;Shape}
}

@Article{Durillo2014,
  Title                    = {Multi-objective workflow scheduling in Amazon EC2},
  Author                   = {Durillo, JuanJ. and Prodan, Radu},
  Journal                  = {Cluster Computing},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {169--189},
  Volume                   = {17},

  Doi                      = {10.1007/s10586-013-0325-0},
  File                     = {:PDF/Durillo2014.pdf:PDF},
  ISSN                     = {1386-7857},
  Keywords                 = {Workflow scheduling; Cloud; Multi-objective optimisation; List-based heuristics},
  Language                 = {English},
  Publisher                = {Springer US}
}

@Article{Durillo2014a,
  Title                    = {Multi-objective energy-efficient workflow scheduling using list-based heuristics },
  Author                   = {Juan J. Durillo and Vlad Nae and Radu Prodan},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Note                     = {Special Section: Intelligent Big Data Processing Special Section: Behavior Data Security Issues in Network Information Propagation Special Section: Energy-efficiency in Large Distributed Computing Architectures Special Section: eScience Infrastructure and Applications },
  Number                   = {0},
  Pages                    = {221--236},
  Volume                   = {36},

  Abstract                 = {Abstract Workflow applications are a popular paradigm used by scientists for modelling applications to be run on heterogeneous high-performance parallel and distributed computing systems. Today, the increase in the number and heterogeneity of multi-core parallel systems facilitates the access to high-performance computing to almost every scientist, yet entailing additional challenges to be addressed. One of the critical problems today is the power required for operating these systems for both environmental and financial reasons. To decrease the energy consumption in heterogeneous systems, different methods such as energy-efficient scheduling are receiving increasing attention. Current schedulers are, however, based on simplistic energy models not matching the reality, use techniques like \{DVFS\} not available on all types of systems, or do not approach the problem as a multi-objective optimisation considering both performance and energy as simultaneous objectives. In this paper, we present a new Pareto-based multi-objective workflow scheduling algorithm as an extension to an existing state-of-the-art heuristic capable of computing a set of tradeoff optimal solutions in terms of makespan and energy efficiency. Our approach is based on empirical models which capture the real behaviour of energy consumption in heterogeneous parallel systems. We compare our new approach with a classical mono-objective scheduling heuristic and state-of-the-art multi-objective optimisation algorithm and demonstrate that it computes better or similar results in different scenarios. We analyse the different tradeoff solutions computed by our algorithm under different experimental configurations and we observe that in some cases it finds solutions which reduce the energy consumption by up to 34.5% with a slight increase of 2% in the makespan. },
  Doi                      = {10.1016/j.future.2013.07.005},
  File                     = {:PDF/Durillo2014a.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Energy-efficient scheduling;Workflow scheduling;Multi-objective optimisation},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X13001507}
}

@InProceedings{Fahringer2005,
  Title                    = {ASKALON: A Grid Application Development and Computing Environment},
  Author                   = {Fahringer, T. and Prodan, R. and Duan, Rubing and Nerieri, F. and Podlipnig, S. and Qin, Jun and Siddiqui, M. and Truong, Hong-Linh and Villazon, A. and Wieczorek, M.},
  Booktitle                = {Proceedings of the 6\textsuperscript{th} IEEE/ACM International Workshop on Grid Computing},
  Year                     = {2005},

  Address                  = {Washington, DC, USA},
  Pages                    = {122--131},
  Publisher                = {IEEE Computer Society},
  Series                   = {GRID '05},

  Acmid                    = {1253487},
  Doi                      = {10.1109/GRID.2005.1542733},
  File                     = {:PDF/Fahringer2005.pdf:PDF},
  ISBN                     = {0-7803-9492-5},
  Numpages                 = {10}
}

@Article{Falco2014,
  Title                    = {Two new fast heuristics for mapping parallel applications on cloud computing },
  Author                   = {I. De Falco and U. Scafuri and E. Tarantino},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Note                     = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications },
  Number                   = {0},
  Pages                    = {1--13},
  Volume                   = {37},

  Abstract                 = {Abstract In this paper two new heuristics, named Min–min-C and Max–min-C, are proposed able to provide near-optimal solutions to the mapping of parallel applications, modeled as Task Interaction Graphs, on computational clouds. The aim of these heuristics is to determine mapping solutions which allow exploiting at best the available cloud resources to execute such applications concurrently with the other cloud services. Differently from their originating Min–min and Max–min models, the two introduced heuristics take also communications into account. Their effectiveness is assessed on a set of artificial mapping problems differing in applications and in node working conditions. The analysis, carried out also by means of statistical tests, reveals the robustness of the two algorithms proposed in coping with the mapping of small- and medium-sized high performance computing applications on non-dedicated cloud nodes. },
  Doi                      = {10.1016/j.future.2014.02.019},
  File                     = {:PDF/Falco2014.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Cloud computing;Mapping;Communicating tasks;Heuristics },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X14000442}
}

@InProceedings{Fard2013a,
  Title                    = {Budget-Constrained Resource Provisioning for Scientific Applications in Clouds},
  Author                   = {Fard, H.M. and Fahringer, T. and Prodan, R.},
  Booktitle                = {Cloud Computing Technology and Science (CloudCom), 2013 IEEE 5\textsuperscript{th} International Conference on},
  Year                     = {2013},
  Month                    = dec,
  Pages                    = {315--322},
  Volume                   = {1},

  Abstract                 = {Public commercial clouds emerged as new and attractive resource provisioning option for scientific computing. This new alternative raises new challenges for users of such clouds, since optimizing the completion time of scientific applications might substantially increase the monetary cost of leasing cloud resources. In this paper, we first propose a set of basic rescheduling operations covering a broad set of scenarios for reducing the costs of running scientific workflows in clouds. Based on them, we design two heuristic scheduling algorithms. The first algorithm aims at reducing the cost of resource provisioning while still attaining the optimal make span. The second algorithm further reduces the costs to meet a budget constraint with a small increase in the make span. The experiments conducted using real-world and synthetic workflow applications demonstrate important benefits compared to related state-of-the-art approaches.},
  Doi                      = {10.1109/CloudCom.2013.48},
  File                     = {:PDF/Fard2013a.pdf:PDF},
  Keywords                 = {budgeting;cloud computing;optimisation;resource allocation;scheduling;budget constraint;budget-constrained resource provisioning;completion time optimization;heuristic scheduling algorithms;monetary cost;optimal makespan;public commercial cloud resources;real-world workflow applications;rescheduling operations;resource provisioning cost reduction;scientific computing;scientific workflows;synthetic workflow applications;Algorithm design and analysis;Booting;Cloud computing;Delays;Optimization;Schedules;Scheduling;cloud computing;makespan;monetary cost;resource provisioning;scheduling;scientific workflows}
}

@Article{Fard2013,
  Title                    = {A Truthful Dynamic Workflow Scheduling Mechanism for Commercial Multicloud Environments},
  Author                   = {Fard, H.M. and Prodan, R. and Fahringer, T.},
  Journal                  = {Parallel and Distributed Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {1203--1212},
  Volume                   = {24},

  Abstract                 = {The ultimate goal of cloud providers by providing resources is increasing their revenues. This goal leads to a selfish behavior that negatively affects the users of a commercial multicloud environment. In this paper, we introduce a pricing model and a truthful mechanism for scheduling single tasks considering two objectives: monetary cost and completion time. With respect to the social cost of the mechanism, i.e., minimizing the completion time and monetary cost, we extend the mechanism for dynamic scheduling of scientific workflows. We theoretically analyze the truthfulness and the efficiency of the mechanism and present extensive experimental results showing significant impact of the selfish behavior of the cloud providers on the efficiency of the whole system. The experiments conducted using real-world and synthetic workflow applications demonstrate that our solutions dominate in most cases the Pareto-optimal solutions estimated by two classical multiobjective evolutionary algorithms.},
  Doi                      = {10.1109/TPDS.2012.257},
  File                     = {:PDF/Fard2013.pdf:PDF},
  ISSN                     = {1045-9219},
  Keywords                 = {Pareto optimisation;cloud computing;evolutionary computation;natural sciences computing;pricing;scheduling;workflow management software;Pareto-optimal solutions;classical multiobjective evolutionary algorithms;commercial multicloud environments;completion time minimization;monetary cost;pricing model;real-world workflow applications;scientific workflows;selfish behavior;single task scheduling;social cost;synthetic workflow applications;truthful dynamic workflow scheduling mechanism;Dynamic scheduling;Game theory;Games;Heuristic algorithms;Optimization;Processor scheduling;Workflow scheduling;game theory;multicloud environment;reverse auction;truthful mechanism}
}

@InProceedings{Fard2012,
  Title                    = {A Multi-objective Approach for Workflow Scheduling in Heterogeneous Environments},
  Author                   = {Fard, Hamid Mohammadi and Prodan, Radu and Barrionuevo, Juan Jose Durillo and Fahringer, Thomas},
  Booktitle                = {Proceedings of the 2012 12\textsuperscript{th} IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
  Year                     = {2012},

  Address                  = {Washington, DC, USA},
  Pages                    = {300--309},
  Publisher                = {IEEE Computer Society},
  Series                   = {CCGRID '12},

  Acmid                    = {2310188},
  Doi                      = {10.1109/CCGrid.2012.114},
  File                     = {:PDF/Fard2012.pdf:PDF},
  ISBN                     = {978-0-7695-4691-9},
  Keywords                 = {computing systems, workflow scheduling, multi-objective optimization, Grids and Clouds},
  Numpages                 = {10}
}

@Article{Fard2014,
  Title                    = {Multi-objective list scheduling of workflow applications in distributed computing infrastructures },
  Author                   = {Hamid Mohammadi Fard and Radu Prodan and Thomas Fahringer},
  Journal                  = {Journal of Parallel and Distributed Computing },
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {2152--2165},
  Volume                   = {74},

  Abstract                 = {Abstract Executing large-scale applications in distributed computing infrastructures (DCI), for example modern Cloud environments, involves optimization of several conflicting objectives such as makespan, reliability, energy, or economic cost. Despite this trend, scheduling in heterogeneous \{DCIs\} has been traditionally approached as a single or bi-criteria optimization problem. In this paper, we propose a generic multi-objective optimization framework supported by a list scheduling heuristic for scientific workflows in heterogeneous DCIs. The algorithm approximates the optimal solution by considering user-specified constraints on objectives in a dual strategy: maximizing the distance to the user’s constraints for dominant solutions and minimizing it otherwise. We instantiate the framework and algorithm for a four-objective case study comprising makespan, economic cost, energy consumption, and reliability as optimization goals. We implemented our method as part of the \{ASKALON\} environment (Fahringer et al., 2007) for Grid and Cloud computing and demonstrate through extensive real and synthetic simulation experiments that our algorithm outperforms related bi-criteria heuristics while meeting the user constraints most of the time. },
  Doi                      = {10.1016/j.jpdc.2013.12.004},
  File                     = {:PDF/Fard2014.pdf:PDF},
  ISSN                     = {0743-7315},
  Keywords                 = {Multi-objective scheduling;Scientific workflows;Distributed computing infrastructures },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0743731513002384}
}

@Article{Fonseca1995,
  Title                    = {An overview of evolutionary algorithms in multiobjective optimization},
  Author                   = {Fonseca, Carlos M and Fleming, Peter J},
  Journal                  = {Evolutionary computation},
  Year                     = {1995},
  Number                   = {1},
  Pages                    = {1--16},
  Volume                   = {3},

  File                     = {:PDF/Fonseca1995.pdf:PDF},
  Publisher                = {MIT Press}
}

@InProceedings{Foster2008,
  Title                    = {Cloud Computing and Grid Computing 360-Degree Compared},
  Author                   = {Foster, I. and Yong Zhao and Raicu, I. and Shiyong Lu},
  Booktitle                = {Grid Computing Environments Workshop, 2008. GCE '08},
  Year                     = {2008},
  Month                    = nov,
  Pages                    = {1--10},

  Abstract                 = {Cloud computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for cloud computing and there seems to be no consensus on what a cloud is. On the other hand, cloud computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established grid computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast cloud computing with grid computing from various angles and give insights into the essential characteristics of both.},
  Doi                      = {10.1109/GCE.2008.4738445},
  File                     = {:PDF/Foster2008.pdf:PDF},
  Keywords                 = {grid computing;cloud computing;cluster computing;distributed system;grid computing;utility computing;Cloud computing;Computer science;Computer vision;Costs;Distributed computing;Economies of scale;Grid computing;Laboratories;Large-scale systems;Standards organizations}
}

@InProceedings{Frincu2011,
  Title                    = {Multi-objective Meta-heuristics for Scheduling Applications with High Availability Requirements and Cost Constraints in Multi-Cloud Environments},
  Author                   = {Frincu, M.E. and Craciun, C.},
  Booktitle                = {Utility and Cloud Computing (UCC), 2011 Fourth IEEE International Conference on},
  Year                     = {2011},
  Month                    = dec,
  Pages                    = {267--274},

  Abstract                 = {As the popularity of cloud computing increases, more and more applications are migrated onto them. Web 2.0 applications are the most common example of such applications. These applications require to scale, be highly available, fault tolerant and able to run uninterrupted for long periods of time (or even indefinitely). Moreover as new cloud providers appear there is a natural tendency towards choosing the best provider or a combination of them for deploying the application. Thus multi-cloud scenarios emerge from this situation. However, as multi-cloud resource provisioning is both complex and costly, the choice of which resources to lend and how to allocate them to application components needs to rely on efficient strategies. These need to take into account many factors including deployment and run-time cost, resource load, and application availability in case of failures. For this aim multi-objective scheduling algorithms seem an appropriate choice. This paper presents an algorithm which tries to achieve application high-availability and fault-tolerance while reducing the application cost and keeping the resource load maximized. The proposed algorithm is compared with a classic Round Robin strategy - used by many commercial clouds - and the obtained results prove the efficiency of our solution.},
  Doi                      = {10.1109/UCC.2011.43},
  File                     = {:PDF/Frincu2011.pdf:PDF},
  Keywords                 = {cloud computing;fault tolerant computing;processor scheduling;resource allocation;Round Robin strategy;Web 2.0 applications;cloud computing;fault tolerance;high availability requirements;multicloud resource provisioning;multiobjective metaheuristics;multiobjective scheduling algorithms;resource load maximization;Availability;Cloud computing;Databases;Generators;Graphics;Processor scheduling;Schedules;cloud scheduling;meta-heuristics;multi-objective scheduling}
}

@InCollection{Garg2011,
  Title                    = {Multi-objective Workflow Grid Scheduling Based on Discrete Particle Swarm Optimization},
  Author                   = {Garg, Ritu and Singh, AwadheshKumar},
  Booktitle                = {Swarm, Evolutionary, and Memetic Computing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2011},
  Editor                   = {Panigrahi, BijayaKetan and Suganthan, PonnuthuraiNagaratnam and Das, Swagatam and Satapathy, SureshChandra},
  Pages                    = {183--190},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {7076},

  Doi                      = {10.1007/978-3-642-27172-4_23},
  File                     = {:PDF/Garg2011.pdf:PDF},
  ISBN                     = {978-3-642-27171-7},
  Language                 = {English}
}

@Article{Garg2013,
  Title                    = {Multi-objective workflow grid scheduling using $\varepsilon$ -fuzzy dominance sort based discrete particle swarm optimization},
  Author                   = {Garg, Ritu and Singh, Awadhesh Kumar},
  Journal                  = {J Supercomput},
  Year                     = {2013},

  Month                    = nov,
  Number                   = {2},
  Pages                    = {709–732},
  Volume                   = {68},

  Doi                      = {10.1007/s11227-013-1059-8},
  File                     = {:PDF/Garg2013.pdf:PDF},
  ISSN                     = {1573-0484},
  Publisher                = {Springer Science + Business Media}
}

@Article{GhorbanniaDelavar2014,
  Title                    = {HSGA: a hybrid heuristic algorithm for workflow scheduling in cloud systems},
  Author                   = {Ghorbannia Delavar, Arash and Aryan, Yalda},
  Journal                  = {Cluster Computing},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {129--137},
  Volume                   = {17},

  Doi                      = {10.1007/s10586-013-0275-6},
  File                     = {:PDF/GhorbanniaDelavar2014.pdf:PDF},
  ISSN                     = {1386-7857},
  Keywords                 = {Heterogeneous distributed computing systems; Cloud computing; Workflow scheduling; Heuristic; Genetic Algorithm},
  Language                 = {English},
  Publisher                = {Springer US}
}

@Electronic{Google2014,
  Title                    = {Google Compute Engine Pricing},
  Author                   = {Google},
  Url                      = {https://developers.google.com/compute/pricing},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/fKQwzb}
}

@Article{Hirales-Carbajal2012,
  Title                    = {Multiple Workflow Scheduling Strategies with User Run Time Estimates on a Grid},
  Author                   = {Hirales-Carbajal, Adán and Tchernykh, Andrei and Yahyapour, Ramin and González-García, JoséLuis and Röblitz, Thomas and Ramírez-Alcaraz, JuanManuel},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {325--346},
  Volume                   = {10},

  Doi                      = {10.1007/s10723-012-9215-6},
  File                     = {:PDF/Hirales-Carbajal2012.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Grid computing; Workflow scheduling; Resource management; User run time estimate},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@Article{Iosup2011,
  Title                    = {Performance Analysis of Cloud Computing Services for Many-Tasks Scientific Computing},
  Author                   = {Iosup, A. and Ostermann, S. and Yigitbasi, M.N. and Prodan, R. and Fahringer, T. and Epema, D.H.J.},
  Journal                  = {Parallel and Distributed Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {931--945},
  Volume                   = {22},

  Abstract                 = {Cloud computing is an emerging commercial infrastructure paradigm that promises to eliminate the need for maintaining expensive computing facilities by companies and institutes alike. Through the use of virtualization and resource time sharing, clouds serve with a single set of physical resources a large user base with different needs. Thus, clouds have the potential to provide to their owners the benefits of an economy of scale and, at the same time, become an alternative for scientists to clusters, grids, and parallel production environments. However, the current commercial clouds have been built to support web and small database workloads, which are very different from typical scientific computing workloads. Moreover, the use of virtualization and resource time sharing may introduce significant performance penalties for the demanding scientific computing workloads. In this work, we analyze the performance of cloud computing services for scientific computing workloads. We quantify the presence in real scientific computing workloads of Many-Task Computing (MTC) users, that is, of users who employ loosely coupled applications comprising many tasks to achieve their scientific goals. Then, we perform an empirical evaluation of the performance of four commercial cloud computing services including Amazon EC2, which is currently the largest commercial cloud. Last, we compare through trace-based simulation the performance characteristics and cost models of clouds and other scientific computing platforms, for general and MTC-based scientific computing workloads. Our results indicate that the current clouds need an order of magnitude in performance improvement to be useful to the scientific community, and show which improvements should be considered first to address this discrepancy between offer and demand.},
  Doi                      = {10.1109/TPDS.2011.66},
  File                     = {:PDF/Iosup2011.pdf:PDF},
  ISSN                     = {1045-9219},
  Keywords                 = {cloud computing;software performance evaluation;task analysis;Amazon EC2;cloud computing services;clouds cost model;loosely coupled application;many tasks scientific computing;performance analysis;real scientific computing workload;trace based simulation;Cloud computing;Computational modeling;Kernel;Performance evaluation;Production;Supercomputers;Distributed systems;distributed applications;metrics/measurement;performance evaluation;performance measures.}
}

@Article{Ishibuchi2015,
  Title                    = {Behavior of Multiobjective Evolutionary Algorithms on Many-Objective Knapsack Problems},
  Author                   = {Ishibuchi, H. and Akedo, N. and Nojima, Y.},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {2015},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {264--283},
  Volume                   = {19},

  Abstract                 = {We examine the behavior of three classes of evolutionary multiobjective optimization (EMO) algorithms on many-objective knapsack problems. They are Pareto dominance-based, scalarizing function-based, and hypervolume-based algorithms. NSGA-II, MOEA/D, SMS-EMOA, and HypE are examined using knapsack problems with 2-10 objectives. Our test problems are generated by randomly specifying coefficients (i.e., profits) in objectives. We also generate other test problems by combining two objectives to create a dependent or correlated objective. Experimental results on randomly generated many-objective knapsack problems are consistent with well-known performance deterioration of Pareto dominance-based algorithms. That is, NSGA-II is outperformed by the other algorithms. However, it is also shown that NSGA-II outperforms the other algorithms when objectives are highly correlated. MOEA/D shows totally different search behavior depending on the choice of a scalarizing function and its parameter value. Some MOEA/D variants work very well only on two-objective problems while others work well on many-objective problems with 4-10 objectives. We also obtain other interesting observations such as the performance improvement by similar parent recombination and the necessity of diversity improvement for many-objective knapsack problems.},
  Doi                      = {10.1109/TEVC.2014.2315442},
  File                     = {:PDF/Ishibuchi2015.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {Pareto optimisation;genetic algorithms;knapsack problems;EMO algorithms;HypE;MOEA/D;NSGA-II;Pareto dominance-based algorithms;SMS-EMOA;evolutionary multiobjective optimization;hypervolume-based algorithms;many-objective knapsack problems;scalarizing function-based algorithms;Approximation algorithms;Pareto optimization;Search problems;Sociology;Vectors;Evolutionary many-objective optimization;evolutionary multiobjective optimization (EMO);many-objective problems}
}

@InCollection{Ishibuchi2010,
  Title                    = {Many-Objective Test Problems to Visually Examine the Behavior of Multiobjective Evolution in a Decision Space},
  Author                   = {Ishibuchi, Hisao and Hitotsuyanagi, Yasuhiro and Tsukamoto, Noritaka and Nojima, Yusuke},
  Booktitle                = {Parallel Problem Solving from Nature, PPSN XI},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2010},
  Editor                   = {Schaefer, Robert and Cotta, Carlos and Kołodziej, Joanna and Rudolph, Günter},
  Pages                    = {91--100},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {6239},

  Doi                      = {10.1007/978-3-642-15871-1_10},
  File                     = {:PDF/Ishibuchi2010.pdf:PDF},
  ISBN                     = {978-3-642-15870-4},
  Keywords                 = {Evolutionary multiobjective optimization (EMO); many-objective optimization; multiobjective optimization problems; test problems},
  Language                 = {English}
}

@InProceedings{Janetschek2013,
  Title                    = {Bringing Scientific Workflows to Amazon SWF},
  Author                   = {Janetschek, M. and Ostermann, S. and Prodan, R.},
  Booktitle                = {Software Engineering and Advanced Applications (SEAA), 2013 39\textsuperscript{th} EUROMICRO Conference on},
  Year                     = {2013},
  Month                    = sep,
  Pages                    = {389--396},

  Abstract                 = {In response to the ever-increasing needs of scientific applications for resources, Cloud computing emerged as an alternative on-demand and cost-effective resource provisioning approach. In this context, Cloud providers have recognised the importance of workflow applications to science and provide their own native solutions, such as the Amazon Simple Workflow Service (SWF). Nevertheless, an important downside of SWF is its incompatibility with existing workflow systems, and lack of means for reusing scientific legacy code. Similarly, existing workflow middlewares and applications require non-trivial extensions to take advantage of Cloud resources. We present in this paper a software engineering solution that allows the scientific workflow community access the Amazon Cloud through one single front-end converter, and propose a legacy wrapper service for executing legacy code using SWF. Empirical results using a real-world scientific workflow demonstrate that our automatically generated SWF application performs almost as fast as a native manually-optimised version, and outperforms other workflow middleware systems using the Amazon Cloud.},
  Doi                      = {10.1109/SEAA.2013.13},
  File                     = {:PDF/Janetschek2013.pdf:PDF},
  Keywords                 = {cloud computing;middleware;natural sciences computing;program compilers;resource allocation;software maintenance;software reusability;workflow management software;Amazon SWF;Amazon cloud;Amazon simple workflow service;automatic SWF application generation;cloud computing;cloud providers;cloud resources;cost-effective resource provisioning;front-end converter;legacy code execution;legacy wrapper service;scientific applications;scientific legacy code reusability;scientific workflow community;scientific workflows;software engineering solution;workflow applications;workflow middlewares;workflow systems;Cloud computing;History;Java;Ports (Computers);Semantics;Amazon SWF;cloud computing;legacy code;scientific workflows;workflow converter}
}

@InProceedings{Ji2013,
  Title                    = {Profit-Oriented Scheduling Optimization for Workflow in Clouds},
  Author                   = {Haoran Ji and Weidong Bao and Xiaomin Zhu and Shu Yin},
  Booktitle                = {High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing (HPCC_EUC), 2013 IEEE 10\textsuperscript{th} International Conference on},
  Year                     = {2013},
  Month                    = nov,
  Pages                    = {1922--1929},

  Abstract                 = {Clouds have become a new paradigm by enabling on-demand provisioning of applications, platforms or computing resources for clients. Workflow scheduling is one of the most challenging problems in Clouds. Getting more profits is one of the most important objectives in workflow scheduling. Conventional workflow scheduling strategies developed on the kind of systems mainly focus on the workflow. In this paper, we take the communication into account and develop a novel scheduling algorithm based on the topology characters of degree and path length named Music Chair Algorithm (MCA). The algorithm gives a good performance on searching for the optimum schedule in getting the most profit. Also, we find that there exists a certain resource amount, which gets the most profit to help us get more enthusiasm for further developing the Clouds. Experimental results demonstrate that the analysis of the strategies for most profits are reasonable, and the MCA is available to efficiently get the optimum schedule with low computing complexity.},
  Doi                      = {10.1109/HPCC.and.EUC.2013.276},
  File                     = {:PDF/Ji2013.pdf:PDF},
  Keywords                 = {cloud computing;computational complexity;optimisation;scheduling;search problems;MCA;cloud computing;computing complexity;degree topology characters;music chair algorithm;on-demand application provisioning;on-demand computing resource provisioning;on-demand platform provisioning;optimum schedule search;path length;profit-oriented workflow scheduling optimization;Computational modeling;Optimal scheduling;Processor scheduling;Schedules;Scheduling;Topology;DAG;topologic characteristics;workflow scheduling}
}

@Article{Jiao2015,
  Title                    = {Immune optimization of task scheduling on multidimensional QoS constraints},
  Author                   = {Jiao, Hejun and Zhang, Jing and Li, JunHuai and Shi, Jinfa and Li, Jian},
  Journal                  = {Cluster Computing},
  Year                     = {2015},
  Pages                    = {1--10},

  Doi                      = {10.1007/s10586-015-0447-7},
  File                     = {:PDF/Jiao2015.pdf:PDF},
  ISSN                     = {1386-7857},
  Keywords                 = {Cloud computing; Multiple QoS parameter constraint; Immune optimization; Application preference; Task scheduling},
  Language                 = {English},
  Publisher                = {Springer US}
}

@InProceedings{Jrad2013,
  Title                    = {A Broker-based Framework for Multi-cloud Workflows},
  Author                   = {Jrad, Foued and Tao, Jie and Streit, Achim},
  Booktitle                = {Proceedings of the 2013 International Workshop on Multi-cloud Applications and Federated Clouds},
  Year                     = {2013},

  Address                  = {New York, NY, USA},
  Pages                    = {61--68},
  Publisher                = {ACM},
  Series                   = {MultiCloud '13},

  Acmid                    = {2462339},
  Doi                      = {10.1145/2462326.2462339},
  File                     = {:PDF/Jrad2013.pdf:PDF},
  ISBN                     = {978-1-4503-2050-4},
  Keywords                 = {cloud broker, cloud computing, cloud workflow, intercloud computing, multi-cloud},
  Location                 = {Prague, Czech Republic},
  Numpages                 = {8}
}

@InProceedings{Jung2013,
  Title                    = {Optimal Time-Cost Tradeoff of Parallel Service Workflow in Federated Heterogeneous Clouds},
  Author                   = {Gueyoung Jung and Hyunjoo Kim},
  Booktitle                = {Web Services (ICWS), 2013 IEEE 20\textsuperscript{th} International Conference on},
  Year                     = {2013},
  Month                    = jun,
  Pages                    = {499--506},

  Abstract                 = {Federated cloud enables a workflow to be deployed in multiple private and public clouds. By facilitating external cloud-based services to execute sub-tasks of the workflow, service workflow owners can reduce the cost of executing the workflow, while meeting a performance requirement, since those cloud-based services can be more cost efficient and have better performance than internal ones. However, due to inter-dependencies between sub-tasks, the complexity of the workflow, and the heterogeneity of clouds, it is a challenge to achieve the optimal tradeoff between cost and performance. This paper presents a novel workflow scheduler designed to achieve the optimal end-to-end execution time and cost when deploying such complex workflows in heterogeneous computing nodes in clouds. Specifically, our scheduling algorithm addresses the tradeoff between the execution cost, the computing time, and the data transfer delay between sub-tasks. Our scheduler can handle complex workflows that contain recursively paralleled sub-flows caused by branch and merging sub-tasks. Experiments indicate that our scheduler can efficiently compute the near optimal deployment compared with greedy and evolutionary algorithms for both end-to-end execution time and corresponding cost.},
  Doi                      = {10.1109/ICWS.2013.73},
  File                     = {:PDF/Jung2013.pdf:PDF},
  Keywords                 = {cloud computing;scheduling;clouds heterogeneity;data transfer;end-to-end execution time;external cloud-based services;federated heterogeneous clouds;heterogeneous computing nodes;optimal time-cost tradeoff;parallel service workflow;private clouds;public clouds;sub-tasks interdependencies;workflow complexity;workflow scheduler;Abstracts;Cloud computing;Data transfer;Delays;Equations;Heuristic algorithms;Merging;cloud;optimization;parallel workflow;tradeoff}
}

@Article{Juve2013a,
  Title                    = {Characterizing and profiling scientific workflows },
  Author                   = {Gideon Juve and Ann Chervenak and Ewa Deelman and Shishir Bharathi and Gaurang Mehta and Karan Vahi},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2013},
  Note                     = {Special Section: Recent Developments in High Performance Computing and Security },
  Number                   = {3},
  Pages                    = {682--692},
  Volume                   = {29},

  Abstract                 = {Researchers working on the planning, scheduling, and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. This paper provides a characterization of workflows from six diverse scientific applications, including astronomy, bioinformatics, earthquake science, and gravitational-wave physics. The characterization is based on novel workflow profiling tools that provide detailed information about the various computational tasks that are present in the workflow. This information includes I/O, memory and computational characteristics. Although the workflows are diverse, there is evidence that each workflow has a job type that consumes the most amount of runtime. The study also uncovered inefficiency in a workflow component implementation, where the component was re-reading the same data multiple times. },
  Doi                      = {10.1016/j.future.2012.08.015},
  File                     = {:PDF/Juve2013a.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Scientific workflows;Profiling;Performance;Measurement },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X12001732}
}

@InCollection{Juve2011,
  Title                    = {Scientific Workflows in the Cloud},
  Author                   = {Juve, Gideon and Deelman, Ewa},
  Booktitle                = {Grids, Clouds and Virtualization},
  Publisher                = {Springer London},
  Year                     = {2011},
  Editor                   = {Cafaro, Massimo and Aloisio, Giovanni},
  Pages                    = {71--91},
  Series                   = {Computer Communications and Networks},

  Doi                      = {10.1007/978-0-85729-049-6_4},
  File                     = {:PDF/Juve2011.pdf:PDF},
  ISBN                     = {978-0-85729-048-9},
  Language                 = {English}
}

@Article{Juve2013,
  Title                    = {Comparing FutureGrid, Amazon EC2, and Open Science Grid for Scientific Workflows},
  Author                   = {Juve, Gideon and Rynge, Mats and Deelman, Ewa and Vockler, Jens-S. and Berriman, G. Bruce},
  Journal                  = {Computing in Science \& Engineering},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {20--29},
  Volume                   = {15},

  Doi                      = {10.1109/MCSE.2013.44},
  File                     = {:PDF/Juve2013.pdf:PDF}
}

@Article{Khajemohammadi2014,
  Title                    = {Efficient Workflow Scheduling for Grid Computing Using a Leveled Multi-objective Genetic Algorithm},
  Author                   = {Khajemohammadi, Hassan and Fanian, Ali and Gulliver, T.Aaron},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {637--663},
  Volume                   = {12},

  Doi                      = {10.1007/s10723-014-9306-7},
  File                     = {:PDF/Khajemohammadi2014.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Workflow scheduling; Genetic algorithm; Multi-objective optimization; Grid computing},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@InProceedings{Kllapi2011,
  Title                    = {Schedule Optimization for Data Processing Flows on the Cloud},
  Author                   = {Kllapi, Herald and Sitaridi, Eva and Tsangaris, Manolis M. and Ioannidis, Yannis},
  Booktitle                = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Pages                    = {289--300},
  Publisher                = {ACM},
  Series                   = {SIGMOD '11},

  Acmid                    = {1989355},
  Doi                      = {10.1145/1989323.1989355},
  File                     = {:PDF/Kllapi2011.pdf:PDF},
  ISBN                     = {978-1-4503-0661-4},
  Keywords                 = {cloud computing, dataflows, query optimization, scheduling},
  Location                 = {Athens, Greece},
  Numpages                 = {12}
}

@Article{Li2009,
  Title                    = {Multiobjective Optimization Problems With Complicated Pareto Sets, MOEA/D and NSGA-II},
  Author                   = {Hui Li and Qingfu Zhang},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {2009},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {284--302},
  Volume                   = {13},

  Abstract                 = {Partly due to lack of test problems, the impact of the Pareto set (PS) shapes on the performance of evolutionary algorithms has not yet attracted much attention. This paper introduces a general class of continuous multiobjective optimization test instances with arbitrary prescribed PS shapes, which could be used for studying the ability of multiobjective evolutionary algorithms for dealing with complicated PS shapes. It also proposes a new version of MOEA/D based on differential evolution (DE), i.e., MOEA/D-DE, and compares the proposed algorithm with NSGA-II with the same reproduction operators on the test instances introduced in this paper. The experimental results indicate that MOEA/D could significantly outperform NSGA-II on these test instances. It suggests that decomposition based multiobjective evolutionary algorithms are very promising in dealing with complicated PS shapes.},
  Doi                      = {10.1109/TEVC.2008.925798},
  File                     = {:PDF/Li2009.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {Pareto optimisation;evolutionary computation;set theory;MOEA/D;NSGA-II;Pareto sets;differential evolution;evolutionary algorithms;multiobjective optimization problems;Aggregation;Pareto optimality;decomposition;differential evolution;evolutionary algorithms;multiobjective optimization;test problems}
}

@Article{Li2011,
  Title                    = {An energy-efficient scheduling approach based on private clouds},
  Author                   = {Li, Jiandun and Peng, Junjie and Lei, Zhou and Zhang, Wu},
  Journal                  = {Journal of Information \& Computational Science},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {716--724},
  Volume                   = {8},

  File                     = {:PDF/Li2011.pdf:PDF}
}

@Article{Li2014,
  Title                    = {Evolutionary Algorithms With Segment-Based Search for Multiobjective Optimization Problems},
  Author                   = {Miqing Li and Shengxiang Yang and Ke Li and Xiaohui Liu},
  Journal                  = {Cybernetics, IEEE Transactions on},
  Year                     = {2014},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {1295--1313},
  Volume                   = {44},

  Abstract                 = {This paper proposes a variation operator, called segment-based search (SBS), to improve the performance of evolutionary algorithms on continuous multiobjective optimization problems. SBS divides the search space into many small segments according to the evolutionary information feedback from the set of current optimal solutions. Two operations, micro-jumping and macro-jumping, are implemented upon these segments in order to guide an efficient information exchange among “good” individuals. Moreover, the running of SBS is adaptive according to the current evolutionary status. SBS is activated only when the population evolves slowly, depending on general genetic operators (e.g., mutation and crossover). A comprehensive set of 36 test problems is employed for experimental verification. The influence of two algorithm settings (i.e., the dimensionality and boundary relaxation strategy) and two probability parameters in SBS (i.e., the SBS rate and micro-jumping proportion) are investigated in detail. Moreover, an empirical comparative study with three representative variation operators is carried out. Experimental results show that the incorporation of SBS into the optimization process can improve the performance of evolutionary algorithms for multiobjective optimization problems.},
  Doi                      = {10.1109/TCYB.2013.2282503},
  File                     = {:PDF/Li2014.pdf:PDF},
  ISSN                     = {2168-2267},
  Keywords                 = {evolutionary computation;mathematical operators;optimisation;search problems;SBS;evolutionary algorithms;evolutionary information feedback;genetic operators;multiobjective optimization problems;representative variation operators;segment-based search;Convergence;Genetics;Optimization;Scattering;Search problems;Sociology;Statistics;Hybrid evolutionary algorithms;multiobjective optimization;segment-based search;variation operators}
}

@Article{Li2014b,
  Title                    = {Shift-Based Density Estimation for Pareto-Based Algorithms in Many-Objective Optimization},
  Author                   = {Miqing Li and Shengxiang Yang and Xiaohui Liu},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {2014},

  Month                    = jun,
  Number                   = {3},
  Pages                    = {348--365},
  Volume                   = {18},

  Abstract                 = {It is commonly accepted that Pareto-based evolutionary multiobjective optimization (EMO) algorithms encounter difficulties in dealing with many-objective problems. In these algorithms, the ineffectiveness of the Pareto dominance relation for a high-dimensional space leads diversity maintenance mechanisms to play the leading role during the evolutionary process, while the preference of diversity maintenance mechanisms for individuals in sparse regions results in the final solutions distributed widely over the objective space but distant from the desired Pareto front. Intuitively, there are two ways to address this problem: 1) modifying the Pareto dominance relation and 2) modifying the diversity maintenance mechanism in the algorithm. In this paper, we focus on the latter and propose a shift-based density estimation (SDE) strategy. The aim of our study is to develop a general modification of density estimation in order to make Pareto-based algorithms suitable for many-objective optimization. In contrast to traditional density estimation that only involves the distribution of individuals in the population, SDE covers both the distribution and convergence information of individuals. The application of SDE in three popular Pareto-based algorithms demonstrates its usefulness in handling many-objective problems. Moreover, an extensive comparison with five state-of-the-art EMO algorithms reveals its competitiveness in balancing convergence and diversity of solutions. These findings not only show that SDE is a good alternative to tackle many-objective problems, but also present a general extension of Pareto-based algorithms in many-objective optimization.},
  Doi                      = {10.1109/TEVC.2013.2262178},
  File                     = {:PDF/Li2014b.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {Pareto optimisation;evolutionary computation;EMO algorithms;Pareto dominance relation;Pareto front;Pareto-based evolutionary multiobjective optimization algorithm;SDE strategy;diversity maintenance mechanisms;high-dimensional space;many-objective optimization;shift-based density estimation;sparse regions;Convergence;Evolutionary multiobjective optimization;convergence;diversity;evolutionary multiobjective optimization;many-objective optimization;manyobjective optimization;shift-based density estimation}
}

@Article{Li2014a,
  Title                    = {Etea: A Euclidean Minimum Spanning Tree-based Evolutionary Algorithm for Multi-objective Optimization},
  Author                   = {Li, Miqing and Yang, Shengxiang and Zheng, Jinhua and Liu, Xiaohui},
  Journal                  = {Evol. Comput.},
  Year                     = {2014},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {189--230},
  Volume                   = {22},

  Acmid                    = {2645287},
  Address                  = {Cambridge, MA, USA},
  Doi                      = {10.1162/EVCO_a_00106},
  File                     = {:PDF/Li2014a.pdf:PDF},
  ISSN                     = {1063-6560},
  Issue_date               = {Summer 2014},
  Keywords                 = {Euclidean minimum spanning tree, Multi-objective optimization, archive truncation, density estimation, evolutionary algorithms, fitness adjustment, fitness assignment},
  Numpages                 = {42},
  Publisher                = {MIT Press}
}

@InCollection{Li2003,
  Title                    = {A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective Optimization},
  Author                   = {Li, Xiaodong},
  Booktitle                = {Genetic and Evolutionary Computation — GECCO 2003},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2003},
  Editor                   = {Cantú-Paz, Erick and Foster, JamesA. and Deb, Kalyanmoy and Davis, LawrenceDavid and Roy, Rajkumar and O’Reilly, Una-May and Beyer, Hans-Georg and Standish, Russell and Kendall, Graham and Wilson, Stewart and Harman, Mark and Wegener, Joachim and Dasgupta, Dipankar and Potter, MitchA. and Schultz, AlanC. and Dowsland, KathrynA. and Jonoska, Natasha and Miller, Julian},
  Pages                    = {37--48},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {2723},

  Doi                      = {10.1007/3-540-45105-6_4},
  File                     = {:PDF/Li2003.pdf:PDF},
  ISBN                     = {978-3-540-40602-0},
  Language                 = {English}
}

@InProceedings{Lin2011,
  Title                    = {Scheduling Scientific Workflows Elastically for Cloud Computing},
  Author                   = {Cui Lin and Shiyong Lu},
  Booktitle                = {Cloud Computing (CLOUD), 2011 IEEE International Conference on},
  Year                     = {2011},
  Month                    = jul,
  Pages                    = {746--747},

  Abstract                 = {Most existing workflow scheduling algorithms only consider a computing environment in which the number of compute resources is bounded. Compute resources in such an environment usually cannot be provisioned or released on demand of the size of a workflow, and these resources are not released to the environment until an execution of the workflow completes. To address the problem, we firstly formalize a model of a Cloud environment and a workflow graph representation for such an environment. Then, we propose the SHEFT workflow scheduling algorithm to schedule a workflow elastically on a Cloud computing environment. Our preliminary experiments show that SHEFT not only outperforms several representative workflow scheduling algorithms in optimizing workflow execution time, but also enables resources to scale elastically at runtime.},
  Doi                      = {10.1109/CLOUD.2011.110},
  File                     = {:PDF/Lin2011.pdf:PDF},
  ISSN                     = {2159-6182},
  Keywords                 = {cloud computing;graph theory;scheduling;SHEFT workflow scheduling algorithm;cloud computing;cloud environment;computing environment;scheduling scientific workflows elastically;workflow graph representation;Cloud computing;Computational modeling;Data models;Runtime;Schedules;Scheduling algorithm;Workflow scheduling;cloud computing;elastic scaling;heterogeneous environment}
}

@Electronic{Liu2014,
  Title                    = {Amazon EC2 grows 62\% in 2 years},
  Author                   = {Liu, Huan},
  Month                    = feb,
  Url                      = {http://huanliu.wordpress.com/2014/02/26/amazon-ec2-grows-62-in-2-years/},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/FgkxoR}
}

@InProceedings{Liu2008,
  Title                    = {A fast and elitist multi-objective particle swarm algorithm: NSPSO},
  Author                   = {Yang Liu},
  Booktitle                = {Granular Computing, 2008. GrC 2008. IEEE International Conference on},
  Year                     = {2008},
  Month                    = aug,
  Pages                    = {470--475},

  Abstract                 = {In this paper, a new nondominated sorting particle swarm optimisation (NSPSO), is proposed, that combines the operations (fast ranking of non-dominated solutions, crowding distance ranking and elitist strategy of combining parent population and offspring population together) of a known MOGA NSGA-II and the other advanced operations (selection and mutation operations) with a single particle swarm optimiser (PSO). The efficacy of this algorithm is demonstrated on 2 test functions, and the comparison is made with the NSGA-II and a multi-objective PSO (MOPSO-CD). The simulation results suggest that the proposed optimisation framework is able to achieve good solutions as well diversity compared to NSGA-II and MOPSO-CD optimisation framework.},
  Doi                      = {10.1109/GRC.2008.4664711},
  File                     = {:PDF/Liu2008.pdf:PDF},
  Keywords                 = {particle swarm optimisation;crowding distance ranking;elitist strategy;multiobjective particle swarm algorithm;mutation operations;nondominated solution ranking;nondominated sorting particle swarm optimisation;offspring population;parent population;Birds;Diversity reception;Educational institutions;Genetic algorithms;Genetic mutations;Marine animals;Pareto optimization;Particle swarm optimization;Sorting;Testing}
}

@Article{Luo2015,
  Title                    = {An Enhanced Workflow Scheduling Strategy for Deadline Guarantee on Hybrid Grid/Cloud Infrastructure},
  Author                   = {Luo, Huimin and Yan, Chaokun and Hu, Zhigang},
  Journal                  = {Journal of Applied Science and Engineering},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {67--78},
  Volume                   = {18},

  File                     = {:PDF/Luo2015.pdf:PDF}
}

@InProceedings{Malawski2012,
  Title                    = {Cost- and Deadline-constrained Provisioning for Scientific Workflow Ensembles in IaaS Clouds},
  Author                   = {Malawski, Maciej and Juve, Gideon and Deelman, Ewa and Nabrzyski, Jarek},
  Booktitle                = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
  Year                     = {2012},

  Address                  = {Los Alamitos, CA, USA},
  Pages                    = {22:1--22:11},
  Publisher                = {IEEE Computer Society Press},
  Series                   = {SC '12},

  Acmid                    = {2389026},
  Articleno                = {22},
  File                     = {Malawski2012.pdf:PDF/Malawski2012.pdf:PDF},
  ISBN                     = {978-1-4673-0804-5},
  Location                 = {Salt Lake City, Utah},
  Numpages                 = {11},
  Url                      = {http://dl.acm.org/citation.cfm?id=2388996.2389026}
}

@InProceedings{Man2013,
  Title                    = {Cost and efficiency-based scheduling on a general framework combining between cloud computing and local thick clients},
  Author                   = {Man, Nguyen Doan and Huh, Eui-Nam},
  Booktitle                = {Computing, Management and Telecommunications (ComManTel), 2013 International Conference on},
  Year                     = {2013},
  Month                    = jan,
  Pages                    = {258--263},

  Abstract                 = {Today, the rapid growth of the large-scale appli-cations conduces to a difficult challenge to individuals as well as the commercial organizations due to the limitation of the local computing resources. Meanwhile, the extension of the local computing platforms requires a huge investment about both finance and human power. Therefore, the use of nearly-unlimited resources and on-demand scaling of Cloud computing is considered as a potential solution to address the problems above. However, since Cloud computing is built from a pay-as-you-go model, the novel application scheduling approaches are challenged to guarantee the high efficiency of application execution and the reasonable cost for renting Cloud resources. In this paper, we present a novel framework built from the combination between the computing resources on Cloud computing and the computing components in the local systems. The key component of this framework is the Cost with Finish Time-based scheduling algorithm, which provides the balance between performance of application schedule and the mandatory cost for the use of Cloud resources. The experiments and comparison with the other scheduling approaches demonstrated the potential benefits of our proposed algorithm.},
  Doi                      = {10.1109/ComManTel.2013.6482401},
  File                     = {:PDF/Man2013.pdf:PDF},
  Keywords                 = {Cloud computing;Computational modeling;Data transfer;Schedules;Scheduling;Scheduling algorithms}
}

@InProceedings{Martens2012,
  Title                    = {Costing of Cloud Computing Services: A Total Cost of Ownership Approach},
  Author                   = {Martens, B. and Walterbusch, M. and Teuteberg, F.},
  Booktitle                = {System Science (HICSS), 2012 45\textsuperscript{th} Hawaii International Conference on},
  Year                     = {2012},
  Month                    = jan,
  Pages                    = {1563--1572},

  Abstract                 = {The use of Cloud Computing Services appears to offer significant cost advantages. Particularly start-up companies benefit from these advantages, since frequently they do not operate an internal IT infrastructure. But are costs associated with Cloud Computing Services really that low? We found that particular cost types and factors are frequently underestimated by practitioners. In this paper we present a Total Cost of Ownership (TCO) approach for Cloud Computing Services. We applied a multi-method approach (systematic literature review, analysis of real Cloud Computing Services, expert interview, case study) for the development and evaluation of the formal mathematical model. We found that our model fits the practical requirements and supports decision-making in Cloud Computing.},
  Doi                      = {10.1109/HICSS.2012.186},
  File                     = {:PDF/Martens2012.pdf:PDF},
  ISSN                     = {1530-1605},
  Keywords                 = {cloud computing;costing;decision making;cloud computing service;cost type;decision-making;formal mathematical model;multimethod approach;start-up companies;total cost of ownership;Analytical models;Cloud computing;Companies;Computational modeling;Mathematical model;Pricing;Systematics;Cloud Computing;Cloud Computing Services;Costing;Total Cost of Ownership}
}

@Article{Mell2009,
  Title                    = {The NIST definition of cloud computing},
  Author                   = {Mell, Peter and Grance, Tim},
  Journal                  = {National Institute of Standards and Technology},
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {50},
  Volume                   = {53},

  File                     = {:PDF/Mell2009.pdf:PDF},
  Institution              = {National Institute of Standards and Technology},
  Publisher                = {IGI Publication},
  Url                      = {http://faculty.winthrop.edu/domanm/csci411/Handouts/NIST.pdf}
}

@Electronic{Microsoft2014,
  Title                    = {Virtual Machines Pricing Details},
  Author                   = {Microsoft},
  Url                      = {http://goo.gl/UrDkvF},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/UrDkvF}
}

@Article{Nesmachnow2013,
  Title                    = {Energy-Aware Scheduling on Multicore Heterogeneous Grid Computing Systems},
  Author                   = {Nesmachnow, Sergio and Dorronsoro, Bernabé and Pecero, JohnatanE. and Bouvry, Pascal},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {653--680},
  Volume                   = {11},

  Doi                      = {10.1007/s10723-013-9258-3},
  File                     = {:PDF/Nesmachnow2013.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Grid scheduling; Energy-aware; Heterogeneous computing},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@Article{Oliveira2012,
  Title                    = {A Provenance-based Adaptive Scheduling Heuristic for Parallel Scientific Workflows in Clouds},
  Author                   = {de Oliveira, Daniel and Ocaña, KaryA.C.S. and Baião, Fernanda and Mattoso, Marta},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {521--552},
  Volume                   = {10},

  Doi                      = {10.1007/s10723-012-9227-2},
  File                     = {:PDF/Oliveira2012.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Cloud computing; Scientific workflow; Scientific experiment; Provenance},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@InCollection{Ostermann2012,
  Title                    = {Impact of Variable Priced Cloud Resources on Scientific Workflow Scheduling},
  Author                   = {Ostermann, Simon and Prodan, Radu},
  Booktitle                = {Euro-Par 2012 Parallel Processing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2012},
  Editor                   = {Kaklamanis, Christos and Papatheodorou, Theodore and Spirakis, PaulG.},
  Pages                    = {350--362},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {7484},

  Doi                      = {10.1007/978-3-642-32820-6_35},
  File                     = {:PDF/Ostermann2012.pdf:PDF},
  ISBN                     = {978-3-642-32819-0},
  Keywords                 = {Cloud computing; Grid computing; Spot instances; Scheduling; Scientific workflows; Performance; Cost},
  Language                 = {English}
}

@InProceedings{Pandey2010,
  Title                    = {A Particle Swarm Optimization-Based Heuristic for Scheduling Workflow Applications in Cloud Computing Environments},
  Author                   = {Pandey, S. and Linlin Wu and Guru, S.M. and Buyya, R.},
  Booktitle                = {Advanced Information Networking and Applications (AINA), 2010 24\textsuperscript{th} IEEE International Conference on},
  Year                     = {2010},
  Month                    = apr,
  Pages                    = {400--407},

  Abstract                 = {Cloud computing environments facilitate applications by providing virtualized resources that can be provisioned dynamically. However, users are charged on a pay-per-use basis. User applications may incur large data retrieval and execution costs when they are scheduled taking into account only the `execution time'. In addition to optimizing execution time, the cost arising from data transfers between resources as well as execution costs must also be taken into account. In this paper, we present a particle swarm optimization (PSO) based heuristic to schedule applications to cloud resources that takes into account both computation cost and data transmission cost. We experiment with a workflow application by varying its computation and communication costs. We compare the cost savings when using PSO and existing `Best Resource Selection' (BRS) algorithm. Our results show that PSO can achieve: (a) as much as 3 times cost savings as compared to BRS, and (b) good distribution of workload onto resources.},
  Doi                      = {10.1109/AINA.2010.31},
  File                     = {:PDF/Pandey2010.pdf:PDF},
  ISSN                     = {1550-445X},
  Keywords                 = {Internet;groupware;particle swarm optimisation;BRS algorithm;PSO-based heuristic;best resource selection;cloud computing;collaborative scientific experiments;computation cost;data retrieval;data transmission cost;execution cost;execution time;particle swarm optimization;workflow applications scheduling;Application software;Application virtualization;Cloud computing;Computational efficiency;Computer science;Cost function;Dynamic scheduling;Particle swarm optimization;Processor scheduling;Software engineering;Cloud computing;Workflow scheduling;particle swarm optimization}
}

@InProceedings{Phan2012,
  Title                    = {Evolutionary Multiobjective Optimization for Green Clouds},
  Author                   = {Phan, Dung H. and Suzuki, Junichi and Carroll, Raymond and Balasubramaniam, Sasitharan and Donnelly, William and Botvich, Dmitri},
  Booktitle                = {Proceedings of the 14\textsuperscript{th} Annual Conference Companion on Genetic and Evolutionary Computation},
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Pages                    = {19--26},
  Publisher                = {ACM},
  Series                   = {GECCO '12},

  Acmid                    = {2330788},
  Doi                      = {10.1145/2330784.2330788},
  File                     = {:PDF/Phan2012.pdf:PDF},
  ISBN                     = {978-1-4503-1178-6},
  Keywords                 = {cloud computing, evolutionary multiobjective optimization, internet data centers, renewable energy, sustainability},
  Location                 = {Philadelphia, Pennsylvania, USA},
  Numpages                 = {8}
}

@InProceedings{Raghavan2015,
  Title                    = {Bat algorithm for scheduling workflow applications in cloud},
  Author                   = {Raghavan, S. and Marimuthu, C. and Sarwesh, P. and Chandrasekaran, K.},
  Booktitle                = {Electronic Design, Computer Networks Automated Verification (EDCAV), 2015 International Conference on},
  Year                     = {2015},
  Month                    = jan,
  Pages                    = {139--144},

  Abstract                 = {Workflow is one of the important aspects of cloud computing today. Cloud computing is one of the fastest growing technologies in the world. Workflows can be used in cloud as we use them in grid. Many operations in the cloud are based on workflow execution. Workflow systems are now becoming more complex and for such kind of systems efficient workflow management is important. Workflow scheduling is an important part of workflow management. Scheduling in general is NP-hard problem. To solve such kind of problems exhaustive methods cannot be used. Only non-exhaustive techniques can be used. In this paper we have used a metaheuristic approach called bat algorithm. Bat algorithm is specifically designed for optimizing hard problems. Here, bat algorithm with the help of binary bat algorithm is used for scheduling workflow in a cloud. Specifically the mapping of tasks and resources is done using this method. The optimal resources are selected such that the overall cost of the workflow is minimal.},
  Doi                      = {10.1109/EDCAV.2015.7060555},
  File                     = {:PDF/Raghavan2015.pdf:PDF},
  Keywords                 = {cloud computing;optimisation;NP-hard problem;binary bat algorithm;cloud computing;metaheuristic approach;workflow execution;workflow management;workflow scheduling;Algorithm design and analysis;Cloud computing;Heuristic algorithms;Optimization;Scheduling;Scheduling algorithms;Bat Algorithm;Binary Bat Algorithm (BBA);Cloud Workflow Scheduling;Workflow Scheduling}
}

@InProceedings{Rahman2011,
  Title                    = {Hybrid Heuristic for Scheduling Data Analytics Workflow Applications in Hybrid Cloud Environment},
  Author                   = {Rahman, M. and Xiaorong Li and Palit, H.},
  Booktitle                = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on},
  Year                     = {2011},
  Month                    = may,
  Pages                    = {966--974},

  Abstract                 = {Effective scheduling is a key concern for the execution of performance driven applications, such as workflows in dynamic and cost driven environment including Cloud. The majority of existing scheduling techniques are based on meta-heuristics that produce good schedules with advance reservation given the current state of Cloud services or heuristics that are dynamic in nature, and map the workflow tasks to services on-the-fly, but lack the ability of generating schedules considering workflow-level optimization and user QoS constraints. In this paper, we propose an Adaptive Hybrid Heuristic for user constrained data-analytics workflow scheduling in hybrid Cloud environment by integrating the dynamic nature of heuristic based approaches as well as workflow-level optimization capability of meta-heuristic based approaches. The effectiveness of the proposed approach is illustrated by a comprehensive case study with comparison to existing techniques.},
  Doi                      = {10.1109/IPDPS.2011.243},
  File                     = {:PDF/Rahman2011.pdf:PDF},
  ISSN                     = {1530-2075},
  Keywords                 = {cloud computing;data analysis;scheduling;QoS constraint;adaptive hybrid heuristic;cloud services;cost driven environment;data analytics workflow application scheduling;data-analytics workflow scheduling;hybrid cloud environment;meta-heuristics;scheduling technique;workflow-level optimization capability;Cloud computing;Dynamic scheduling;Optimization;Processor scheduling;Quality of service;Schedules}
}

@InCollection{Sakellariou2007,
  Title                    = {Scheduling Workflows with Budget Constraints},
  Author                   = {Sakellariou, Rizos and Zhao, Henan and Tsiakkouri, Eleni and Dikaiakos, MariosD.},
  Booktitle                = {Integrated Research in GRID Computing},
  Publisher                = {Springer US},
  Year                     = {2007},
  Editor                   = {Gorlatch, Sergei and Danelutto, Marco},
  Pages                    = {189--202},

  Doi                      = {10.1007/978-0-387-47658-2_14},
  File                     = {:PDF/Sakellariou2007.pdf:PDF},
  ISBN                     = {978-0-387-47656-8},
  Keywords                 = {Workflows; Scheduling; Budget Constrained Scheduling; DAG Scheduling},
  Language                 = {English}
}

@Article{Salimi2014,
  Title                    = {Task scheduling using \{NSGA\} \{II\} with fuzzy adaptive operators for computational grids },
  Author                   = {Reza Salimi and Homayun Motameni and Hesam Omranpour},
  Journal                  = {Journal of Parallel and Distributed Computing },
  Year                     = {2014},
  Number                   = {5},
  Pages                    = {2333--2350},
  Volume                   = {74},

  Abstract                 = {Abstract Scheduling algorithms have an essential role in computational grids for managing jobs, and assigning them to appropriate resources. An efficient task scheduling algorithm can achieve minimum execution time and maximum resource utilization by providing the load balance between resources in the grid. The superiority of genetic algorithm in the scheduling of tasks has been proven in the literature. In this paper, we improve the famous multi-objective genetic algorithm known as NSGA-II using fuzzy operators to improve quality and performance of task scheduling in the market-based grid environment. Load balancing, Makespan and Price are three important objectives for multi-objective optimization in the task scheduling problem in the grid. Grid users do not attend load balancing in making decision, so it is desirable that all solutions have good load balancing. Thus to decrease computation and ease decision making through the users, we should consider and improve the load balancing problem in the task scheduling indirectly using the fuzzy system without implementing the third objective function. We have used fuzzy operators for this purpose and more quality and variety in Pareto-optimal solutions. Three functions are defined to generate inputs for fuzzy systems. Variance of costs, variance of frequency of involved resources in scheduling and variance of genes values are used to determine probabilities of crossover and mutation intelligently. Variance of frequency of involved resources with cooperation of Makespan objective satisfies load balancing objective indirectly. Variance of genes values and variance of costs are used in the mutation fuzzy system to improve diversity and quality of Pareto optimal front. Our method conducts the algorithm towards best and most appropriate solutions with load balancing in less iteration. The obtained results have proved that our innovative algorithm converges to Pareto-optimal solutions faster and with more quality. },
  Doi                      = {10.1016/j.jpdc.2014.01.006},
  File                     = {:PDF/Salimi2014.pdf:PDF},
  ISSN                     = {0743-7315},
  Keywords                 = {Task scheduling,Load balancing,Grid computing,Non-dominated sorting genetic algorithm \{II\},Variance-based fuzzy operators,Multi-objective optimization },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0743731514000161}
}

@Article{SimsonLGarfinkel2007,
  Title                    = {An Evaluation of Amazon's Grid Computing Services: EC2, S3, and SQS},
  Author                   = {Simson L Garfinkel, Simson L Garfinkel},
  Year                     = {2007},

  File                     = {:PDF/SimsonLGarfinkel2007.pdf:PDF},
  Keywords                 = {Cloud, Evaluation},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.172.2239}
}

@Article{Smanchat2015,
  Title                    = {Taxonomies of workflow scheduling problem and techniques in the cloud },
  Author                   = {Sucha Smanchat and Kanchana Viriyapant},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Scientific workflows, like other applications, benefit from the cloud computing, which offers access to virtually unlimited resources provisioned elastically on demand. In order to efficiently execute a workflow in the cloud, scheduling is required to address many new aspects introduced by cloud resource provisioning. In the last few years, many techniques have been proposed to tackle different cloud environments enabled by the flexible nature of the cloud, leading to the techniques of different designs. In this paper, taxonomies of cloud workflow scheduling problem and techniques are proposed based on analytical review. We identify and explain the aspects and classifications unique to workflow scheduling in the cloud environment in three categories, namely, scheduling process, task and resource. Lastly, review of several scheduling techniques are included and classified onto the proposed taxonomies. We hope that our taxonomies serve as a stepping stone for those entering this research area and for further development of scheduling technique. },
  Doi                      = {10.1016/j.future.2015.04.019},
  File                     = {:PDF/Smanchat2015.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Cloud computing;Cloud workflow;Workflow scheduling;Workflow scheduling taxonomy },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X15001776}
}

@Article{Somasundaram2014,
  Title                    = {CLOUDRB: A framework for scheduling and managing High-Performance Computing (HPC) applications in science cloud },
  Author                   = {Thamarai Selvi Somasundaram and Kannan Govindarajan},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Note                     = {Special Section: Distributed Solutions for Ubiquitous Computing and Ambient Intelligence },
  Number                   = {0},
  Pages                    = {47--65},
  Volume                   = {34},

  Abstract                 = {Abstract In recent years, the Cloud environment has played a major role in running High-Performance Computing (HPC) applications, which are computationally intensive and data intensive in nature. The High-Performance Computing Cloud (HPCC) or Science Cloud (SC) provides the resources to these types of applications in an on demand and scalable manner. Scheduling of jobs or applications in a Cloud environment is NP-Complete and complex in nature due to the dynamicity of resources and on demand user application requirements. The main motivation behind this research study is to design and develop a \{CLOUD\} Resource Broker (CLOUDRB) for efficiently managing cloud resources and completing jobs for scientific applications within a user-specified deadline. It is implemented and integrated with a Deadline-based Job Scheduling and Particle Swarm Optimization (PSO)-based Resource Allocation mechanism. Our proposed approach intends to achieve the objectives of minimizing both execution time and cost based on the defined fitness function. It is simulated by modeling the \{HPC\} jobs and Cloud resources using the Matlab programming environment. The simulation results prove the effectiveness of the proposed research work by minimizing the completion time, cost and job rejection ratio and maximizing the number of jobs completing their applications within a deadline and meeting the user’s satisfaction. The proposed work has been tested in our Eucalyptus-based cloud environments by submitting real-world \{HPC\} applications and observed the improvements in performance. },
  Doi                      = {10.1016/j.future.2013.12.024},
  File                     = {:PDF/Somasundaram2014.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Cloud computing;High Performance Computing (HPC);\{CLOUD\} Resource Broker (CLOUDRB);Resource allocation;Job scheduling;Particle Swarm Optimization (PSO);Science cloud },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X13002884}
}

@Article{Su2013,
  Title                    = {Cost-efficient task scheduling for executing large programs in the cloud },
  Author                   = {Sen Su and Jian Li and Qingjia Huang and Xiao Huang and Kai Shuang and Jie Wang},
  Journal                  = {Parallel Computing },
  Year                     = {2013},
  Number                   = {4–5},
  Pages                    = {177--188},
  Volume                   = {39},

  Abstract                 = {Abstract Executing a large program using clouds is a promising approach, as this class of programs may be decomposed into multiple sequences of tasks that can be executed on multiple virtual machines (VMs) in a cloud. Such sequences of tasks can be represented as a directed acyclic graph (DAG), where nodes are tasks and edges are precedence constraints between tasks. Cloud users pay for what their programs actually use according to the pricing models of the cloud providers. Early task scheduling algorithms are focused on minimizing makespan, without mechanisms to reduce the monetary cost incurred in the setting of clouds. We present a cost-efficient task-scheduling algorithm using two heuristic strategies.The first strategy dynamically maps tasks to the most cost-efficient \{VMs\} based on the concept of Pareto dominance. The second strategy, a complement to the first strategy, reduces the monetary costs of non-critical tasks. We carry out extensive numerical experiments on large \{DAGs\} generated at random as well as on real applications. The simulation results show that our algorithm can substantially reduce monetary costs while producing makespan as good as the best known task-scheduling algorithm can provide. },
  Doi                      = {10.1016/j.parco.2013.03.002},
  File                     = {:PDF/1-s2.0-S0167819113000355-main.pdf:PDF},
  ISSN                     = {0167-8191},
  Keywords                 = {Cloud computing,Cost-efficient scheduling,Parallel task scheduling,Directed acyclic graph },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167819113000355}
}

@InProceedings{Szabo2012,
  Title                    = {Evolving multi-objective strategies for task allocation of scientific workflows on public clouds},
  Author                   = {Szabo, C. and Kroeger, T.},
  Booktitle                = {Evolutionary Computation (CEC), 2012 IEEE Congress on},
  Year                     = {2012},
  Month                    = jun,
  Pages                    = {1--8},

  Abstract                 = {With the increase in deployment of scientific application on public and private clouds, the allocation of workflow tasks to specific cloud instances to reduce runtime and cost has emerged as an important challenge. The allocation of scientific workflows on public clouds can be described through a variety of perspectives and parameters and has been proved to be NP-complete. This paper presents an optimization framework for task allocation on public clouds. We present a solution that considers important parameters such as workflow runtime, communication overhead, and overall execution cost. Our multi-objective optimization framework builds on a simple and extensible cost model and uses a heuristic to determine the optimal number of cloud instances to be used. Using the Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) as an example, we show how our optimization heuristics lead to significantly better strategies than other state-of-the-art approaches. Specifically, our single-objective optimization is slightly better than a simple heuristic and a particle swarm optimization approach for small workflows, and achieves significant improvements for larger workflows. In a similar manner, our multi-objective optimization obtains similar results to our single-objective optimization for small-size workflows, and achieves up to 80% improvement for large-size workflows.},
  Doi                      = {10.1109/CEC.2012.6256556},
  File                     = {:PDF/Szabo2012.pdf:PDF},
  Keywords                 = {cloud computing;computational complexity;natural sciences computing;particle swarm optimisation;resource allocation;workflow management software;Amazon elastic compute cloud;Amazon simple storage service;EC2;NP-complete problem;S3;communication overhead;multiobjective optimization framework;multiobjective strategy;overall execution cost;particle swarm optimization approach;private clouds;public clouds;scientific workflows;single-objective optimization;task allocation;workflow runtime;Bandwidth;Biological cells;Cloud computing;Computational modeling;Optimization;Resource management;Runtime}
}

@Article{Szabo2014,
  Title                    = {Science in the Cloud: Allocation and Execution of Data-Intensive Scientific Workflows},
  Author                   = {Szabo, Claudia and Sheng, QuanZ. and Kroeger, Trent and Zhang, Yihong and Yu, Jian},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {245--264},
  Volume                   = {12},

  Doi                      = {10.1007/s10723-013-9282-3},
  File                     = {:PDF/Szabo2014.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Data-intensive workflows; Cloud computing; Scheduling; Allocation; Evolutionary computation},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@Article{Talukder2009,
  Title                    = {Multiobjective differential evolution for scheduling workflow applications on global Grids},
  Author                   = {Talukder, A. K. M. Khaled Ahsan and Kirley, Michael and Buyya, Rajkumar},
  Journal                  = {Concurrency and Computation: Practice and Experience},
  Year                     = {2009},

  Month                    = sep,
  Number                   = {13},
  Pages                    = {1742–1756},
  Volume                   = {21},

  Doi                      = {10.1002/cpe.1417},
  File                     = {:PDF/Talukder2009.pdf:PDF},
  ISSN                     = {1532-0634},
  Publisher                = {Wiley-Blackwell}
}

@Electronic{Topchiy2013,
  Title                    = {Testing Amazon EC2 network speed},
  Author                   = {Topchiy, Serhiy},
  Month                    = mar,
  Url                      = {http://goo.gl/9iHcl3},
  Year                     = {2013},

  Bdsk-url-1               = {http://goo.gl/9iHcl3}
}

@Article{Topcuoglu2002,
  Title                    = {Performance-effective and low-complexity task scheduling for heterogeneous computing},
  Author                   = {Topcuoglu, H. and Hariri, S. and Min-You Wu},
  Journal                  = {Parallel and Distributed Systems, IEEE Transactions on},
  Year                     = {2002},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {260--274},
  Volume                   = {13},

  Abstract                 = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics},
  Doi                      = {10.1109/71.993206},
  File                     = {:PDF/Topcuoglu2002.pdf:PDF},
  ISSN                     = {1045-9219},
  Keywords                 = {directed graphs;processor scheduling;workstation clusters;Critical-Path-on-a-Processor algorithm;DAG scheduling;Heterogeneous Earliest-Finish-Time algorithm;application scheduling problem;heterogeneous computing environments;list scheduling;parametric graph generator;scheduling costs;task graphs;time metrics;weighted directed acyclic graphs;Processor scheduling}
}

@Article{Tsai2013,
  Title                    = {Optimized task scheduling and resource allocation on cloud computing environment using improved differential evolution algorithm },
  Author                   = {Jinn-Tsong Tsai and Jia-Cen Fang and Jyh-Horng Chou},
  Journal                  = {Computers \& Operations Research },
  Year                     = {2013},
  Number                   = {12},
  Pages                    = {3045--3055},
  Volume                   = {40},

  Abstract                 = {AbstractPurpose The objective of this study is to optimize task scheduling and resource allocation using an improved differential evolution algorithm (IDEA) based on the proposed cost and time models on cloud computing environment. Methods The proposed \{IDEA\} combines the Taguchi method and a differential evolution algorithm (DEA). The \{DEA\} has a powerful global exploration capability on macro-space and uses fewer control parameters. The systematic reasoning ability of the Taguchi method is used to exploit the better individuals on micro-space to be potential offspring. Therefore, the proposed \{IDEA\} is well enhanced and balanced on exploration and exploitation. The proposed cost model includes the processing and receiving cost. In addition, the time model incorporates receiving, processing, and waiting time. The multi-objective optimization approach, which is the non-dominated sorting technique, not with normalized single-objective method, is applied to find the Pareto front of total cost and makespan. Results In the five-task five-resource problem, the mean coverage ratios C(IDEA, DEA) of 0.368 and C(IDEA, NSGA-II) of 0.3 are superior to the ratios C(DEA, IDEA) of 0.249 and C(NSGA-II, IDEA) of 0.288, respectively. In the ten-task ten-resource problem, the mean coverage ratios C(IDEA, DEA) of 0.506 and C(IDEA, NSGA-II) of 0.701 are superior to the ratios C(DEA, IDEA) of 0.286 and C(NSGA-II, IDEA) of 0.052, respectively. Wilcoxon matched-pairs signed-rank test confirms there is a significant difference between \{IDEA\} and the other methods. In summary, the above experimental results confirm that the \{IDEA\} outperforms both the \{DEA\} and NSGA-II in finding the better Pareto-optimal solutions. Conclusions In the study, the \{IDEA\} shows its effectiveness to optimize task scheduling and resource allocation compared with both the \{DEA\} and the NSGA-II. Moreover, for decision makers, the Gantt charts of task scheduling in terms of having smaller makespan, cost, and both can be selected to make their decision when conflicting objectives are present. },
  Doi                      = {10.1016/j.cor.2013.06.012},
  File                     = {:PDF/Tsai2013.pdf:PDF},
  ISSN                     = {0305-0548},
  Keywords                 = {Cloud computing;Differential evolution algorithm;Task scheduling;Cost and time models;Multi-objective approach },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S030505481300169X}
}

@InProceedings{Udomkasemsub2012,
  Title                    = {A multiple-objective workflow scheduling framework for cloud data analytics},
  Author                   = {Udomkasemsub, O. and Li Xiaorong and Achalakul, T.},
  Booktitle                = {Computer Science and Software Engineering (JCSSE), 2012 International Joint Conference on},
  Year                     = {2012},
  Month                    = may,
  Pages                    = {391--398},

  Abstract                 = {One of the most important characteristics of a cloud system is elasticity in resources provisioning. Cloud fabric often composes of massive and heterogeneous types of resources allowing the sciences and engineering applications in many domains to collaboratively utilize the infrastructure. As the cloud systems are designed for a large number of users, a large volume of data, and various types of applications, efficient task management is needed for cloud data analytics. One of the popular methods used in task management is to represent a set of tasks with a workflow diagram, which can capture task decomposition, communication between subtasks, and cost of computation and communication. In this paper, we proposed a workflow scheduling framework that can efficiently schedule series workflows with multiple objectives onto a cloud system. Our designed framework uses a meta-heuristics method, called Artificial Bee Colony (ABC), to create an optimized scheduling plan. The framework allows multiple constraints and objectives to be set. Conflicts among objectives can also be resolved using Pareto-based technique. A series of experiments are then conducted to investigate the performance in comparison to the algorithms often used in cloud scheduling. Results show that our proposed method is able to reduce 57% cost and 50% scheduling time within a similar makespan of HEFT/LOSS for a typical scientific workflow like Chimera-2.},
  Doi                      = {10.1109/JCSSE.2012.6261985},
  File                     = {:PDF/Udomkasemsub2012.pdf:PDF},
  Keywords                 = {Pareto optimisation;cloud computing;data analysis;resource allocation;scheduling;workflow management software;ABC;Chimera-2;HEFT/LOSS makespan;Pareto-based technique;artificial bee colony;cloud data analytics;cloud fabric;cloud scheduling;computation cost reduction;heterogeneous resources;meta-heuristics method;multiple-objective workflow scheduling framework;resource provisioning process;scheduling plan optimization;scheduling time reduction;scientific workflow;subtask communication cost reduction;task decomposition;task management;workflow diagram;Algorithm design and analysis;Equations;Genetic algorithms;Optimization;Processor scheduling;Schedules;Scheduling;Artificial Bee Colony;cloud computing;multiple-objective optimization;workflow scheduling}
}

@InProceedings{Vairavanathan2012,
  Title                    = {A Workflow-Aware Storage System: An Opportunity Study},
  Author                   = {Vairavanathan, Emalayan and Al-Kiswany, Samer and Costa, Lauro Beltr\~{a}o and Zhang, Zhao and Katz, Daniel S. and Wilde, Michael and Ripeanu, Matei},
  Booktitle                = {Proceedings of the 2012 12\textsuperscript{th} IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
  Year                     = {2012},

  Address                  = {Washington, DC, USA},
  Pages                    = {326--334},
  Publisher                = {IEEE Computer Society},
  Series                   = {CCGRID '12},

  Acmid                    = {2310197},
  Doi                      = {10.1109/CCGrid.2012.109},
  File                     = {:PDF/Vairavanathan2012.pdf:PDF},
  ISBN                     = {978-0-7695-4691-9},
  Keywords                 = {Large-scale storage systems, workflow-aware storage systems, workflow optimizations},
  Numpages                 = {9}
}

@Article{Vecchiola2009,
  Title                    = {Multi-Objective Problem Solving With Offspring on Enterprise Clouds},
  Author                   = {Christian Vecchiola and Michael Kirley and Rajkumar Buyya},
  Journal                  = {CoRR},
  Year                     = {2009},
  Volume                   = {abs/0903.1386},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-0903-1386},
  File                     = {:PDF/Vecchiola2009.pdf:PDF},
  Timestamp                = {Mon, 05 Dec 2011 18:05:37 +0100},
  Url                      = {http://arxiv.org/abs/0903.1386}
}

@InProceedings{Vecchiola2009a,
  Title                    = {High-Performance Cloud Computing: A View of Scientific Applications},
  Author                   = {Vecchiola, C. and Pandey, S. and Buyya, R.},
  Booktitle                = {Pervasive Systems, Algorithms, and Networks (ISPAN), 2009 10\textsuperscript{th} International Symposium on},
  Year                     = {2009},
  Month                    = dec,
  Pages                    = {4--16},

  Abstract                 = {Scientific computing often requires the availability of a massive number of computers for performing large scale experiments. Traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. Cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. Compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. These resources can be released when they are no more needed. Such services are often offered within the context of a service level agreement (SLA), which ensure the desired quality of service (QoS). Aneka, an enterprise cloud computing solution, harnesses the power of compute resources by relying on private and public clouds and delivers to users the desired QoS. Its flexible and service based infrastructure supports multiple programming paradigms that make Aneka address a variety of different scenarios: from finance applications to computational science. As examples of scientific computing in the cloud, we present a preliminary case study on using Aneka for the classification of gene expression data and the execution of fMRI brain imaging workflow.},
  Doi                      = {10.1109/I-SPAN.2009.150},
  File                     = {:PDF/Vecchiola2009a.pdf:PDF},
  Keywords                 = {Internet;quality of service;ubiquitous computing;QoS;high-performance cloud computing;quality of service;scientific computing;service level agreement;Application software;Availability;Cloud computing;Computer applications;Context-aware services;Finance;Gene expression;Large-scale systems;Quality of service;Scientific computing;Cloud computing;Scientific computing;computational science;high-performance computing}
}

@InProceedings{Wan2012,
  Title                    = {A QoS-awared scientific workflow scheduling schema in cloud computing},
  Author                   = {Cong Wan and Cuirong Wang and Jianxun Pei},
  Booktitle                = {Information Science and Technology (ICIST), 2012 International Conference on},
  Year                     = {2012},
  Month                    = mar,
  Pages                    = {634--639},

  Abstract                 = {Now it is a trend to deploy the applications in the cloud computing environment. Most of scientific workflow (SWF) needs to process a large amount of data and requires parallel computing to reduce response time. SWF under grid computing environment have been already studied. As a business model, cloud computing is more scalable and cost-effective. In this paper, we propose a schedule schema that enable user to change the processing time by adjusting the budget and to change the price by adjusting the request response time in the SWF.},
  Doi                      = {10.1109/ICIST.2012.6221722},
  File                     = {:PDF/Wan2012.pdf:PDF},
  Keywords                 = {cloud computing;natural sciences computing;parallel processing;quality of service;scheduling;QoS-awared scientific workflow scheduling schema;SWF;business model;cloud computing environment;parallel computing;Algorithm design and analysis;Cloud computing;Computational modeling;Processor scheduling;Schedules;Time factors}
}

@InProceedings{Wen2012,
  Title                    = {A Particle Swarm Optimization Algorithm for Batch Processing Workflow Scheduling},
  Author                   = {Yiping Wen and Zhigang Chen and Tiemin Chen and Jianxun Liu and Guosheng Kang},
  Booktitle                = {Cloud and Green Computing (CGC), 2012 Second International Conference on},
  Year                     = {2012},
  Month                    = nov,
  Pages                    = {645--649},

  Abstract                 = {Aiming at shortcomings in existing scheduling methods for batch processing workflow, this paper attempt to investigate and solve the optimization problem for grouping and scheduling multiple activity instances in batch processing workflow. A multiple objective optimal model of problem with constraints is presented firstly. Then, a discrete particle swarm optimization algorithm is proposed to produce a set of optimal Pareto solutions by optimizing the two objective functions simultaneously. The result of simulation experiment shows the effectiveness of this algorithm.},
  Doi                      = {10.1109/CGC.2012.67},
  File                     = {:PDF/Wen2012.pdf:PDF},
  Keywords                 = {Pareto optimisation;batch production systems;optimised production technology;particle swarm optimisation;scheduling;set theory;batch processing workflow scheduling;discrete particle swarm optimization algorithm;multiple activity instance grouping;multiple activity instance scheduling;multiple objective optimal model;objective function optimization;optimal Pareto solutions;Argon;Batch production systems;Dynamic scheduling;Heuristic algorithms;Job shop scheduling;Optimization;Particle swarm optimization;batch processing workflow;particle swarm optimization;scheduling}
}

@Article{Wieczorek2009,
  Title                    = {Towards a general model of the multi-criteria workflow scheduling on the grid },
  Author                   = {Marek Wieczorek and Andreas Hoheisel and Radu Prodan},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {237--256},
  Volume                   = {25},

  Abstract                 = {Workflow scheduling on the Grid becomes more challenging when multiple scheduling criteria are considered. Existing studies provide different approaches to the multi-criteria Grid workflow scheduling problem, and address different variants of the problem. A profound understanding of the problem’s nature can be an important step towards more generic scheduling approaches. Based on the related work and on our own experience, we propose several novel taxonomies of the problem, considering five facets: workflow model, scheduling criteria, scheduling process, resource model, and task model. We make a survey of the existing related work, and classify it according to the proposed taxonomies, identifying the most common use cases and the areas that have not been sufficiently explored yet. },
  Doi                      = {10.1016/j.future.2008.09.002},
  File                     = {:PDF/Wieczorek2009.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Grid computing;Workflow;Multi-criteria scheduling;Taxonomy },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X08001398}
}

@InProceedings{Wieczorek2006,
  Title                    = {Applying Advance Reservation to Increase Predictability of Workflow Execution on the Grid},
  Author                   = {Wieczorek, M. and Siddiqui, M. and Villazon, A. and Prodan, R. and Fahringer, T.},
  Booktitle                = {e-Science and Grid Computing, 2006. e-Science '06. Second IEEE International Conference on},
  Year                     = {2006},
  Month                    = dec,
  Pages                    = {82--82},

  Abstract                 = {In this paper we present an extension to devise and implement advance reservation as part of the scheduling and resource management services of the ASKALON Grid application development and runtime environment. The scheduling service has been enhanced to offer a list of resources that can execute a specific task and to negotiatewith the resource manager about resources capable of processing tasks in the shortest possible time. We introduce progressive reservation approach which tries to allocate resources based on a fair-share principle. Experiments are shown that demonstrate the effectiveness of our approach, and that reflect different QoS parameters including performance, predictability, resource usage and resource fairness.},
  Doi                      = {10.1109/E-SCIENCE.2006.261166},
  File                     = {:PDF/Wieczorek2006.pdf:PDF},
  Keywords                 = {Application software;Computer applications;Computer architecture;Computer science;Economic forecasting;Grid computing;Processor scheduling;Resource management;Runtime environment;Scheduling algorithm}
}

@Article{Wu2015a,
  Title                    = {Workflow scheduling in cloud: a survey},
  Author                   = {Wu, Fuhui and Wu, Qingbo and Tan, Yusong},
  Journal                  = {The Journal of Supercomputing},
  Year                     = {2015},
  Pages                    = {1--46},

  Doi                      = {10.1007/s11227-015-1438-4},
  File                     = {:home/tefx/Documents/Papers/PDF/Wu2015a.pdf:PDF},
  ISSN                     = {0920-8542},
  Keywords                 = {Cloud computing; Workflow scheduling; QoS constrained scheduling; Workflow-as-a-service; Robust scheduling; Hybrid environment; Data-intensive workflow scheduling},
  Language                 = {English},
  Publisher                = {Springer US}
}

@Article{Wu2015,
  Title                    = {Resource and Instance Hour Minimization for Deadline Constrained DAG Applications Using Computer Clouds},
  Author                   = {Wu, H. and Hua, X. and Li, Z. and Ren, S.},
  Journal                  = {Parallel and Distributed Systems, IEEE Transactions on},
  Year                     = {2015},
  Number                   = {99},
  Pages                    = {1--1},
  Volume                   = {PP},

  Abstract                 = {n this paper, we address the resource and virtual machine instance hour minimization problem for directed-acyclic-graph-based deadline constrained applications deployed on computer clouds. The allocated resources and instance hours on computer clouds must: (1) guarantee the satisfaction of a deadline constrained application’s end-to-end deadline; (2) ensure that the number of virtual machine (VM) instances allocated to the application is minimized; (3) under the allocated number of VM instances, determine application execution schedule that minimizes the application’s makespan; and (4) under the decided application execution schedule, determine a VM operation schedule, i.e., when a VM should be turned on or off, that minimizes total VM instance hours needed to execute the application. We first give lower and upper bounds for the number of VM instances needed to guarantee the satisfaction of a deadline constrained application’s end-to-end deadline. Based on the bounds, we develop a heuristic algorithm called minimal slack time and minimal distance (MSMD) algorithm that finds the minimum number of VM instances needed to guarantee the application’s deadline and schedules tasks on the allocated VM instances so that the application’s makespan is minimized. Once the application execution schedule and the number of VM instances needed are determined, the proposed VM instance hour minimization (IHM) algorithm is applied to further reduce the instance hours needed by VMs to complete the application’s execution. Our experimental results show that the MSMD algorithm can guarantee applications’ end-to-end deadlines with less resources than the HEFT [32], MOHEFT [16], DBUS [9], QoS-base [40] and Auto-Scaling [25] heuristic scheduling algorithms in the literature. Furthermore, under allocated resources, the MSMD algorithm can, on average, reduce an application’s makespan by 3.4% of its deadline. In addition, with th- IHM algorithm we can effectively reduce the application’s execution instance hours compared with when IHM is not applied.},
  Doi                      = {10.1109/TPDS.2015.2411257},
  File                     = {:PDF/Wu2015.pdf:PDF},
  ISSN                     = {1045-9219},
  Keywords                 = {Cloud computing;Computers;Heuristic algorithms;Minimization;Processor scheduling;Schedules;Virtual machining;Cloud;Cost minimization;Instance Hour Minimization;MSMD;Makespan minimization;Real-time;Resource Minimization;Scheduling}
}

@Article{Wu2013,
  Title                    = {A market-oriented hierarchical scheduling strategy in cloud workflow systems},
  Author                   = {Wu, Zhangjun and Liu, Xiao and Ni, Zhiwei and Yuan, Dong and Yang, Yun},
  Journal                  = {The Journal of Supercomputing},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {256--293},
  Volume                   = {63},

  Doi                      = {10.1007/s11227-011-0578-4},
  File                     = {:PDF/Wu2013.pdf:PDF},
  ISSN                     = {0920-8542},
  Keywords                 = {Cloud workflow system; Cloud computing; Workflow scheduling; Hierarchical scheduling; Metaheuristics},
  Language                 = {English},
  Publisher                = {Springer US}
}

@InProceedings{Wu2010,
  Title                    = {A Revised Discrete Particle Swarm Optimization for Cloud Workflow Scheduling},
  Author                   = {Zhangjun Wu and Zhiwei Ni and Lichuan Gu and Xiao Liu},
  Booktitle                = {Computational Intelligence and Security (CIS), 2010 International Conference on},
  Year                     = {2010},
  Month                    = dec,
  Pages                    = {184--188},

  Abstract                 = {A cloud workflow system is a type of platform service which facilitates the automation of distributed applications based on the novel cloud infrastructure. Compared with grid environment, data transfer is a big overhead for cloud workflows due to the market-oriented business model in the cloud environments. In this paper, a Revised Discrete Particle Swarm Optimization (RDPSO) is proposed to schedule applications among cloud services that takes both data transmission cost and computation cost into account. Experiment is conducted with a set of workflow applications by varying their data communication costs and computation costs according to a cloud price model. Comparison is made on make span and cost optimization ratio and the cost savings with RDPSO, the standard PSO and BRS (Best Resource Selection) algorithm. Experimental results show that the proposed RDPSO algorithm can achieve much more cost savings and better performance on make span and cost optimization.},
  Doi                      = {10.1109/CIS.2010.46},
  File                     = {:PDF/Wu2010.pdf:PDF},
  Keywords                 = {cloud computing;grid computing;particle swarm optimisation;scheduling;best resource selection algorithm;cloud price model;cloud workflow scheduling;computation cost;data transfer;data transmission cost;distributed applications;grid environment;market-oriented business model;revised discrete particle swarm optimization;cloud computing;discrete particle swarm optimization;workflow scheduling}
}

@Article{Xavier2013,
  Title                    = {A Survey of Various Workflow Scheduling Algorithms in Cloud Environment},
  Author                   = {Xavier, S and Lovesum, SPJ},
  Journal                  = {International Journal of Scientific and Research {\ldots}},
  Year                     = {2013},

  File                     = {:PDF/Xavier2013.pdf:PDF},
  Url                      = {http://www.ijsrp.org/research-paper-0213/ijsrp-p1444.pdf}
}

@InProceedings{Xie2014,
  Title                    = {A High-Performance DAG Task Scheduling Algorithm for Heterogeneous Networked Embedded Systems},
  Author                   = {Guoqi Xie and Renfa Li and Xiongren Xiao and Yuekun Chen},
  Booktitle                = {Advanced Information Networking and Applications (AINA), 2014 IEEE 28\textsuperscript{th} International Conference on},
  Year                     = {2014},
  Month                    = may,
  Pages                    = {1011--1016},

  Abstract                 = {A high-performance scheduling for a DAG (Directed Acyclic Graph) task graph on heterogeneous networked embedded systems or parallel and distributed systems is to maximize concurrency and minimize inter-processor communication. Most of the algorithms using upward rank value for task prioritizing and earliest finish time for processor assignment. But both approaches ignored the heterogeneity of system and could not create accurate and efficient schedules. Yet no one has doubled about and recognized that. A fully heterogeneous task scheduling algorithm is proposed to address the above problems in this paper. The fundamentals of DAG model and corresponding algorithms are investigated. New concepts called Heterogeneous Upward Rank Value (HURV) and Heterogeneous Priority Rank Value (HPRV) are defined. An algorithm called Heterogeneous Select Value (HSV) is proposed in paper. Both benchmark and extensive experimental evaluation demonstrate the significant improvements in proposed algorithm.},
  Doi                      = {10.1109/AINA.2014.123},
  File                     = {:PDF/Xie2014.pdf:PDF},
  ISSN                     = {1550-445X},
  Keywords                 = {embedded systems;parallel processing;scheduling;DAG model;directed acyclic graph;distributed systems;heterogeneous networked embedded systems;heterogeneous select value;heterogeneous task scheduling algorithm;heterogeneous upward rank value;high-performance DAG task scheduling algorithm;inter-processor communication;parallel systems;Automotive electronics;Complexity theory;Embedded systems;Schedules;Scheduling;Scheduling algorithms;DAG;heterogeneous networked embedded systems;heterogeneous select value;heterogeneous upward rank value}
}

@Article{Xu2013,
  Title                    = {Dynamic Cloud Pricing for Revenue Maximization},
  Author                   = {Hong Xu and Baochun Li},
  Journal                  = {Cloud Computing, IEEE Transactions on},
  Year                     = {2013},

  Month                    = jul,
  Number                   = {2},
  Pages                    = {158--171},
  Volume                   = {1},

  Abstract                 = {In cloud computing, a provider leases its computing resources in the form of virtual machines to users, and a price is charged for the period they are used. Though static pricing is the dominant pricing strategy in today's market, intuitively price ought to be dynamically updated to improve revenue. The fundamental challenge is to design an optimal dynamic pricing policy, with the presence of stochastic demand and perishable resources, so that the expected long-term revenue is maximized. In this paper, we make three contributions in addressing this question. First, we conduct an empirical study of the spot price history of Amazon, and find that surprisingly, the spot price is unlikely to be set according to market demand. This has important implications on understanding the current market, and motivates us to develop and analyze market-driven dynamic pricing mechanisms. Second, we adopt a revenue management framework from economics, and formulate the revenue maximization problem with dynamic pricing as a stochastic dynamic program. We characterize its optimality conditions, and prove important structural results. Finally, we extend to consider a nonhomogeneous demand model.},
  Doi                      = {10.1109/TCC.2013.15},
  File                     = {:PDF/Xu2013.pdf:PDF},
  ISSN                     = {2168-7161},
  Keywords                 = {cloud computing;dynamic programming;pricing;stochastic programming;virtual machines;AmazonV spot price history;cloud computing;dynamic cloud pricing;market-driven dynamic pricing mechanisms;nonhomogeneous demand model;perishable resources;revenue management framework;revenue maximization;static pricing;stochastic demand;stochastic dynamic program;virtual machines;Analytical models;Cloud computing;Cost accounting;Numerical models;Pricing;Stochastic processes;Dynamic pricing;cloud computing;dynamic programming;public cloud;revenue maximization;spot market}
}

@InProceedings{Xu2009,
  Title                    = {A Multiple QoS Constrained Scheduling Strategy of Multiple Workflows for Cloud Computing},
  Author                   = {Meng Xu and Lizhen Cui and Haiyang Wang and Yanbing Bi},
  Booktitle                = {Parallel and Distributed Processing with Applications, 2009 IEEE International Symposium on},
  Year                     = {2009},
  Month                    = aug,
  Pages                    = {629--634},

  Abstract                 = {Cloud computing has gained popularity in recent times. As a cloud must provide services to many users at the same time and different users have different QoS requirements, the scheduling strategy should be developed for multiple workflows with different QoS requirements. In this paper, we introduce a multiple QoS constrained scheduling strategy of multi-workflows (MQMW) to address this problem. The strategy can schedule multiple workflows which are started at any time and the QoS requirements are taken into account. Experimentation shows that our strategy is able to increase the scheduling success rate significantly.},
  Doi                      = {10.1109/ISPA.2009.95},
  File                     = {:PDF/Xu2009.pdf:PDF},
  Keywords                 = {Internet;quality of service;scheduling;workflow management software;cloud computing;multiple QoS constrained scheduling strategy;multiple workflows;Algorithm design and analysis;Application software;Bismuth;Cloud computing;Costs;Distributed processing;Partitioning algorithms;Processor scheduling;Quality of service;Scheduling algorithm;Cloud Computing;Multiple QoS Requirements;Multiple Workflows;Scheduling}
}

@Article{Xue2012,
  Title                    = {Scheduling Workflow in Cloud Computing Based on Hybrid Particle Swarm Algorithm},
  Author                   = {Xue, S J and Wu, W},
  Journal                  = {TELKOMNIKA Indonesian Journal of Electrica},
  Year                     = {2012},

  Doi                      = {10.11591/telkomnika.v10i7.1452},
  File                     = {:PDF/Xue2012.pdf:PDF},
  Url                      = {http://iaesjournal.com/online/index.php/TELKOMNIKA/article/view/1452}
}

@Article{Yassa,
  Title                    = {Multi-Objective Approach for Energy-Aware Workflow Scheduling in Cloud Computing Environments},
  Author                   = {Yassa, S and Chelouah, R and Kadima, H},
  Journal                  = {The Scientific World},

  File                     = {:PDF/Yassa.pdf:PDF}
}

@Article{Yu2005,
  Title                    = {A Taxonomy of Scientific Workflow Systems for Grid Computing},
  Author                   = {Yu, Jia and Buyya, Rajkumar},
  Journal                  = {SIGMOD Rec.},
  Year                     = {2005},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {44--49},
  Volume                   = {34},

  Acmid                    = {1084814},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1084805.1084814},
  File                     = {:PDF/Yu2005.pdf:PDF},
  ISSN                     = {0163-5808},
  Issue_date               = {September 2005},
  Keywords                 = {grid computing, scientific workflows, taxonomy},
  Numpages                 = {6},
  Publisher                = {ACM}
}

@InProceedings{Yu2005a,
  Title                    = {Cost-based scheduling of scientific workflow applications on utility grids},
  Author                   = {Jia Yu and Buyya, R. and Chen Khong Tham},
  Booktitle                = {e-Science and Grid Computing, 2005. First International Conference on},
  Year                     = {2005},
  Month                    = jul,
  Pages                    = {8 pp.-147},

  Abstract                 = {Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such "pay-per-use" grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a cost-based workflow scheduling algorithm that minimizes execution cost while meeting the deadline for delivering results. It can also adapt to the delays of service executions by rescheduling unexecuted tasks. We also attempt to optimally solve the task scheduling problem in branches with several sequential tasks by modeling the branch as a Markov decision process and using the value iteration method},
  Doi                      = {10.1109/E-SCIENCE.2005.26},
  File                     = {:PDF/Yu2005a.pdf:PDF},
  Keywords                 = {Markov processes;decision theory;grid computing;natural sciences computing;quality of service;scheduling;Markov decision process;cost-based scheduling;grid computing;grid technologies;pay-per-use grids;quality of service;scientific workflow applications;service execution delays;service provisioning;task scheduling;utility grids;value iteration method;Computer networks;Costs;Delay;Grid computing;Optimal scheduling;Processor scheduling;Quality of service;Scheduling algorithm;Time factors;Workflow management software}
}

@InProceedings{Yu2007,
  Title                    = {Multi-objective Planning for Workflow Execution on Grids},
  Author                   = {Yu, Jia and Kirley, Michael and Buyya, Rajkumar},
  Booktitle                = {Proceedings of the 8\textsuperscript{th} IEEE/ACM International Conference on Grid Computing},
  Year                     = {2007},

  Address                  = {Washington, DC, USA},
  Pages                    = {10--17},
  Publisher                = {IEEE Computer Society},
  Series                   = {GRID '07},

  Acmid                    = {1513496},
  Doi                      = {10.1109/GRID.2007.4354110},
  File                     = {:PDF/Yu2007.pdf:PDF},
  ISBN                     = {978-1-4244-1559-5},
  Numpages                 = {8}
}

@InProceedings{Yu2008,
  Title                    = {A Planner-Guided Scheduling Strategy for Multiple Workflow Applications},
  Author                   = {Zhifeng Yu and Weisong Shi},
  Booktitle                = {Parallel Processing - Workshops, 2008. ICPP-W '08. International Conference on},
  Year                     = {2008},
  Month                    = sep,
  Pages                    = {1--8},

  Abstract                 = {Workflow applications are gaining popularity in recent years because of the prevalence of cluster environments. Many algorithms have been developed since, however most static algorithms are designed in the problem domain of scheduling single workflow applications, thus not applicable to a common cluster environment where multiple workflow applications and other independent jobs compete for resources. Dynamic scheduling approaches can handle the mixed workload practically by nature but their performance has yet to optimize as they do not have a global view of workflow applications. Recent research efforts suggest merging multiple workflows into one workflow before execution, but fail to address an important issue that multiple workflow applications may be submitted at different times by different users. In this paper, we propose a planner-guided dynamic scheduling strategy for multiple workflow applications, leveraging job dependence information and execution time estimation.Our approach schedules individual jobs dynamically without requiring merging the workflow applications a priori. The simulation results show that the proposed algorithm significantly outperforms two other algorithms by 43.6% and 36.7% with respect to workflow makespan and turnaround time respectively, and it performs even better when the number of concurrent workflow applications increases and the resources are scarce.},
  Doi                      = {10.1109/ICPP-W.2008.10},
  File                     = {:PDF/Yu2008.pdf:PDF},
  ISSN                     = {1530-2016},
  Keywords                 = {graph theory;scheduling;dynamic scheduling strategy;execution time estimation;leveraging job dependence information;multiple workflow applications;planner-guided scheduling strategy;Algorithm design and analysis;Clustering algorithms;Dynamic scheduling;Heuristic algorithms;Job design;Merging;Parallel processing;Resource management;Scheduling algorithm;Time measurement;cluster;scheduling;workflow}
}

@InProceedings{Zhang2011,
  Title                    = {Ordinal Optimized Scheduling of Scientific Workflows in Elastic Compute Clouds},
  Author                   = {Fan Zhang and Cao, Junwei and Kai Hwang and Cheng Wu},
  Booktitle                = {Cloud Computing Technology and Science (CloudCom), 2011 IEEE Third International Conference on},
  Year                     = {2011},
  Month                    = nov,
  Pages                    = {9--17},

  Abstract                 = {Elastic compute clouds are best represented by the virtual clusters in Amazon EC2 or in IBM RC2. This paper proposes a simulation based approach to scheduling scientific workflows onto elastic clouds. Scheduling multitask workflows in virtual clusters is a NP-hard problem. Excessive simulations in months of time may be needed to produce the optimal schedule using Monte Carlo simulations. To reduce this scheduling overhead is necessary in real-time cloud computing. We present a new workflow scheduling method based on iterative ordinal optimization (IOO). This new method outperforms the Monte Carlo and Blind-Pick methods to yield higher performance against rapid workflow variations. For example, to execute 20,000 tasks on 128 virtual machines for gravitational wave analysis, an ordinal optimized schedule can be generated in a few minutes, which is O(103)~O(104) faster than using Monte Carlo simulations. The ordinal optimized schedule results in higher throughput with lower memory demand. The cloud experimental results being reported verified our theoretical findings on the relative performance of three workflow scheduling methods studied in this paper.},
  Doi                      = {10.1109/CloudCom.2011.12},
  File                     = {:PDF/Zhang2011.pdf:PDF},
  Keywords                 = {Monte Carlo methods;cloud computing;computational complexity;optimisation;scheduling;virtual machines;virtual reality;Amazon EC2;Blind-Pick method;IBM RC2;Monte Carlo simulation;NP-hard problem;elastic compute clouds;gravitational wave analysis;iterative ordinal optimization;memory demand;multitask workflow scheduling method;optimal scheduling;ordinal optimized scheduling;real time cloud computing;scientific workflow scheduling;simulation based approach;virtual cluster;virtual machine;Computational modeling;Monte Carlo methods;Optimal scheduling;Processor scheduling;Schedules;Servers;Cloud computing;ordinal optimization;virtual clustering;workflow scheduling}
}

@Article{Zhang2014,
  Title                    = {Multi-objective scheduling of many tasks in cloud platforms },
  Author                   = {Fan Zhang and Junwei Cao and Keqin Li and Samee U. Khan and Kai Hwang},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Note                     = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications },
  Number                   = {0},
  Pages                    = {309--320},
  Volume                   = {37},

  Abstract                 = {Abstract The scheduling of a many-task workflow in a distributed computing platform is a well known NP-hard problem. The problem is even more complex and challenging when the virtualized clusters are used to execute a large number of tasks in a cloud computing platform. The difficulty lies in satisfying multiple objectives that may be of conflicting nature. For instance, it is difficult to minimize the makespan of many tasks, while reducing the resource cost and preserving the fault tolerance and/or the quality of service (QoS) at the same time. These conflicting requirements and goals are difficult to optimize due to the unknown runtime conditions, such as the availability of the resources and random workload distributions. Instead of taking a very long time to generate an optimal schedule, we propose a new method to generate suboptimal or sufficiently good schedules for smooth multitask workflows on cloud platforms. Our new multi-objective scheduling (MOS) scheme is specially tailored for clouds and based on the ordinal optimization (OO) method that was originally developed by the automation community for the design optimization of very complex dynamic systems. We extend the \{OO\} scheme to meet the special demands from cloud platforms that apply to virtual clusters of servers from multiple data centers. We prove the suboptimality through mathematical analysis. The major advantage of our \{MOS\} method lies in the significantly reduced scheduling overhead time and yet a close to optimal performance. Extensive experiments were carried out on virtual clusters with 16 to 128 virtual machines. The multitasking workflow is obtained from a real scientific \{LIGO\} workload for earth gravitational wave analysis. The experimental results show that our proposed algorithm rapidly and effectively generates a small set of semi-optimal scheduling solutions. On a 128-node virtual cluster, the method results in a thousand times of reduction in the search time for semi-optimal workflow schedules compared with the use of the Monte Carlo and the Blind Pick methods for the same purpose. },
  Doi                      = {10.1016/j.future.2013.09.006},
  File                     = {:PDF/Zhang2014.pdf:PDF},
  ISSN                     = {0167-739X},
  Keywords                 = {Cloud computing;Many-task computing;Ordinal optimization;Performance evaluation;Virtual machines;Workflow scheduling },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X13001854}
}

@Article{Zhang2007,
  Title                    = {MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition},
  Author                   = {Qingfu Zhang and Hui Li},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {2007},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {712--731},
  Volume                   = {11},

  Abstract                 = {Decomposition is a basic strategy in traditional multiobjective optimization. However, it has not yet been widely used in multiobjective evolutionary optimization. This paper proposes a multiobjective evolutionary algorithm based on decomposition (MOEA/D). It decomposes a multiobjective optimization problem into a number of scalar optimization subproblems and optimizes them simultaneously. Each subproblem is optimized by only using information from its several neighboring subproblems, which makes MOEA/D have lower computational complexity at each generation than MOGLS and nondominated sorting genetic algorithm II (NSGA-II). Experimental results have demonstrated that MOEA/D with simple decomposition methods outperforms or performs similarly to MOGLS and NSGA-II on multiobjective 0-1 knapsack problems and continuous multiobjective optimization problems. It has been shown that MOEA/D using objective normalization can deal with disparately-scaled objectives, and MOEA/D with an advanced decomposition method can generate a set of very evenly distributed solutions for 3-objective test instances. The ability of MOEA/D with small population, the scalability and sensitivity of MOEA/D have also been experimentally investigated in this paper.},
  Doi                      = {10.1109/TEVC.2007.892759},
  File                     = {:PDF/Zhang2007.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {computational complexity;genetic algorithms;computational complexity;decomposition;genetic algorithm;knapsack problem;multiobjective evolutionary algorithm;scalar optimization subproblem;Computational complexity;Pareto optimality;decomposition;evolutionary algorithm;multiobjective optimization}
}

@InProceedings{Zhao2013,
  Title                    = {Towards high-performance and cost-effective distributed storage systems with information dispersal algorithms},
  Author                   = {Dongfang Zhao and Burlingame, K. and Debains, C. and Alvarez-Tabio, P. and Raicu, I.},
  Booktitle                = {Cluster Computing (CLUSTER), 2013 IEEE International Conference on},
  Year                     = {2013},
  Month                    = sep,
  Pages                    = {1--5},

  Abstract                 = {Reliability is one of the most fundamental challenges for high performance computing (HPC) and cloud computing. Data replication is the de facto mechanism to achieve high reliability, even though it has been criticized for its high cost and low efficiency. Recent research showed promising results by switching the traditional data replication to a software-based RAID. In order to systematically study the effectiveness of this new method, we built two storage systems from the ground up: a POSIX-compliant distributed file system (FusionFS) and a distributed key-value store (IStore), both supporting information dispersal algorithms (IDA) for data redundancy. FusionFS is crafted to have excellent throughput and scalability for HPC, whereas IStore is architected mainly as a light-weight key-value storage in cloud computing. We evaluated both systems with a large number of parameter combinations. Results show that, for both HPC and cloud computing communities, IDA-based methods with current commodity hardware could outperform data replication in some cases, and would completely surpass data replication with the growing computational capacity through multi/many-core processors (e.g. Intel Xeon Phi, NVIDIA GPU).},
  Doi                      = {10.1109/CLUSTER.2013.6702655},
  File                     = {:PDF/Zhao2013.pdf:PDF},
  Keywords                 = {cloud computing;multiprocessing systems;parallel processing;redundancy;storage management;FusionFS system;HPC;IDA;IStore;POSIX-compliant distributed file system;cloud computing;data replication;distributed key-value store;distributed storage systems;high performance computing;information dispersal algorithms;lightweight key-value storage;many-core processors;multicore processors;redundant array of independent disks;software-based RAID;Artificial neural networks;Encoding;Nickel;Redundancy;Switches;Writing}
}

@InCollection{Zhao2007,
  Title                    = {Advance Reservation Policies for Workflows},
  Author                   = {Zhao, Henan and Sakellariou, Rizos},
  Booktitle                = {Job Scheduling Strategies for Parallel Processing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2007},
  Editor                   = {Frachtenberg, Eitan and Schwiegelshohn, Uwe},
  Pages                    = {47--67},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {4376},

  Doi                      = {10.1007/978-3-540-71035-6_3},
  File                     = {:PDF/Zhao2007.pdf:PDF},
  ISBN                     = {978-3-540-71034-9},
  Language                 = {English}
}

@InProceedings{Zhao2011,
  Title                    = {Opportunities and Challenges in Running Scientific Workflows on the Cloud},
  Author                   = {Yong Zhao and Xubo Fei and Raicu, I. and Shiyong Lu},
  Booktitle                = {Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC), 2011 International Conference on},
  Year                     = {2011},
  Month                    = oct,
  Pages                    = {455--462},

  Abstract                 = {Cloud computing is gaining tremendous momentum in both academia and industry. The application of Cloud computing, however, has mostly focused on Web applications and business applications; while the recognition of using Cloud computing to support large-scale workflows, especially data-intensive scientific workflows on the Cloud is still largely overlooked. We coin the term "Cloud Workflow", to refer to the specification, execution, provenance tracking of large-scale scientific workflows, as well as the management of data and computing resources to enable the execution of scientific workflows on the Cloud. In this paper, we analyze why there has been such a gap between the two technologies, and what it means to bring Cloud and workflow together; we then present the key challenges in running Cloud workflow, and discuss the research opportunities in realizing workflows on the Cloud.},
  Doi                      = {10.1109/CyberC.2011.80},
  File                     = {:PDF/Zhao2011.pdf:PDF},
  Keywords                 = {cloud computing;natural sciences computing;workflow management software;Web applications;business applications;cloud computing;cloud workflow;scientific workflow;Cloud computing;Computational modeling;Computer architecture;Engines;Monitoring;Programming;Scalability;Cloud computing;Cloud workflow;Data Intensive Computing;Scientific Workflow}
}

@InProceedings{Zhao2012,
  Title                    = {Scientific-Workflow-Management-as-a-Service in the Cloud},
  Author                   = {Yong Zhao and Youfu Li and Wenhong Tian and Ruini Xue},
  Booktitle                = {Cloud and Green Computing (CGC), 2012 Second International Conference on},
  Year                     = {2012},
  Month                    = nov,
  Pages                    = {97--104},

  Abstract                 = {Scientific workflow management systems have been around for many years and provide essential support such as management of data and task dependencies, job scheduling and execution, provenance tracking, etc. to scientific computing. While we are entering into a “big data” era, it is necessary for scientific workflow systems to integrate with Cloud platforms so as to deal with the ever increasing data scale and analysis complexity. In this paper, we present our experience in offering the Swift scientific workflow management system as a service in the Cloud. Our solution integrates Swift with the OpenNebula Cloud platform, and supports workflow specification and submission, on-demand virtual cluster provisioning, high-throughput task scheduling and execution, and efficient and scalable Cloud resource management. We demonstrate the capability of the solution using a NASA MODIS image processing workflow.},
  Doi                      = {10.1109/CGC.2012.70},
  File                     = {:PDF/Zhao2012.pdf:PDF},
  Keywords                 = {cloud computing;natural sciences computing;workflow management software;NASA MODIS image processing workflow;OpenNebula Cloud platform;Swift scientific workflow management system as a service;analysis complexity;big data era;data scale;high-throughput task execution;high-throughput task scheduling;on-demand virtual cluster provisioning;workflow specification;Cloud computing;Computer architecture;Customer relationship management;Monitoring;Processor scheduling;Resource management;Scalability;Cloud workflow;Scientific Workflow;Swift;Workflow-as-a-Service}
}

@Article{Zheng2013,
  Title                    = {Budget-Deadline Constrained Workflow Planning for Admission Control},
  Author                   = {Zheng, Wei and Sakellariou, Rizos},
  Journal                  = {Journal of Grid Computing},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {633--651},
  Volume                   = {11},

  Doi                      = {10.1007/s10723-013-9257-4},
  File                     = {:PDF/Zheng2013.pdf:PDF},
  ISSN                     = {1570-7873},
  Keywords                 = {Admission control; Bi-criteria DAG scheduling; SLA-based resource reservation; Workflow planning},
  Language                 = {English},
  Publisher                = {Springer Netherlands}
}

@InProceedings{Zhu2012,
  Title                    = {A cost-effective scheduling algorithm for scientific workflows in clouds},
  Author                   = {Mengxia Zhu and Qishi Wu and Yang Zhao},
  Booktitle                = {Performance Computing and Communications Conference (IPCCC), 2012 IEEE 31\textsuperscript{st} International},
  Year                     = {2012},
  Month                    = dec,
  Pages                    = {256--265},

  Abstract                 = {Cloud computing enables the delivery of computing, software, storage, and data access through web browsers as a metered service. In addition to commercial applications, an increasing number of large-scale workflow-based scientific applications are being supported by cloud computing. In order to meet the rapidly growing and dynamic computing demands of scientific users, the cloud service provider needs to employ efficient and cost-effective job schedulers to guarantee workflow completion time as well as improve resource utilization for high throughput. Based on rigorous cost models, we formulate a delay-constrained optimization problem to maximize resource utilization and propose a two-step workflow scheduling algorithm to minimize the cloud overhead within a user-specified execution time bound. The extensive simulation results illustrate that our approach consistently achieves lower computing overhead or higher resource utilization than existing methods within the execution time bound. Our approach also significantly reduces the total execution time by strategically selecting appropriate mapping nodes for prioritized modules.},
  Doi                      = {10.1109/PCCC.2012.6407766},
  File                     = {:PDF/Zhu2012.pdf:PDF},
  ISSN                     = {1097-2641},
  Keywords                 = {cloud computing;online front-ends;optimisation;resource allocation;scheduling;scientific information systems;Web browsers;cloud computing;cloud overhead minimization;cloud service provider;computing delivery;cost-effective job scheduling algorithm;data access;delay-constrained optimization problem;dynamic computing demands;large-scale workflow-based scientific applications;metered service;resource utilization improvement;scientific users;software delivery;total execution time reduction;two-step workflow scheduling algorithm;user-specified execution time bound;workflow completion time;Cloud computing;Computational modeling;Delay;Resource management;Scheduling algorithms;Virtual machining;Scientific workflow;cloud computing;workflow scheduling}
}

@TechReport{Zitzler2001,
  Title                    = {SPEA2: Improving the strength Pareto evolutionary algorithm},
  Author                   = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar and Zitzler, Eckart and Zitzler, Eckart and Thiele, Lothar and Thiele, Lothar},
  Institution              = {Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH), Institut f{\"u}r Technische Informatik und Kommunikationsnetze (TIK)},
  Year                     = {2001},

  File                     = {:PDF/Zitzler2001.pdf:PDF},
  Owner                    = {tefx},
  Publisher                = {{\ldots} Engineering and Networks Laboratory (TIK)},
  Timestamp                = {2015.05.13},
  Url                      = {http://scholar.google.com/scholar?q=related:bSRSr77srVYJ:scholar.google.com/&hl=en&num=20&as_sdt=0,5}
}

@Article{Zitzler1999,
  Title                    = {Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  Author                   = {Zitzler, E. and Thiele, L.},
  Journal                  = {Evolutionary Computation, IEEE Transactions on},
  Year                     = {1999},

  Month                    = nov,
  Number                   = {4},
  Pages                    = {257--271},
  Volume                   = {3},

  Abstract                 = {Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem},
  Doi                      = {10.1109/4235.797969},
  File                     = {:PDF/Zitzler1999.pdf:PDF},
  ISSN                     = {1089-778X},
  Keywords                 = {evolutionary computation;knapsack problems;optimisation;Pareto dominance relationship;clustering procedure;conflicting objectives;continuously updated population;digital hardware-software multiprocessor system;extended 0/1 knapsack problem;multiobjective evolutionary algorithms;multiobjective optimization;nondominated solutions;population diversity;strength Pareto approach;Computer aided software engineering;Computer architecture;Cost function;Evolutionary computation;Hardware;Multiprocessing systems;Pareto optimization;Sampling methods;Software systems;Space exploration}
}

@Book{Chi-KeongGoh2009,
  Title                    = {Multi-Objective Memetic Algorithms},
  Editor                   = {Chi-Keong Goh,Yew-Soon Ong,Kay Chen Tan},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2009},
  Series                   = {Studies in Computational Intelligence},
  Volume                   = {171}
}

