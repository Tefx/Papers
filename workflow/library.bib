% Encoding: UTF-8

@InProceedings{Catalyuerek2011,
  author    = {\c{C}ataly\"{u}rek, \"{U}mit V. and Kaya, Kamer and U\c{c}ar, Bora},
  title     = {Integrated Data Placement and Task Assignment for Scientific Workflows in Clouds},
  booktitle = {Proceedings of the Fourth International Workshop on Data-intensive Distributed Computing},
  year      = {2011},
  series    = {DIDC '11},
  publisher = {ACM},
  location  = {San Jose, California, USA},
  isbn      = {978-1-4503-0704-8},
  pages     = {45--54},
  doi       = {10.1145/1996014.1996022},
  acmid     = {1996022},
  address   = {New York, NY, USA},
  file      = {Catalyuerek2011.pdf:Catalyuerek2011 - Integrated Data Placement and Task Assignment for Scientific Workflows in Clouds.pdf:PDF},
  keywords  = {cloud computing, data placement, hypergraph partitioning, scientific workflows, task assignment},
  numpages  = {10},
}

@Article{Abrishami2012,
  author       = {S. Abrishami and M. Naghibzadeh},
  title        = {Deadline-constrained workflow scheduling in software as a service Cloud},
  journaltitle = {Scientia Iranica},
  year         = {2012},
  volume       = {19},
  number       = {3},
  pages        = {680--689},
  issn         = {1026-3098},
  doi          = {10.1016/j.scient.2011.11.047},
  abstract     = {The advent of Cloud computing as a new model of service provisioning in distributed systems, encourages researchers to investigate its benefits and drawbacks in executing scientific applications such as workflows. In this model, the users request for available services according to their desired Quality of Service, and they are charged on a pay-per-use basis. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS of the user as well as minimizing the cost of workflow execution. In this paper, we propose a new QoS-based workflow scheduling algorithm based on a novel concept called Partial Critical Paths (PCP), which tries to minimize the cost of workflow execution while meeting a user-defined deadline. This algorithm recursively schedules the partial critical paths ending at previously scheduled tasks. The simulation results show that the performance of our algorithm is very promising.},
  file         = {Abrishami2012.pdf:Abrishami2012 - Deadline-constrained workflow scheduling in software as a service Cloud.pdf:PDF},
  keywords     = {Cloud computing},
}

@Article{Abrishami2012a,
  author       = {Abrishami, S. and Naghibzadeh, M. and Epema, D.H.J.},
  title        = {Cost-Driven Scheduling of Grid Workflows Using Partial Critical Paths},
  journaltitle = {IEEE Trans. Parallel and Distributed Systems},
  year         = {2012},
  volume       = {23},
  number       = {8},
  month        = aug,
  pages        = {1400--1414},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2011.303},
  abstract     = {Recently, utility Grids have emerged as a new model of service provisioning in heterogeneous distributed systems. In this model, users negotiate with service providers on their required Quality of Service and on the corresponding price to reach a Service Level Agreement. One of the most challenging problems in utility Grids is workflow scheduling, i.e., the problem of satisfying the QoS of the users as well as minimizing the cost of workflow execution. In this paper, we propose a new QoS-based workflow scheduling algorithm based on a novel concept called Partial Critical Paths (PCP), that tries to minimize the cost of workflow execution while meeting a user-defined deadline. The PCP algorithm has two phases: in the deadline distribution phase it recursively assigns subdeadlines to the tasks on the partial critical paths ending at previously assigned tasks, and in the planning phase it assigns the cheapest service to each task while meeting its subdeadline. The simulation results show that the performance of the PCP algorithm is very promising.},
  file         = {Abrishami2012a.pdf:Abrishami2012a - Cost-Driven Scheduling of Grid Workflows Using Partial Critical Paths.pdf:PDF},
  keywords     = {grid computing;quality of service;scheduling;workflow management software;PCP algorithm;QoS-based workflow scheduling algorithm;cost-driven scheduling;deadline distribution phase;grid workflows;heterogeneous distributed systems;partial critical paths;planning phase;quality of service;service level agreement;service provisioning model;utility grids;Communities;Complexity theory;Quality of service;Schedules;Scheduling;Scheduling algorithm;Grid computing;QoS-based scheduling.;economic Grids;utility Grids;workflow scheduling},
}

@Article{Ahmad2016,
  author       = {Saima Gulzar Ahmad and Chee Sun Liew and Ehsan Ullah Munir and Tan Fong Ang and Samee U. Khan},
  title        = {A hybrid genetic algorithm for optimization of scheduling workflow applications in heterogeneous computing systems},
  journaltitle = {Journal of Parallel and Distributed Computing},
  year         = {2016},
  volume       = {87},
  pages        = {80 - 90},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2015.10.001},
  url          = {http://www.sciencedirect.com/science/article/pii/S0743731515001860},
  abstract     = {Abstract Workflow scheduling is a key component behind the process for an optimal workflow enactment. It is a well-known NP-hard problem and is more challenging in the heterogeneous computing environment. The increasing complexity of the workflow applications is forcing researchers to explore hybrid approaches to solve the workflow scheduling problem. The performance of genetic algorithms can be enhanced by the modification in genetic operators and involving an efficient heuristic. These features are incorporated in the proposed Hybrid Genetic Algorithm (HGA). A solution obtained from a heuristic is seeded in the initial population that provides a direction to reach an optimal (makespan)solution. The modified two fold genetic operators search rigorously and converge the algorithm at the best solution in less amount of time. This is proved to be the strength of the \{HGA\} in the optimization of fundamental objective (makespan) of scheduling. The proposed algorithm also optimizes the load balancing during the execution side to utilize resources at maximum. The performance of the proposed algorithm is analyzed by using synthesized datasets, and real-world application workflows. The \{HGA\} is evaluated by comparing the results with renowned and state of the art algorithms. The experimental results validate that the \{HGA\} outperforms these approaches and provides quality schedules with less makespans. },
  keywords     = {Workflow},
  owner        = {tefx},
  timestamp    = {2015.11.18},
}

@Electronic{Amazon2015,
  author    = {Amazon},
  title     = {Amazon EC2---FAQs},
  year      = {2015},
  url       = {http://goo.gl/43zeHN},
  owner     = {tefx},
  timestamp = {2015.07.27},
}

@Electronic{Amazon2014,
  Title                    = {Amazon EC2 Pricing},
  Author                   = {Amazon},
  Url                      = {http://goo.gl/yKb41s},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/yKb41s}
}

@InProceedings{Barrett2011,
  author    = {Barrett, E. and Howley, E. and Duggan, J.},
  title     = {A Learning Architecture for Scheduling Workflow Applications in the Cloud},
  booktitle = {Web Services (ECOWS), 2011 Ninth IEEE European Conference on},
  year      = {2011},
  month     = sep,
  pages     = {83--90},
  doi       = {10.1109/ECOWS.2011.27},
  abstract  = {The scheduling of workflow applications involves the mapping of individual workflow tasks to computational resources, based on a range of functional and non-functional quality of service requirements. Workflow applications such as scientific workflows often require extensive computational processing and generate significant amounts of experimental data. The emergence of cloud computing has introduced a utility-type market model, where computational resources of varying capacities can be procured on demand, in a pay-per-use fashion. In workflow based applications dependencies exist amongst tasks which requires the generation of schedules in accordance with defined precedence constraints. These constraints pose a difficult planning problem, where tasks must be scheduled for execution only once all their parent tasks have completed. In general the two most important objectives of workflow schedulers are the minimisation of both cost and make span. The cost of workflow execution consists of both computational costs incurred from processing individual tasks, and data transmission costs. With scientific workflows potentially large amounts of data must be transferred between compute and storage sites. This paper proposes a novel cloud workflow scheduling approach which employs a Markov Decision Process to optimally guide the workflow execution process depending on environmental state. In addition the system employs a genetic algorithm to evolve workflow schedules. The overall architecture is presented, and initial results indicate the potential of this approach for developing viable workflow schedules on the Cloud.},
  file      = {Barrett2011.pdf:Barrett2011 - A Learning Architecture for Scheduling Workflow Applications in the Cloud.pdf:PDF},
  keywords  = {Markov processes;cloud computing;genetic algorithms;minimisation;quality of service;scheduling;software architecture;storage management;workflow management software;Markov decision process;cloud computing;cloud workflow scheduling;computational costs;computational processing;computational resources;data transmission costs;environmental state;genetic algorithm;learning architecture;minimisation;pay-per-use fashion;precedence constraints;quality of service requirements;schedules;scheduling workflow applications;scientific workflows;storage sites;utility-type market model;workflow based applications dependency;workflow execution process;workflow schedulers;workflow tasks;Biological cells;Genetic algorithms;Markov processes;Optimal scheduling;Processor scheduling;Schedules;Scheduling;Bayesian Model Learning;Genetic Algorithm;Markov Decision Process;Workflow Scheduling},
}

@Article{Benoit2013,
  author       = {Benoit, Anne and \c{C}ataly\"{u}rek, \"{U}mit V. and Robert, Yves and Saule, Erik},
  title        = {A Survey of Pipelined Workflow Scheduling: Models and Algorithms},
  journaltitle = {ACM Computing Surv.},
  year         = {2013},
  volume       = {45},
  number       = {4},
  month        = aug,
  pages        = {50:1--50:36},
  issn         = {0360-0300},
  doi          = {10.1145/2501654.2501664},
  acmid        = {2501664},
  articleno    = {50},
  file         = {Benoit2013.pdf:Benoit2013 - A Survey of Pipelined Workflow Scheduling_ Models and Algorithms.pdf:PDF},
  issue_date   = {August 2013},
  keywords     = {Workflow programming, algorithms, distributed systems, filter-stream programming, latency, models, parallel systems, pipeline, scheduling, throughput},
  location     = {New York, NY, USA},
  numpages     = {36},
  publisher    = {ACM},
}

@InProceedings{Bessai2012,
  author    = {Bessai, K. and Youcef, S. and Oulamara, A. and Godart, Claude and Nurcan, S.},
  title     = {Bi-criteria Workflow Tasks Allocation and Scheduling in Cloud Computing Environments},
  booktitle = {Cloud Computing (CLOUD), 2012 IEEE 5\textsuperscript{th} International Conference on},
  year      = {2012},
  month     = jun,
  pages     = {638--645},
  doi       = {10.1109/CLOUD.2012.83},
  abstract  = {Although there are few efficient algorithms in the literature for scientific workflow tasks allocation and scheduling for heterogeneous resources such as those proposed in grid computing context, they usually require a bounded number of computer resources that cannot be applied in Cloud computing environment. Indeed, unlike grid, elastic computing, such asAmazon's EC2, allows users to allocate and release compute resources on-demand and pay only for what they use. Therefore, it is reasonable to assume that the number of resources is infinite. This feature of Clouds has been called âillusion of infiniteresourcesâ. However, despite the proven benefits of using Cloud to run scientific workflows, users lack guidance for choosing between multiple offering while taking into account several objectives which are often conflicting. On the other side, the workflow tasks allocation and scheduling have been shown to be NP-complete problems. Thus, it is convenient to use heuristic rather than deterministic algorithm. The objective of this paper is to design an allocation strategy for Cloud computing platform. More precisely, we propose three complementary bi-criteria approaches for scheduling workflows on distributed Cloud resources, taking into account the overall execution time and the cost incurred by using a set of resources.},
  file      = {Bessai2012.pdf:Bessai2012 - Bi-criteria Workflow Tasks Allocation and Scheduling in Cloud Computing Environments.pdf:PDF},
  issn      = {2159-6182},
  keywords  = {cloud computing;grid computing;optimisation;scheduling;workflow management software;NP-complete problems;allocation strategy;bi-criteria workflow tasks allocation;cloud computing environments;distributed Cloud resources;elastic computing;grid computing context;heterogeneous resources;workflow task scheduling;Cloud computing;Computational modeling;Processor scheduling;Resource management;Schedules;Scheduling;Virtual machining},
}

@InProceedings{Bharambe2013,
  author    = {Bharambe, U. and Durbha, S.S. and Kurte, K. and Younan, N.H. and King, R.L.},
  title     = {Pareto optimization for multiobjective matching of geospatial ontologies},
  booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
  year      = {2013},
  month     = jul,
  pages     = {1159--1162},
  doi       = {10.1109/IGARSS.2013.6721371},
  abstract  = {Geospatial information is different than conventional information. Harmonization is needed for interoperability and seamless access to data. Ontology matching is an emerging solution to achieve this harmonization. The input data of the Geospatial ontologies vary from the conventional ontologies and hence it is conceptualized in a different manner. There are two major obstacles for geoinformation fusion: heterogeneity and uncertainty. Heterogeneity is more prevalent and uncertainty is an unavoidable entity in geospatial domain. This paper explores a novel multi-objective algorithm for geospatial ontology matching. It uses Pareto ranking to sort the probable solution and derives the pareto front. This pareto front is used further to find the best match.},
  file      = {Bharambe2013.pdf:Bharambe2013 - Pareto optimization for multiobjective matching of geospatial ontologies.pdf:PDF},
  issn      = {2153-6996},
  keywords  = {Pareto optimisation;geophysics computing;ontologies (artificial intelligence);open systems;pattern matching;sensor fusion;Pareto front;Pareto optimization;Pareto ranking;geoinformation fusion;geospatial domain;geospatial information;geospatial ontology matching;harmonization;input data;interoperability;multiobjective algorithm;Atmospheric waves;Geospatial analysis;Interoperability;Ontologies;Pareto optimization;Semantics;Interoperability;Ontology Matching;Pareto Front;Pareto Ranking},
}

@InProceedings{Bharathi2008,
  author    = {Bharathi, S. and Chervenak, A. and Deelman, E. and Mehta, G. and Mei-Hui Su and Vahi, K.},
  title     = {Characterization of scientific workflows},
  booktitle = {Workflows in Support of Large-Scale Science, 2008. WORKS 2008. Third Workshop on},
  year      = {2008},
  month     = nov,
  pages     = {1--10},
  doi       = {10.1109/WORKS.2008.4723958},
  abstract  = {Researchers working on the planning, scheduling and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. We describe basic workflow structures that are composed into complex workflows by scientific communities. We provide a characterization of workflows from five diverse scientific applications, describing their composition and data and computational requirements. We also describe the effect of the size of the input datasets on the structure and execution profiles of these workflows. Finally, we describe a workflow generator that produces synthetic, parameterizable workflows that closely resemble the workflows that we characterize. We make these workflows available to the community to be used as benchmarks for evaluating various workflow systems and scheduling algorithms.},
  file      = {Bharathi2008.pdf:Bharathi2008 - Characterization of scientific workflows.pdf:PDF},
  keywords  = {data analysis;data structures;natural sciences computing;scheduling;workflow management software;computational requirement;data requirement;scientific workflow characterization;scientific workflow planning;scientific workflow scheduling;workflow execution profile;Astronomy;Biology;Character generation;Earthquakes;Geometry;Libraries;Performance analysis;Physics;Scheduling algorithm;Workflow management software},
}

@Article{Birman2009,
  author       = {Birman, Ken and Chockler, Gregory and van Renesse, Robbert},
  title        = {Toward a Cloud Computing Research Agenda},
  journaltitle = {SIGACT News},
  year         = {2009},
  volume       = {40},
  number       = {2},
  month        = jun,
  pages        = {68--80},
  issn         = {0163-5700},
  doi          = {10.1145/1556154.1556172},
  acmid        = {1556172},
  file         = {Birman2009.pdf:Birman2009 - Toward a Cloud Computing Research Agenda.pdf:PDF},
  issue_date   = {June 2009},
  location     = {New York, NY, USA},
  numpages     = {13},
  publisher    = {ACM},
}

@Article{Bittencourt2011,
  author       = {Bittencourt, LuizFernando and Madeira, EdmundoRobertoMauro},
  title        = {HCOC: a cost optimization algorithm for workflow scheduling in hybrid clouds},
  journaltitle = {J. Internet Services and Applications},
  year         = {2011},
  language     = {English},
  volume       = {2},
  number       = {3},
  pages        = {207--227},
  issn         = {1867-4828},
  doi          = {10.1007/s13174-011-0032-0},
  file         = {Bittencourt2011.pdf:Bittencourt2011 - HCOC_ a cost optimization algorithm for workflow scheduling in hybrid clouds.pdf:PDF},
  keywords     = {Workflow; Scheduling; DAG; Cloud computing},
  publisher    = {Springer-Verlag},
}

@InProceedings{Bittencourt2012,
  author    = {Bittencourt, L.F. and Madeira, E.R.M. and da Fonseca, N.L.S.},
  title     = {Impact of communication uncertainties on workflow scheduling in hybrid clouds},
  booktitle = {Global Communications Conference (GLOBECOM), 2012 IEEE},
  year      = {2012},
  month     = dec,
  pages     = {1623--1628},
  doi       = {10.1109/GLOCOM.2012.6503346},
  abstract  = {The so-called hybrid cloud is the composition of an infrastructure that comprises private resources as well as public resources leased from public clouds. Hybrid clouds can be utilized for the execution of applications composed of dependent jobs, usually modeled as workflows. In this scenario, a scheduler must distribute the components of the workflow onto available resources considering the communication demands and the available bandwidth in network links. However, such information can be imprecise, and consequently decisions on resource allocation can be ineffective. In this paper, we evaluate scheduling algorithms in the face of imprecise information on the availability of communication channels. Results showed that schedules are negatively affected by the unforeseen variations in bandwidth during the execution of the application.},
  file      = {Bittencourt2012.pdf:Bittencourt2012 - Impact of communication uncertainties on workflow scheduling in hybrid clouds.pdf:PDF},
  issn      = {1930-529X},
  keywords  = {cloud computing;resource allocation;scheduling;telecommunication channels;workflow management software;communication channels;communication uncertainties;hybrid clouds;network links;private resources;public clouds;public resources;resource allocation;scheduling algorithms;workflow scheduling},
}

@InCollection{Cai2013,
  author    = {Cai, Zhicheng and Li, Xiaoping and Gupta, JatinderN.D.},
  title     = {Critical Path-Based Iterative Heuristic for Workflow Scheduling in Utility and Cloud Computing},
  booktitle = {Service-Oriented Computing},
  year      = {2013},
  editor    = {Basu, Samik and Pautasso, Cesare and Zhang, Liang and Fu, Xiang},
  language  = {English},
  volume    = {8274},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-642-45004-4},
  pages     = {207--221},
  doi       = {10.1007/978-3-642-45005-1_15},
  file      = {Cai2013.pdf:Cai2013 - Critical Path-Based Iterative Heuristic for Workflow Scheduling in Utility and Cloud Computing.pdf:PDF},
  keywords  = {Cloud computing; workflow scheduling; utility computing; critical path; dynamic programming; multi-stage decision process},
}

@InProceedings{Cao2013,
  author    = {Fei Cao and Zhu, M.M.},
  title     = {Energy-Aware Workflow Job Scheduling for Green Clouds},
  booktitle = {Green Computing and Communications (GreenCom), 2013 IEEE and Internet of Things (iThings/CPSCom), IEEE International Conference on and IEEE Cyber, Physical and Social Computing},
  year      = {2013},
  month     = aug,
  pages     = {232--239},
  doi       = {10.1109/GreenCom-iThings-CPSCom.2013.58},
  abstract  = {With the increasing deployment of many data centers and computer servers around the globe, the energy cost on running the computing, communication and cooling together with the amount of CO2 emissions have increased dramatically. In order to maintain sustainable Cloud computing with ever-increasing problem scale, we design and develop energy-aware scientific workflow scheduling algorithm to minimize energy consumption and CO2 emission without sacrificing Quality of Service (QoS) such as response time specified in Service Level Agreement (SLA). The underlying available computing capacity and network bandwidth is represented as time-dependent because of the dual operation modes of on-demand and reservation instances supported by many commercial Cloud data centers. The Dynamic Voltage and Frequency Scaling (DVFS) is utilized to lower the CPU frequencies of virtual machines as long as the finishing time is still before the specified deadline. Our resource provision and allocation algorithm aims to meet the response time requirement and minimize the Virtual Machine (VM) overhead for reduced energy consumption. The consolidated VM reuse can lead to higher resource utilization rate for higher system throughput. The effectiveness of our algorithm is evaluated under various performance metrics and experimental scenarios using software adapted from open source CloudSim simulator. The simulation results show that our algorithm is able to achieve an average up to 30% of energy savings.},
  file      = {Cao2013.pdf:Cao2013 - Energy-Aware Workflow Job Scheduling for Green Clouds.pdf:PDF},
  keywords  = {air pollution;cloud computing;computer centres;contracts;energy consumption;green computing;performance evaluation;power aware computing;public domain software;quality of service;resource allocation;scheduling;sustainable development;virtual machines;workflow management software;CPU frequency;DVFS;QoS;SLA;VM overhead;allocation algorithm;carbon dioxide emissions;cloud data centers;computer servers;computing capacity;consolidated VM reuse;deployment;dual operation modes;dynamic voltage and frequency scaling;energy consumption;energy cost;energy-aware workflow job scheduling;finishing time;green clouds;higher system throughput;network bandwidth;open source CloudSim simulator;performance metric evaluation;quality of service;reservation instances;resource provision;resource utilization rate;response time requirement;service level agreement;sustainable cloud computing;virtual machine overhead;virtual machines;Clouds;Computational modeling;Cooling;Energy consumption;Green products;Resource management;Servers},
}

@InProceedings{Chard2015,
  Title                    = {Cost-Aware Elastic Cloud Provisioning for Scientific Workloads},
  Author                   = {Chard, R. and Chard, K. and Bubendorfer, K. and Lacinski, L. and Madduri, R. and Foster, I.},
  Booktitle                = {Cloud Computing (CLOUD), 2015 IEEE 8\textsuperscript{th} International Conference on},
  Year                     = {2015},
  Month                    = jun,
  Pages                    = {971--974},

  Abstract                 = {Cloud computing provides an efficient model to host and scale scientific applications. While cloud-based approaches can reduce costs as users pay only for the resources used, it is often challenging to scale execution both efficiently and cost-effectively. We describe here a cost-aware elastic cloud provisioner designed to elastically provision cloud infrastructure to execute analyses cost-effectively. The provisioner considers real-time spot instance prices across availability zones, leverages application profiles to optimize instance type selection, over-provisions resources to alleviate bottlenecks caused by oversubscribed instance types, and is capable of reverting to on-demand instances when spot prices exceed thresholds. We evaluate the usage of our cost-aware provisioner using four production scientific gateways and show that it can produce cost savings of up to 97.2% when compared to naive provisioning approaches.},
  Doi                      = {10.1109/CLOUD.2015.130},
  Keywords                 = {cloud computing;scientific information systems;software cost estimation;availability zones;cloud computing;cost savings;cost-aware elastic cloud provisioner;elastically provision cloud infrastructure design;instance type selection optimization;oversubscribed instance types;production scientific gateways;real-time spot instance prices;scientific workload;Cloud computing;Computational modeling;Logic gates;Monitoring;Pricing;Production;Real-time systems;Cloud;Cost-aware;Provisioning}
}

@InProceedings{Chopra2013,
  author    = {Chopra, N. and Singh, S.},
  title     = {Deadline and cost based workflow scheduling in hybrid cloud},
  booktitle = {Advances in Computing, Communications and Informatics (ICACCI), 2013 International Conference on},
  year      = {2013},
  month     = aug,
  pages     = {840--846},
  doi       = {10.1109/ICACCI.2013.6637285},
  abstract  = {Cloud computing provides on demand resources for compute and storage requirements. Private cloud is a good option for cost saving for executing workflow applications but when the resources in private cloud are not enough to meet storage and compute requirements of an application then public clouds are the option left. While public clouds charge users on pay-per-use basis, private clouds are owned by users and can be utilized with no charge. When a public cloud and a private cloud is merged, we get a hybrid cloud. In hybrid cloud, task scheduling is a complex process as jobs can be allocated resources either from private cloud or from public cloud. Deadline based scheduling is the main focus in many of the workflow applications. Proposed algorithm does cost optimization by deciding which resources should be taken on lease from public cloud to complete the workflow execution within deadline. In the proposed work, we have developed a level based scheduling algorithm which executes tasks level wise and it uses the concept of sub-deadline which is helpful in finding best resources on public cloud for cost saving and also completes workflow execution within deadlines. Performance analysis and comparison of the proposed algorithm with min-min approach is also presented.},
  file      = {Chopra2013.pdf:Chopra2013 - Deadline and cost based workflow scheduling in hybrid cloud.pdf:PDF},
  keywords  = {cloud computing;performance evaluation;processor scheduling;resource allocation;storage management;cloud computing;complex process;cost based workflow scheduling;deadline based workflow scheduling;hybrid cloud;level based scheduling algorithm;level wise task execution;min-min approach;on demand resources;pay-per-use basis charge;performance analysis;private cloud;resource allocation;storage requirements;subdeadline concept;task scheduling;workflow applications;Cloud computing;Hardware;Organizations;Schedules;Scheduling;Scheduling algorithms;Cloud Computing;DAG;Hybrid cloud;Private cloud;Public cloud;Workflow Scheduling},
}

@Book{Coello2004,
  Title                    = {Applications of multi-objective evolutionary algorithms},
  Author                   = {Coello, Carlos A Coello and Lamont, Gary B},
  Publisher                = {World Scientific},
  Year                     = {2004},
  Volume                   = {1}
}

@Book{Coello2007,
  Title                    = {Evolutionary algorithms for solving multi-objective problems},
  Author                   = {Coello, Carlos Coello and Lamont, Gary B and Van Veldhuizen, David A},
  Publisher                = {Springer Science \& Business Media},
  Year                     = {2007}
}

@Book{Deb2001,
  Title                    = {Multi-objective optimization using evolutionary algorithms},
  Author                   = {Kalyanmoy Deb},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2001},
  Series                   = {Wiley Interscience Series in Systems and Optimization},
  Volume                   = {16}
}

@Article{Delimitrou2013,
  author       = {Delimitrou, Christina and Kozyrakis, Christos},
  title        = {QoS-Aware Scheduling in Heterogeneous Datacenters with Paragon},
  journaltitle = {ACM Trans. Computing System},
  year         = {2013},
  volume       = {31},
  number       = {4},
  month        = dec,
  pages        = {12:1--12:34},
  issn         = {0734-2071},
  doi          = {10.1145/2556583},
  acmid        = {2556583},
  articleno    = {12},
  file         = {Delimitrou2013.pdf:Delimitrou2013 - QoS-Aware Scheduling in Heterogeneous Datacenters with Paragon.pdf:PDF},
  issue_date   = {December 2013},
  keywords     = {Datacenter, QoS, cloud computing, heterogeneity, interference, resource-efficiency, scheduling},
  location     = {New York, NY, USA},
  numpages     = {34},
  publisher    = {ACM},
}

@InProceedings{Dongarra2007,
  author    = {Dongarra, Jack J. and Jeannot, Emmanuel and Saule, Erik and Shi, Zhiao},
  title     = {Bi-objective Scheduling Algorithms for Optimizing Makespan and Reliability on Heterogeneous Systems},
  booktitle = {Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures},
  year      = {2007},
  series    = {SPAA '07},
  publisher = {ACM},
  location  = {San Diego, California, USA},
  isbn      = {978-1-59593-667-7},
  pages     = {280--288},
  doi       = {10.1145/1248377.1248423},
  acmid     = {1248423},
  address   = {New York, NY, USA},
  file      = {Dongarra2007.pdf:Dongarra2007 - Bi-objective Scheduling Algorithms for Optimizing Makespan and Reliability on Heterogeneous Systems.pdf:PDF},
  keywords  = {DAG, pareto-curve, reliability, scheduling},
  numpages  = {9},
}

@Article{Durillo2014,
  author       = {Juan J. Durillo and Vlad Nae and Radu Prodan},
  title        = {Multi-objective energy-efficient workflow scheduling using list-based heuristics},
  journaltitle = {Future Generation Computer Systems},
  year         = {2014},
  volume       = {36},
  number       = {0},
  pages        = {221--236},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2013.07.005},
  abstract     = {Abstract Workflow applications are a popular paradigm used by scientists for modelling applications to be run on heterogeneous high-performance parallel and distributed computing systems. Today, the increase in the number and heterogeneity of multi-core parallel systems facilitates the access to high-performance computing to almost every scientist, yet entailing additional challenges to be addressed. One of the critical problems today is the power required for operating these systems for both environmental and financial reasons. To decrease the energy consumption in heterogeneous systems, different methods such as energy-efficient scheduling are receiving increasing attention. Current schedulers are, however, based on simplistic energy models not matching the reality, use techniques like \{DVFS\} not available on all types of systems, or do not approach the problem as a multi-objective optimisation considering both performance and energy as simultaneous objectives. In this paper, we present a new Pareto-based multi-objective workflow scheduling algorithm as an extension to an existing state-of-the-art heuristic capable of computing a set of tradeoff optimal solutions in terms of makespan and energy efficiency. Our approach is based on empirical models which capture the real behaviour of energy consumption in heterogeneous parallel systems. We compare our new approach with a classical mono-objective scheduling heuristic and state-of-the-art multi-objective optimisation algorithm and demonstrate that it computes better or similar results in different scenarios. We analyse the different tradeoff solutions computed by our algorithm under different experimental configurations and we observe that in some cases it finds solutions which reduce the energy consumption by up to 34.5% with a slight increase of 2% in the makespan. },
  file         = {Durillo2014a.pdf:Durillo2014 - Multi-objective energy-efficient workflow scheduling using list-based heuristics.pdf:PDF},
  keywords     = {Energy-efficient scheduling;Workflow scheduling;Multi-objective optimisation},
}

@Article{Falco2014,
  author       = {I. De Falco and U. Scafuri and E. Tarantino},
  title        = {Two new fast heuristics for mapping parallel applications on cloud computing},
  journaltitle = {Future Generation Computer Systems},
  year         = {2014},
  volume       = {37},
  number       = {0},
  pages        = {1--13},
  note         = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2014.02.019},
  abstract     = {Abstract In this paper two new heuristics, named Min–min-C and Max–min-C, are proposed able to provide near-optimal solutions to the mapping of parallel applications, modeled as Task Interaction Graphs, on computational clouds. The aim of these heuristics is to determine mapping solutions which allow exploiting at best the available cloud resources to execute such applications concurrently with the other cloud services. Differently from their originating Min–min and Max–min models, the two introduced heuristics take also communications into account. Their effectiveness is assessed on a set of artificial mapping problems differing in applications and in node working conditions. The analysis, carried out also by means of statistical tests, reveals the robustness of the two algorithms proposed in coping with the mapping of small- and medium-sized high performance computing applications on non-dedicated cloud nodes. },
  file         = {Falco2014.pdf:Falco2014 - Two new fast heuristics for mapping parallel applications on cloud computing.pdf:PDF},
  keywords     = {Cloud computing;Mapping;Communicating tasks;Heuristics },
}

@Article{Fard2014,
  author       = {Hamid Mohammadi Fard and Radu Prodan and Thomas Fahringer},
  title        = {Multi-objective list scheduling of workflow applications in distributed computing infrastructures},
  journaltitle = {J. Parallel and Distributed Computing},
  year         = {2014},
  volume       = {74},
  number       = {3},
  pages        = {2152--2165},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2013.12.004},
  abstract     = {Abstract Executing large-scale applications in distributed computing infrastructures (DCI), for example modern Cloud environments, involves optimization of several conflicting objectives such as makespan, reliability, energy, or economic cost. Despite this trend, scheduling in heterogeneous \{DCIs\} has been traditionally approached as a single or bi-criteria optimization problem. In this paper, we propose a generic multi-objective optimization framework supported by a list scheduling heuristic for scientific workflows in heterogeneous DCIs. The algorithm approximates the optimal solution by considering user-specified constraints on objectives in a dual strategy: maximizing the distance to the user’s constraints for dominant solutions and minimizing it otherwise. We instantiate the framework and algorithm for a four-objective case study comprising makespan, economic cost, energy consumption, and reliability as optimization goals. We implemented our method as part of the \{ASKALON\} environment (Fahringer et al., 2007) for Grid and Cloud computing and demonstrate through extensive real and synthetic simulation experiments that our algorithm outperforms related bi-criteria heuristics while meeting the user constraints most of the time. },
  file         = {Fard2014.pdf:Fard2014 - Multi-objective list scheduling of workflow applications in distributed computing infrastructures.pdf:PDF},
  keywords     = {Multi-objective scheduling;Scientific workflows;Distributed computing infrastructures },
}

@Article{Fonseca1995,
  author       = {Fonseca, Carlos M and Fleming, Peter J},
  title        = {An overview of evolutionary algorithms in multiobjective optimization},
  journaltitle = {Evolutionary Computation},
  year         = {1995},
  volume       = {3},
  number       = {1},
  pages        = {1--16},
  file         = {Fonseca1995.pdf:Fonseca1995 - An overview of evolutionary algorithms in multiobjective optimization.pdf:PDF},
  publisher    = {MIT Press},
}

@InProceedings{Foster2008,
  author    = {Foster, I. and Yong Zhao and Raicu, I. and Shiyong Lu},
  title     = {Cloud Computing and Grid Computing 360-Degree Compared},
  booktitle = {Grid Computing Environments Workshop, 2008. GCE '08},
  year      = {2008},
  month     = nov,
  pages     = {1--10},
  doi       = {10.1109/GCE.2008.4738445},
  abstract  = {Cloud computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for cloud computing and there seems to be no consensus on what a cloud is. On the other hand, cloud computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established grid computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast cloud computing with grid computing from various angles and give insights into the essential characteristics of both.},
  file      = {Foster2008.pdf:Foster2008 - Cloud Computing and Grid Computing 360-Degree Compared.pdf:PDF},
  keywords  = {grid computing;cloud computing;cluster computing;distributed system;grid computing;utility computing;Cloud computing;Computer science;Computer vision;Costs;Distributed computing;Economies of scale;Grid computing;Laboratories;Large-scale systems;Standards organizations},
}

@InProceedings{Frincu2011,
  author    = {Frincu, M.E. and Craciun, C.},
  title     = {Multi-objective Meta-heuristics for Scheduling Applications with High Availability Requirements and Cost Constraints in Multi-Cloud Environments},
  booktitle = {Utility and Cloud Computing (UCC), 2011 Fourth IEEE International Conference on},
  year      = {2011},
  month     = dec,
  pages     = {267--274},
  doi       = {10.1109/UCC.2011.43},
  abstract  = {As the popularity of cloud computing increases, more and more applications are migrated onto them. Web 2.0 applications are the most common example of such applications. These applications require to scale, be highly available, fault tolerant and able to run uninterrupted for long periods of time (or even indefinitely). Moreover as new cloud providers appear there is a natural tendency towards choosing the best provider or a combination of them for deploying the application. Thus multi-cloud scenarios emerge from this situation. However, as multi-cloud resource provisioning is both complex and costly, the choice of which resources to lend and how to allocate them to application components needs to rely on efficient strategies. These need to take into account many factors including deployment and run-time cost, resource load, and application availability in case of failures. For this aim multi-objective scheduling algorithms seem an appropriate choice. This paper presents an algorithm which tries to achieve application high-availability and fault-tolerance while reducing the application cost and keeping the resource load maximized. The proposed algorithm is compared with a classic Round Robin strategy - used by many commercial clouds - and the obtained results prove the efficiency of our solution.},
  file      = {Frincu2011.pdf:Frincu2011 - Multi-objective Meta-heuristics for Scheduling Applications with High Availability Requirements and Cost Constraints in Multi-Cloud Environments.pdf:PDF},
  keywords  = {cloud computing;fault tolerant computing;processor scheduling;resource allocation;Round Robin strategy;Web 2.0 applications;cloud computing;fault tolerance;high availability requirements;multicloud resource provisioning;multiobjective metaheuristics;multiobjective scheduling algorithms;resource load maximization;Availability;Cloud computing;Databases;Generators;Graphics;Processor scheduling;Schedules;cloud scheduling;meta-heuristics;multi-objective scheduling},
}

@InCollection{Garg2011,
  author    = {Garg, Ritu and Singh, Awadhesh Kumar},
  title     = {Multi-objective workflow grid scheduling based on discrete particle swarm optimization},
  booktitle = {Swarm, Evolutionary, and Memetic Comput.},
  year      = {2011},
  publisher = {Springer},
  pages     = {183--190},
  file      = {Garg2011.pdf:Garg2011 - Multi-objective workflow grid scheduling based on discrete particle swarm optimization.pdf:PDF},
  owner     = {tefx},
  timestamp = {2015.10.10},
}

@Article{GhorbanniaDelavar2014,
  author       = {Ghorbannia Delavar, Arash and Aryan, Yalda},
  title        = {HSGA: a hybrid heuristic algorithm for workflow scheduling in cloud systems},
  journaltitle = {Cluster Computing},
  year         = {2014},
  language     = {English},
  volume       = {17},
  number       = {1},
  pages        = {129--137},
  issn         = {1386-7857},
  doi          = {10.1007/s10586-013-0275-6},
  file         = {GhorbanniaDelavar2014.pdf:GhorbanniaDelavar2014 - HSGA_ a hybrid heuristic algorithm for workflow scheduling in cloud systems.pdf:PDF},
  keywords     = {Heterogeneous distributed computing systems; Cloud computing; Workflow scheduling; Heuristic; Genetic Algorithm},
  publisher    = {Springer US},
}

@Article{Hirales-Carbajal2012,
  author       = {Hirales-Carbajal, Adán and Tchernykh, Andrei and Yahyapour, Ramin and González-García, JoséLuis and Röblitz, Thomas and Ramírez-Alcaraz, JuanManuel},
  title        = {Multiple Workflow Scheduling Strategies with User Run Time Estimates on a Grid},
  journaltitle = {J. Grid Computing},
  year         = {2012},
  language     = {English},
  volume       = {10},
  number       = {2},
  pages        = {325--346},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-012-9215-6},
  file         = {Hirales-Carbajal2012.pdf:Hirales-Carbajal2012 - Multiple Workflow Scheduling Strategies with User Run Time Estimates on a Grid.pdf:PDF},
  keywords     = {Grid computing; Workflow scheduling; Resource management; User run time estimate},
  publisher    = {Springer Netherlands},
}

@InProceedings{Ilyushkin2015,
  Title                    = {Scheduling Workloads of Workflows with Unknown Task Runtimes},
  Author                   = {Ilyushkin, Alexey and Ghit, Bogdan and Epema, Dick},
  Booktitle                = {Cluster, Cloud and Grid Computing (CCGrid), 2015 15\textsuperscript{th} IEEE/ACM International Symposium on},
  Year                     = {2015},
  Month                    = may,
  Pages                    = {606--616},

  Abstract                 = {Workflows are important computational tools in many branches of science, and because of the dependencies among their tasks and their widely different characteristics, scheduling them is a difficult problem. Most research on scheduling workflows has focused on the offline problem of minimizing the make span of single workflows with known task runtimes. The problem of scheduling multiple workflows has been addressed either in an offline fashion, or still with the assumption of known task runtimes. In this paper, we study the problem of scheduling workloads consisting of an arrival stream of workflows without task runtime estimates. The resource requirements of a workflow can significantly fluctuate during its execution. Thus, we present four scheduling policies for workloads of workflows with as their main feature the extent to which they reserve processors to workflows to deal with these fluctuations. We perform simulations with realistic synthetic workloads and we show that any form of processor reservation only decreases the overall system performance and that a greedy backfilling-like policy performs best.},
  Doi                      = {10.1109/CCGrid.2015.27},
  Keywords                 = {Approximation methods;Periodic structures;Processor scheduling;Program processors;Runtime;Schedules;Scheduling;backfilling;e-Science;online;reservation;scheduling;workflows;workloads}
}

@InProceedings{Janetschek2013,
  author    = {Janetschek, M. and Ostermann, S. and Prodan, R.},
  title     = {Bringing Scientific Workflows to Amazon SWF},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2013 39\textsuperscript{th} EUROMICRO Conference on},
  year      = {2013},
  month     = sep,
  pages     = {389--396},
  doi       = {10.1109/SEAA.2013.13},
  abstract  = {In response to the ever-increasing needs of scientific applications for resources, Cloud computing emerged as an alternative on-demand and cost-effective resource provisioning approach. In this context, Cloud providers have recognised the importance of workflow applications to science and provide their own native solutions, such as the Amazon Simple Workflow Service (SWF). Nevertheless, an important downside of SWF is its incompatibility with existing workflow systems, and lack of means for reusing scientific legacy code. Similarly, existing workflow middlewares and applications require non-trivial extensions to take advantage of Cloud resources. We present in this paper a software engineering solution that allows the scientific workflow community access the Amazon Cloud through one single front-end converter, and propose a legacy wrapper service for executing legacy code using SWF. Empirical results using a real-world scientific workflow demonstrate that our automatically generated SWF application performs almost as fast as a native manually-optimised version, and outperforms other workflow middleware systems using the Amazon Cloud.},
  file      = {Janetschek2013.pdf:Janetschek2013 - Bringing Scientific Workflows to Amazon SWF.pdf:PDF},
  keywords  = {cloud computing;middleware;natural sciences computing;program compilers;resource allocation;software maintenance;software reusability;workflow management software;Amazon SWF;Amazon cloud;Amazon simple workflow service;automatic SWF application generation;cloud computing;cloud providers;cloud resources;cost-effective resource provisioning;front-end converter;legacy code execution;legacy wrapper service;scientific applications;scientific legacy code reusability;scientific workflow community;scientific workflows;software engineering solution;workflow applications;workflow middlewares;workflow systems;Cloud computing;History;Java;Ports (Computers);Semantics;Amazon SWF;cloud computing;legacy code;scientific workflows;workflow converter},
}

@InProceedings{Janetschek2015,
  author    = {Janetschek, Matthias and Prodan, Radu and Benedict, Shajulin},
  title     = {A Workflow Runtime Environment for Manycore Parallel Architectures},
  booktitle = {Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science},
  year      = {2015},
  series    = {WORKS '15},
  publisher = {ACM},
  location  = {Austin, Texas},
  isbn      = {978-1-4503-3989-6},
  pages     = {1:1--1:12},
  doi       = {10.1145/2822332.2822333},
  acmid     = {2822333},
  address   = {New York, NY, USA},
  articleno = {1},
  keywords  = {enactment engine, heterogeneous manycore parallel architectures, scientific workflows, source-to-source compiler},
  numpages  = {12},
}

@InProceedings{Ji2013,
  author    = {Haoran Ji and Weidong Bao and Xiaomin Zhu and Shu Yin},
  title     = {Profit-Oriented Scheduling Optimization for Workflow in Clouds},
  booktitle = {High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing (HPCC_EUC), 2013 IEEE 10\textsuperscript{th} International Conference on},
  year      = {2013},
  month     = nov,
  pages     = {1922--1929},
  doi       = {10.1109/HPCC.and.EUC.2013.276},
  abstract  = {Clouds have become a new paradigm by enabling on-demand provisioning of applications, platforms or computing resources for clients. Workflow scheduling is one of the most challenging problems in Clouds. Getting more profits is one of the most important objectives in workflow scheduling. Conventional workflow scheduling strategies developed on the kind of systems mainly focus on the workflow. In this paper, we take the communication into account and develop a novel scheduling algorithm based on the topology characters of degree and path length named Music Chair Algorithm (MCA). The algorithm gives a good performance on searching for the optimum schedule in getting the most profit. Also, we find that there exists a certain resource amount, which gets the most profit to help us get more enthusiasm for further developing the Clouds. Experimental results demonstrate that the analysis of the strategies for most profits are reasonable, and the MCA is available to efficiently get the optimum schedule with low computing complexity.},
  file      = {Ji2013.pdf:Ji2013 - Profit-Oriented Scheduling Optimization for Workflow in Clouds.pdf:PDF},
  keywords  = {cloud computing;computational complexity;optimisation;scheduling;search problems;MCA;cloud computing;computing complexity;degree topology characters;music chair algorithm;on-demand application provisioning;on-demand computing resource provisioning;on-demand platform provisioning;optimum schedule search;path length;profit-oriented workflow scheduling optimization;Computational modeling;Optimal scheduling;Processor scheduling;Schedules;Scheduling;Topology;DAG;topologic characteristics;workflow scheduling},
}

@Article{Jiao2015,
  author       = {Jiao, Hejun and Zhang, Jing and Li, JunHuai and Shi, Jinfa and Li, Jian},
  title        = {Immune optimization of task scheduling on multidimensional {QoS} constraints},
  journaltitle = {Cluster Computing},
  year         = {2015},
  language     = {English},
  pages        = {1--10},
  issn         = {1386-7857},
  doi          = {10.1007/s10586-015-0447-7},
  file         = {Jiao2015.pdf:Jiao2015 - Immune optimization of task scheduling on multidimensional QoS constraints.pdf:PDF},
  keywords     = {Cloud computing; Multiple QoS parameter constraint; Immune optimization; Application preference; Task scheduling},
  publisher    = {Springer US},
}

@InProceedings{Jrad2013,
  author    = {Jrad, Foued and Tao, Jie and Streit, Achim},
  title     = {A Broker-based Framework for Multi-cloud Workflows},
  booktitle = {Proceedings of the 2013 International Workshop on Multi-cloud Applications and Federated Clouds},
  year      = {2013},
  series    = {MultiCloud '13},
  publisher = {ACM},
  location  = {Prague, Czech Republic},
  isbn      = {978-1-4503-2050-4},
  pages     = {61--68},
  doi       = {10.1145/2462326.2462339},
  acmid     = {2462339},
  address   = {New York, NY, USA},
  file      = {Jrad2013.pdf:Jrad2013 - A Broker-based Framework for Multi-cloud Workflows.pdf:PDF},
  keywords  = {cloud broker, cloud computing, cloud workflow, intercloud computing, multi-cloud},
  numpages  = {8},
}

@InProceedings{Jung2013,
  author    = {Gueyoung Jung and Hyunjoo Kim},
  title     = {Optimal Time-Cost Tradeoff of Parallel Service Workflow in Federated Heterogeneous Clouds},
  booktitle = {Web Services (ICWS), 2013 IEEE 20\textsuperscript{th} International Conference on},
  year      = {2013},
  month     = jun,
  pages     = {499--506},
  doi       = {10.1109/ICWS.2013.73},
  abstract  = {Federated cloud enables a workflow to be deployed in multiple private and public clouds. By facilitating external cloud-based services to execute sub-tasks of the workflow, service workflow owners can reduce the cost of executing the workflow, while meeting a performance requirement, since those cloud-based services can be more cost efficient and have better performance than internal ones. However, due to inter-dependencies between sub-tasks, the complexity of the workflow, and the heterogeneity of clouds, it is a challenge to achieve the optimal tradeoff between cost and performance. This paper presents a novel workflow scheduler designed to achieve the optimal end-to-end execution time and cost when deploying such complex workflows in heterogeneous computing nodes in clouds. Specifically, our scheduling algorithm addresses the tradeoff between the execution cost, the computing time, and the data transfer delay between sub-tasks. Our scheduler can handle complex workflows that contain recursively paralleled sub-flows caused by branch and merging sub-tasks. Experiments indicate that our scheduler can efficiently compute the near optimal deployment compared with greedy and evolutionary algorithms for both end-to-end execution time and corresponding cost.},
  file      = {Jung2013.pdf:Jung2013 - Optimal Time-Cost Tradeoff of Parallel Service Workflow in Federated Heterogeneous Clouds.pdf:PDF},
  keywords  = {cloud computing;scheduling;clouds heterogeneity;data transfer;end-to-end execution time;external cloud-based services;federated heterogeneous clouds;heterogeneous computing nodes;optimal time-cost tradeoff;parallel service workflow;private clouds;public clouds;sub-tasks interdependencies;workflow complexity;workflow scheduler;Abstracts;Cloud computing;Data transfer;Delays;Equations;Heuristic algorithms;Merging;cloud;optimization;parallel workflow;tradeoff},
}

@Article{Juve2013,
  author       = {Gideon Juve and Ann Chervenak and Ewa Deelman and Shishir Bharathi and Gaurang Mehta and Karan Vahi},
  title        = {Characterizing and profiling scientific workflows},
  journaltitle = {Future Generation Computer Systems},
  year         = {2013},
  volume       = {29},
  number       = {3},
  pages        = {682--692},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2012.08.015},
  abstract     = {Researchers working on the planning, scheduling, and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. This paper provides a characterization of workflows from six diverse scientific applications, including astronomy, bioinformatics, earthquake science, and gravitational-wave physics. The characterization is based on novel workflow profiling tools that provide detailed information about the various computational tasks that are present in the workflow. This information includes I/O, memory and computational characteristics. Although the workflows are diverse, there is evidence that each workflow has a job type that consumes the most amount of runtime. The study also uncovered inefficiency in a workflow component implementation, where the component was re-reading the same data multiple times. },
  file         = {:PDF/Juve2013a.pdf:PDF;Juve2013.pdf:Juve2013 - Characterizing and profiling scientific workflows.pdf:PDF;Juve2013.pdf:PDF/Juve2013.pdf:PDF},
  keywords     = {Scientific workflows;Profiling;Performance;Measurement },
}

@InCollection{Juve2011,
  author    = {Juve, Gideon and Deelman, Ewa},
  title     = {Scientific Workflows in the Cloud},
  booktitle = {Grids, Clouds and Virtualization},
  year      = {2011},
  editor    = {Cafaro, Massimo and Aloisio, Giovanni},
  language  = {English},
  series    = {Computer Communications and Networks},
  publisher = {Springer London},
  isbn      = {978-0-85729-048-9},
  pages     = {71--91},
  doi       = {10.1007/978-0-85729-049-6_4},
  file      = {Juve2011.pdf:Juve2011 - Scientific Workflows in the Cloud.pdf:PDF},
}

@Article{Khajemohammadi2014,
  author       = {Khajemohammadi, Hassan and Fanian, Ali and Gulliver, T.Aaron},
  title        = {Efficient Workflow Scheduling for Grid Computing Using a Leveled Multi-objective Genetic Algorithm},
  journaltitle = {J. Grid Computing},
  year         = {2014},
  language     = {English},
  volume       = {12},
  number       = {4},
  pages        = {637--663},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-014-9306-7},
  file         = {Khajemohammadi2014.pdf:Khajemohammadi2014 - Efficient Workflow Scheduling for Grid Computing Using a Leveled Multi-objective Genetic Algorithm.pdf:PDF},
  keywords     = {Workflow scheduling; Genetic algorithm; Multi-objective optimization; Grid computing},
  publisher    = {Springer Netherlands},
}

@InProceedings{Kllapi2011,
  author    = {Kllapi, Herald and Sitaridi, Eva and Tsangaris, Manolis M. and Ioannidis, Yannis},
  title     = {Schedule Optimization for Data Processing Flows on the Cloud},
  booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
  year      = {2011},
  series    = {SIGMOD '11},
  publisher = {ACM},
  location  = {Athens, Greece},
  isbn      = {978-1-4503-0661-4},
  pages     = {289--300},
  doi       = {10.1145/1989323.1989355},
  acmid     = {1989355},
  address   = {New York, NY, USA},
  file      = {Kllapi2011.pdf:Kllapi2011 - Schedule Optimization for Data Processing Flows on the Cloud.pdf:PDF},
  keywords  = {cloud computing, dataflows, query optimization, scheduling},
  numpages  = {12},
}

@Article{Li2015,
  author       = {Li, Jing and Liu, Lei and Wu, Yuan and Feng, Xiaobing and Wu, Chengyong},
  title        = {Two-Level Task Scheduling for Irregular Applications on GPU Platform},
  journaltitle = {International Journal of Parallel Programming},
  year         = {2015},
  language     = {English},
  pages        = {1-15},
  issn         = {0885-7458},
  doi          = {10.1007/s10766-015-0387-0},
  keywords     = {Hierarchical schedule; Resource-aware; Irregular application; GPU},
  owner        = {tefx},
  publisher    = {Springer US},
  timestamp    = {2015.11.18},
}

@Article{Li2011,
  author       = {Li, Jiandun and Peng, Junjie and Lei, Zhou and Zhang, Wu},
  title        = {An energy-efficient scheduling approach based on private clouds},
  journaltitle = {J. Information \& Computational Science},
  year         = {2011},
  volume       = {8},
  number       = {4},
  pages        = {716--724},
  file         = {Li2011.pdf:Li2011 - An energy-efficient scheduling approach based on private clouds.pdf:PDF},
}

@Article{Li2014,
  author       = {Miqing Li and Shengxiang Yang and Ke Li and Xiaohui Liu},
  title        = {Evolutionary Algorithms With Segment-Based Search for Multiobjective Optimization Problems},
  journaltitle = {IEEE Trans. Cybernetics},
  year         = {2014},
  volume       = {44},
  number       = {8},
  month        = aug,
  pages        = {1295--1313},
  issn         = {2168-2267},
  doi          = {10.1109/TCYB.2013.2282503},
  abstract     = {This paper proposes a variation operator, called segment-based search (SBS), to improve the performance of evolutionary algorithms on continuous multiobjective optimization problems. SBS divides the search space into many small segments according to the evolutionary information feedback from the set of current optimal solutions. Two operations, micro-jumping and macro-jumping, are implemented upon these segments in order to guide an efficient information exchange among “good” individuals. Moreover, the running of SBS is adaptive according to the current evolutionary status. SBS is activated only when the population evolves slowly, depending on general genetic operators (e.g., mutation and crossover). A comprehensive set of 36 test problems is employed for experimental verification. The influence of two algorithm settings (i.e., the dimensionality and boundary relaxation strategy) and two probability parameters in SBS (i.e., the SBS rate and micro-jumping proportion) are investigated in detail. Moreover, an empirical comparative study with three representative variation operators is carried out. Experimental results show that the incorporation of SBS into the optimization process can improve the performance of evolutionary algorithms for multiobjective optimization problems.},
  file         = {Li2014.pdf:Li2014 - Evolutionary Algorithms With Segment-Based Search for Multiobjective Optimization Problems.pdf:PDF},
  keywords     = {evolutionary computation;mathematical operators;optimisation;search problems;SBS;evolutionary algorithms;evolutionary information feedback;genetic operators;multiobjective optimization problems;representative variation operators;segment-based search;Convergence;Genetics;Optimization;Scattering;Search problems;Sociology;Statistics;Hybrid evolutionary algorithms;multiobjective optimization;segment-based search;variation operators},
}

@Article{Li2014a,
  author       = {Li, Miqing and Yang, Shengxiang and Zheng, Jinhua and Liu, Xiaohui},
  title        = {Etea: A Euclidean Minimum Spanning Tree-based Evolutionary Algorithm for Multi-objective Optimization},
  journaltitle = {Evolutionary Computation},
  year         = {2014},
  volume       = {22},
  number       = {2},
  month        = jun,
  pages        = {189--230},
  issn         = {1063-6560},
  doi          = {10.1162/EVCO_a_00106},
  acmid        = {2645287},
  file         = {Li2014b.pdf:Li2014a - Etea_ A Euclidean Minimum Spanning Tree-based Evolutionary Algorithm for Multi-objective Optimization.pdf:PDF},
  issue_date   = {Summer 2014},
  keywords     = {Euclidean minimum spanning tree, Multi-objective optimization, archive truncation, density estimation, evolutionary algorithms, fitness adjustment, fitness assignment},
  location     = {Cambridge, MA, USA},
  numpages     = {42},
  publisher    = {MIT Press},
}

@InProceedings{Lin2011,
  author    = {Cui Lin and Shiyong Lu},
  title     = {Scheduling Scientific Workflows Elastically for Cloud Computing},
  booktitle = {Cloud Computing (CLOUD), 2011 IEEE International Conference on},
  year      = {2011},
  month     = jul,
  pages     = {746--747},
  doi       = {10.1109/CLOUD.2011.110},
  abstract  = {Most existing workflow scheduling algorithms only consider a computing environment in which the number of compute resources is bounded. Compute resources in such an environment usually cannot be provisioned or released on demand of the size of a workflow, and these resources are not released to the environment until an execution of the workflow completes. To address the problem, we firstly formalize a model of a Cloud environment and a workflow graph representation for such an environment. Then, we propose the SHEFT workflow scheduling algorithm to schedule a workflow elastically on a Cloud computing environment. Our preliminary experiments show that SHEFT not only outperforms several representative workflow scheduling algorithms in optimizing workflow execution time, but also enables resources to scale elastically at runtime.},
  file      = {Lin2011.pdf:Lin2011 - Scheduling Scientific Workflows Elastically for Cloud Computing.pdf:PDF},
  issn      = {2159-6182},
  keywords  = {cloud computing;graph theory;scheduling;SHEFT workflow scheduling algorithm;cloud computing;cloud environment;computing environment;scheduling scientific workflows elastically;workflow graph representation;Cloud computing;Computational modeling;Data models;Runtime;Schedules;Scheduling algorithm;Workflow scheduling;cloud computing;elastic scaling;heterogeneous environment},
}

@InProceedings{Liu2008,
  author    = {Yang Liu},
  title     = {A fast and elitist multi-objective particle swarm algorithm: NSPSO},
  booktitle = {Granular Computing, 2008. GrC 2008. IEEE International Conference on},
  year      = {2008},
  month     = aug,
  pages     = {470--475},
  doi       = {10.1109/GRC.2008.4664711},
  abstract  = {In this paper, a new nondominated sorting particle swarm optimisation (NSPSO), is proposed, that combines the operations (fast ranking of non-dominated solutions, crowding distance ranking and elitist strategy of combining parent population and offspring population together) of a known MOGA NSGA-II and the other advanced operations (selection and mutation operations) with a single particle swarm optimiser (PSO). The efficacy of this algorithm is demonstrated on 2 test functions, and the comparison is made with the NSGA-II and a multi-objective PSO (MOPSO-CD). The simulation results suggest that the proposed optimisation framework is able to achieve good solutions as well diversity compared to NSGA-II and MOPSO-CD optimisation framework.},
  file      = {Liu2008.pdf:Liu2008 - A fast and elitist multi-objective particle swarm algorithm_ NSPSO.pdf:PDF},
  keywords  = {particle swarm optimisation;crowding distance ranking;elitist strategy;multiobjective particle swarm algorithm;mutation operations;nondominated solution ranking;nondominated sorting particle swarm optimisation;offspring population;parent population;Birds;Diversity reception;Educational institutions;Genetic algorithms;Genetic mutations;Marine animals;Pareto optimization;Particle swarm optimization;Sorting;Testing},
}

@Article{Luo2015,
  author       = {Luo, Huimin and Yan, Chaokun and Hu, Zhigang},
  title        = {An Enhanced Workflow Scheduling Strategy for Deadline Guarantee on Hybrid Grid/Cloud Infrastructure},
  journaltitle = {J. Applied Science and Engineering},
  year         = {2015},
  volume       = {18},
  number       = {1},
  pages        = {67--78},
  file         = {Luo2015.pdf:Luo2015 - An Enhanced Workflow Scheduling Strategy for Deadline Guarantee on Hybrid Grid_Cloud Infrastructure.pdf:PDF},
}

@InProceedings{Man2013,
  author    = {Man, Nguyen Doan and Huh, Eui-Nam},
  title     = {Cost and efficiency-based scheduling on a general framework combining between cloud computing and local thick clients},
  booktitle = {Computing, Management and Telecommunications (ComManTel), 2013 International Conference on},
  year      = {2013},
  month     = jan,
  pages     = {258--263},
  doi       = {10.1109/ComManTel.2013.6482401},
  abstract  = {Today, the rapid growth of the large-scale appli-cations conduces to a difficult challenge to individuals as well as the commercial organizations due to the limitation of the local computing resources. Meanwhile, the extension of the local computing platforms requires a huge investment about both finance and human power. Therefore, the use of nearly-unlimited resources and on-demand scaling of Cloud computing is considered as a potential solution to address the problems above. However, since Cloud computing is built from a pay-as-you-go model, the novel application scheduling approaches are challenged to guarantee the high efficiency of application execution and the reasonable cost for renting Cloud resources. In this paper, we present a novel framework built from the combination between the computing resources on Cloud computing and the computing components in the local systems. The key component of this framework is the Cost with Finish Time-based scheduling algorithm, which provides the balance between performance of application schedule and the mandatory cost for the use of Cloud resources. The experiments and comparison with the other scheduling approaches demonstrated the potential benefits of our proposed algorithm.},
  file      = {Man2013.pdf:Man2013 - Cost and efficiency-based scheduling on a general framework combining between cloud computing and local thick clients.pdf:PDF},
  keywords  = {Cloud computing;Computational modeling;Data transfer;Schedules;Scheduling;Scheduling algorithms},
}

@InProceedings{Martens2012,
  author    = {Martens, B. and Walterbusch, M. and Teuteberg, F.},
  title     = {Costing of Cloud Computing Services: A Total Cost of Ownership Approach},
  booktitle = {System Science (HICSS), 2012 45\textsuperscript{th} Hawaii International Conference on},
  year      = {2012},
  month     = jan,
  pages     = {1563--1572},
  doi       = {10.1109/HICSS.2012.186},
  abstract  = {The use of Cloud Computing Services appears to offer significant cost advantages. Particularly start-up companies benefit from these advantages, since frequently they do not operate an internal IT infrastructure. But are costs associated with Cloud Computing Services really that low? We found that particular cost types and factors are frequently underestimated by practitioners. In this paper we present a Total Cost of Ownership (TCO) approach for Cloud Computing Services. We applied a multi-method approach (systematic literature review, analysis of real Cloud Computing Services, expert interview, case study) for the development and evaluation of the formal mathematical model. We found that our model fits the practical requirements and supports decision-making in Cloud Computing.},
  file      = {Martens2012.pdf:Martens2012 - Costing of Cloud Computing Services_ A Total Cost of Ownership Approach.pdf:PDF},
  issn      = {1530-1605},
  keywords  = {cloud computing;costing;decision making;cloud computing service;cost type;decision-making;formal mathematical model;multimethod approach;start-up companies;total cost of ownership;Analytical models;Cloud computing;Companies;Computational modeling;Mathematical model;Pricing;Systematics;Cloud Computing;Cloud Computing Services;Costing;Total Cost of Ownership},
}

@Article{Mell2009,
  author       = {Mell, Peter and Grance, Tim},
  title        = {The NIST definition of cloud computing},
  journaltitle = {National Institute of Standards and Technology},
  year         = {2009},
  volume       = {53},
  number       = {6},
  pages        = {50},
  file         = {Mell2009.pdf:Mell2009 - The NIST definition of cloud computing.pdf:PDF},
  institution  = {National Institute of Standards and Technology},
  publisher    = {IGI Publication},
}

@Electronic{Microsoft2014,
  Title                    = {Virtual Machines Pricing Details},
  Author                   = {Microsoft},
  Url                      = {http://goo.gl/UrDkvF},
  Year                     = {2014},

  Bdsk-url-1               = {http://goo.gl/UrDkvF},
  Timestamp                = {2015.03.18}
}

@Article{Nesmachnow2013,
  author       = {Nesmachnow, Sergio and Dorronsoro, Bernabé and Pecero, JohnatanE. and Bouvry, Pascal},
  title        = {Energy-Aware Scheduling on Multicore Heterogeneous Grid Computing Systems},
  journaltitle = {J. Grid Computing},
  year         = {2013},
  language     = {English},
  volume       = {11},
  number       = {4},
  pages        = {653--680},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-013-9258-3},
  file         = {Nesmachnow2013.pdf:Nesmachnow2013 - Energy-Aware Scheduling on Multicore Heterogeneous Grid Computing Systems.pdf:PDF},
  keywords     = {Grid scheduling; Energy-aware; Heterogeneous computing},
  publisher    = {Springer Netherlands},
}

@Article{Oliveira2012,
  author       = {de Oliveira, Daniel and Ocaña, KaryA.C.S. and Baião, Fernanda and Mattoso, Marta},
  title        = {A Provenance-based Adaptive Scheduling Heuristic for Parallel Scientific Workflows in Clouds},
  journaltitle = {J. Grid Computing},
  year         = {2012},
  language     = {English},
  volume       = {10},
  number       = {3},
  pages        = {521--552},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-012-9227-2},
  file         = {Oliveira2012.pdf:Oliveira2012 - A Provenance-based Adaptive Scheduling Heuristic for Parallel Scientific Workflows in Clouds.pdf:PDF},
  keywords     = {Cloud computing; Scientific workflow; Scientific experiment; Provenance},
  publisher    = {Springer Netherlands},
}

@InCollection{Ostermann2012,
  author    = {Ostermann, Simon and Prodan, Radu},
  title     = {Impact of Variable Priced Cloud Resources on Scientific Workflow Scheduling},
  booktitle = {Euro-Par 2012 Parallel Processing},
  year      = {2012},
  editor    = {Kaklamanis, Christos and Papatheodorou, Theodore and Spirakis, PaulG.},
  language  = {English},
  volume    = {7484},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-642-32819-0},
  pages     = {350--362},
  doi       = {10.1007/978-3-642-32820-6_35},
  file      = {Ostermann2012.pdf:Ostermann2012 - Impact of Variable Priced Cloud Resources on Scientific Workflow Scheduling.pdf:PDF},
  keywords  = {Cloud computing; Grid computing; Spot instances; Scheduling; Scientific workflows; Performance; Cost},
}

@InProceedings{Pandey2010,
  author    = {Pandey, S. and Linlin Wu and Guru, S.M. and Buyya, R.},
  title     = {A Particle Swarm Optimization-Based Heuristic for Scheduling Workflow Applications in Cloud Computing Environments},
  booktitle = {Advanced Information Networking and Applications (AINA), 2010 24\textsuperscript{th} IEEE International Conference on},
  year      = {2010},
  month     = apr,
  pages     = {400--407},
  doi       = {10.1109/AINA.2010.31},
  abstract  = {Cloud computing environments facilitate applications by providing virtualized resources that can be provisioned dynamically. However, users are charged on a pay-per-use basis. User applications may incur large data retrieval and execution costs when they are scheduled taking into account only the `execution time'. In addition to optimizing execution time, the cost arising from data transfers between resources as well as execution costs must also be taken into account. In this paper, we present a particle swarm optimization (PSO) based heuristic to schedule applications to cloud resources that takes into account both computation cost and data transmission cost. We experiment with a workflow application by varying its computation and communication costs. We compare the cost savings when using PSO and existing `Best Resource Selection' (BRS) algorithm. Our results show that PSO can achieve: (a) as much as 3 times cost savings as compared to BRS, and (b) good distribution of workload onto resources.},
  file      = {Pandey2010.pdf:Pandey2010 - A Particle Swarm Optimization-Based Heuristic for Scheduling Workflow Applications in Cloud Computing Environments.pdf:PDF},
  issn      = {1550-445X},
  keywords  = {Internet;groupware;particle swarm optimisation;BRS algorithm;PSO-based heuristic;best resource selection;cloud computing;collaborative scientific experiments;computation cost;data retrieval;data transmission cost;execution cost;execution time;particle swarm optimization;workflow applications scheduling;Application software;Application virtualization;Cloud computing;Computational efficiency;Computer science;Cost function;Dynamic scheduling;Particle swarm optimization;Processor scheduling;Software engineering;Cloud computing;Workflow scheduling;particle swarm optimization},
}

@InProceedings{Phan2012,
  author    = {Phan, Dung H. and Suzuki, Junichi and Carroll, Raymond and Balasubramaniam, Sasitharan and Donnelly, William and Botvich, Dmitri},
  title     = {Evolutionary Multiobjective Optimization for Green Clouds},
  booktitle = {Proceedings of the 14\textsuperscript{th} Annual Conference Companion on Genetic and Evolutionary Computation},
  year      = {2012},
  series    = {GECCO '12},
  publisher = {ACM},
  location  = {Philadelphia, Pennsylvania, USA},
  isbn      = {978-1-4503-1178-6},
  pages     = {19--26},
  doi       = {10.1145/2330784.2330788},
  acmid     = {2330788},
  address   = {New York, NY, USA},
  file      = {Phan2012.pdf:Phan2012 - Evolutionary Multiobjective Optimization for Green Clouds.pdf:PDF},
  keywords  = {cloud computing, evolutionary multiobjective optimization, internet data centers, renewable energy, sustainability},
  numpages  = {8},
}

@InProceedings{Qian2015,
  Title                    = {Emerald: Enhance scientific workflow performance with computation offloading to the cloud},
  Author                   = {Qian, Hao and Andresen, Daniel},
  Booktitle                = {Computer and Information Science (ICIS), 2015 IEEE/ACIS 14\textsuperscript{th} International Conference on},
  Year                     = {2015},
  Month                    = jun,
  Pages                    = {443--448},

  Abstract                 = {Scientific computational experiments often span multiple computational and analytical steps, and during execution, researchers need to store, access, transfer, and query information. Scientific workflow is a powerful tool to streamline and organize scientific application. Numbers of tools have been developed to help build scientific workflows, they provide mechanisms for creating workflow but lack a native scheduling system for determining where code should be executed. This paper presents Emerald, a system that adds sophisticated computation offloading capabilities to scientific workflows. Emerald automatically offloads computation intensive steps of scientific workflow to the cloud in order to enhance workflow performance. Emerald minimizes the burden on developers to build workflows with computation offloading ability by providing easy-to-use API. Evaluation showed that Emerald can effectively reduce up to 55% of execution time for scientific applications.},
  Doi                      = {10.1109/ICIS.2015.7166634},
  Keywords                 = {Computational modeling;Computers;Data models;Mathematical model;Processor scheduling;Runtime;Synchronization;cloud computing;code offloading;distributed computing;scheduling;scientific workflow}
}

@InProceedings{Raghavan2015,
  author    = {Raghavan, S. and Marimuthu, C. and Sarwesh, P. and Chandrasekaran, K.},
  title     = {Bat algorithm for scheduling workflow applications in cloud},
  booktitle = {Electronic Design, Computer Networks Automated Verification (EDCAV), 2015 International Conference on},
  year      = {2015},
  month     = jan,
  pages     = {139--144},
  doi       = {10.1109/EDCAV.2015.7060555},
  abstract  = {Workflow is one of the important aspects of cloud computing today. Cloud computing is one of the fastest growing technologies in the world. Workflows can be used in cloud as we use them in grid. Many operations in the cloud are based on workflow execution. Workflow systems are now becoming more complex and for such kind of systems efficient workflow management is important. Workflow scheduling is an important part of workflow management. Scheduling in general is NP-hard problem. To solve such kind of problems exhaustive methods cannot be used. Only non-exhaustive techniques can be used. In this paper we have used a metaheuristic approach called bat algorithm. Bat algorithm is specifically designed for optimizing hard problems. Here, bat algorithm with the help of binary bat algorithm is used for scheduling workflow in a cloud. Specifically the mapping of tasks and resources is done using this method. The optimal resources are selected such that the overall cost of the workflow is minimal.},
  file      = {Raghavan2015.pdf:Raghavan2015 - Bat algorithm for scheduling workflow applications in cloud.pdf:PDF},
  keywords  = {cloud computing;optimisation;NP-hard problem;binary bat algorithm;cloud computing;metaheuristic approach;workflow execution;workflow management;workflow scheduling;Algorithm design and analysis;Cloud computing;Heuristic algorithms;Optimization;Scheduling;Scheduling algorithms;Bat Algorithm;Binary Bat Algorithm (BBA);Cloud Workflow Scheduling;Workflow Scheduling},
}

@InProceedings{Rahman2011,
  author    = {Rahman, M. and Xiaorong Li and Palit, H.},
  title     = {Hybrid Heuristic for Scheduling Data Analytics Workflow Applications in Hybrid Cloud Environment},
  booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on},
  year      = {2011},
  month     = may,
  pages     = {966--974},
  doi       = {10.1109/IPDPS.2011.243},
  abstract  = {Effective scheduling is a key concern for the execution of performance driven applications, such as workflows in dynamic and cost driven environment including Cloud. The majority of existing scheduling techniques are based on meta-heuristics that produce good schedules with advance reservation given the current state of Cloud services or heuristics that are dynamic in nature, and map the workflow tasks to services on-the-fly, but lack the ability of generating schedules considering workflow-level optimization and user QoS constraints. In this paper, we propose an Adaptive Hybrid Heuristic for user constrained data-analytics workflow scheduling in hybrid Cloud environment by integrating the dynamic nature of heuristic based approaches as well as workflow-level optimization capability of meta-heuristic based approaches. The effectiveness of the proposed approach is illustrated by a comprehensive case study with comparison to existing techniques.},
  file      = {Rahman2011.pdf:Rahman2011 - Hybrid Heuristic for Scheduling Data Analytics Workflow Applications in Hybrid Cloud Environment.pdf:PDF},
  issn      = {1530-2075},
  keywords  = {cloud computing;data analysis;scheduling;QoS constraint;adaptive hybrid heuristic;cloud services;cost driven environment;data analytics workflow application scheduling;data-analytics workflow scheduling;hybrid cloud environment;meta-heuristics;scheduling technique;workflow-level optimization capability;Cloud computing;Dynamic scheduling;Optimization;Processor scheduling;Quality of service;Schedules},
}

@Article{Rodriguez2014,
  author       = {Rodriguez, M.A. and Buyya, R.},
  title        = {Deadline Based Resource Provisioningand Scheduling Algorithm for Scientific Workflows on Clouds},
  journaltitle = {IEEE Trans. Cloud Computing},
  year         = {2014},
  volume       = {2},
  number       = {2},
  month        = apr,
  pages        = {222--235},
  issn         = {2168-7161},
  doi          = {10.1109/TCC.2014.2314655},
  abstract     = {Cloud computing is the latest distributed computing paradigm and it offers tremendous opportunities to solve large-scale scientific problems. However, it presents various challenges that need to be addressed in order to be efficiently utilized for workflow applications. Although the workflow scheduling problem has been widely studied, there are very few initiatives tailored for cloud environments. Furthermore, the existing works fail to either meet the user's quality of service (QoS) requirements or to incorporate some basic principles of cloud computing such as the elasticity and heterogeneity of the computing resources. This paper proposes a resource provisioning and scheduling strategy for scientific workflows on Infrastructure as a Service (IaaS) clouds. We present an algorithm based on the meta-heuristic optimization technique, particle swarm optimization (PSO), which aims to minimize the overall workflow execution cost while meeting deadline constraints. Our heuristic is evaluated using CloudSim and various well-known scientific workflows of different sizes. The results show that our approach performs better than the current state-of-the-art algorithms.},
  file         = {Rodriguez2014.pdf:Rodriguez2014 - Deadline Based Resource Provisioningand Scheduling Algorithm for Scientific Workflows on Clouds.pdf:PDF},
  keywords     = {cloud computing;cost reduction;natural sciences computing;particle swarm optimisation;quality of service;resource allocation;scheduling;workflow management software;CloudSim;IaaS clouds;PSO;QoS;cloud computing;computing resources elasticity;computing resources heterogeneity;deadline based resource provisioning;deadline constraints;distributed computing paradigm;infrastructure as a service clouds;meta-heuristic optimization technique;particle swarm optimization;scheduling algorithm;scientific workflows;user quality of service;workflow execution cost minimization;Cloud computing;Computational modeling;Computer applications;Distributed processing;Mathematical model;Processor scheduling;Quality of service;Cloud computing;resource provisioning;scheduling;scientific workflow},
}

@InCollection{Sakellariou2007,
  author    = {Sakellariou, Rizos and Zhao, Henan and Tsiakkouri, Eleni and Dikaiakos, MariosD.},
  title     = {Scheduling Workflows with Budget Constraints},
  booktitle = {Integrated Research in GRID Computing},
  year      = {2007},
  editor    = {Gorlatch, Sergei and Danelutto, Marco},
  language  = {English},
  publisher = {Springer US},
  isbn      = {978-0-387-47656-8},
  pages     = {189--202},
  doi       = {10.1007/978-0-387-47658-2_14},
  file      = {Sakellariou2007.pdf:Sakellariou2007 - Scheduling Workflows with Budget Constraints.pdf:PDF},
  keywords  = {Workflows; Scheduling; Budget Constrained Scheduling; DAG Scheduling},
}

@Article{Salimi2014,
  author       = {Reza Salimi and Homayun Motameni and Hesam Omranpour},
  title        = {Task scheduling using \{NSGA\} \{II\} with fuzzy adaptive operators for computational grids},
  journaltitle = {J. Parallel and Distributed Computing},
  year         = {2014},
  volume       = {74},
  number       = {5},
  pages        = {2333--2350},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2014.01.006},
  abstract     = {Abstract Scheduling algorithms have an essential role in computational grids for managing jobs, and assigning them to appropriate resources. An efficient task scheduling algorithm can achieve minimum execution time and maximum resource utilization by providing the load balance between resources in the grid. The superiority of genetic algorithm in the scheduling of tasks has been proven in the literature. In this paper, we improve the famous multi-objective genetic algorithm known as NSGA-II using fuzzy operators to improve quality and performance of task scheduling in the market-based grid environment. Load balancing, Makespan and Price are three important objectives for multi-objective optimization in the task scheduling problem in the grid. Grid users do not attend load balancing in making decision, so it is desirable that all solutions have good load balancing. Thus to decrease computation and ease decision making through the users, we should consider and improve the load balancing problem in the task scheduling indirectly using the fuzzy system without implementing the third objective function. We have used fuzzy operators for this purpose and more quality and variety in Pareto-optimal solutions. Three functions are defined to generate inputs for fuzzy systems. Variance of costs, variance of frequency of involved resources in scheduling and variance of genes values are used to determine probabilities of crossover and mutation intelligently. Variance of frequency of involved resources with cooperation of Makespan objective satisfies load balancing objective indirectly. Variance of genes values and variance of costs are used in the mutation fuzzy system to improve diversity and quality of Pareto optimal front. Our method conducts the algorithm towards best and most appropriate solutions with load balancing in less iteration. The obtained results have proved that our innovative algorithm converges to Pareto-optimal solutions faster and with more quality. },
  file         = {Salimi2014.pdf:Salimi2014 - Task scheduling using _NSGA_ _II_ with fuzzy adaptive operators for computational grids.pdf:PDF},
  keywords     = {Task scheduling,Load balancing,Grid computing,Non-dominated sorting genetic algorithm \{II\},Variance-based fuzzy operators,Multi-objective optimization },
}

@Article{Garfinkel2007,
  author   = {Simson L Garfinkel, Simson L Garfinkel},
  title    = {An Evaluation of Amazon's Grid Computing Services: EC2, S3, and SQS},
  year     = {2007},
  url      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.172.2239},
  file     = {SimsonLGarfinkel2007.pdf:Garfinkel2007 - An Evaluation of Amazon's Grid Computing Services_ EC2, S3, and SQS.pdf:PDF},
  keywords = {Cloud, Evaluation},
}

@Article{Smanchat2015,
  author       = {Sucha Smanchat and Kanchana Viriyapant},
  title        = {Taxonomies of workflow scheduling problem and techniques in the cloud},
  journaltitle = {Future Generation Computer Systems},
  year         = {2015},
  number       = {0},
  pages        = {-},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2015.04.019},
  abstract     = {Abstract Scientific workflows, like other applications, benefit from the cloud computing, which offers access to virtually unlimited resources provisioned elastically on demand. In order to efficiently execute a workflow in the cloud, scheduling is required to address many new aspects introduced by cloud resource provisioning. In the last few years, many techniques have been proposed to tackle different cloud environments enabled by the flexible nature of the cloud, leading to the techniques of different designs. In this paper, taxonomies of cloud workflow scheduling problem and techniques are proposed based on analytical review. We identify and explain the aspects and classifications unique to workflow scheduling in the cloud environment in three categories, namely, scheduling process, task and resource. Lastly, review of several scheduling techniques are included and classified onto the proposed taxonomies. We hope that our taxonomies serve as a stepping stone for those entering this research area and for further development of scheduling technique. },
  file         = {Smanchat2015.pdf:Smanchat2015 - Taxonomies of workflow scheduling problem and techniques in the cloud.pdf:PDF},
  keywords     = {Cloud computing;Cloud workflow;Workflow scheduling;Workflow scheduling taxonomy },
}

@Article{Somasundaram2014,
  author       = {Thamarai Selvi Somasundaram and Kannan Govindarajan},
  title        = {CLOUDRB: A framework for scheduling and managing High-Performance Computing (HPC) applications in science cloud},
  journaltitle = {Future Generation Computer Systems},
  year         = {2014},
  volume       = {34},
  number       = {0},
  pages        = {47--65},
  note         = {Special Section: Distributed Solutions for Ubiquitous Computing and Ambient Intelligence},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2013.12.024},
  abstract     = {Abstract In recent years, the Cloud environment has played a major role in running High-Performance Computing (HPC) applications, which are computationally intensive and data intensive in nature. The High-Performance Computing Cloud (HPCC) or Science Cloud (SC) provides the resources to these types of applications in an on demand and scalable manner. Scheduling of jobs or applications in a Cloud environment is NP-Complete and complex in nature due to the dynamicity of resources and on demand user application requirements. The main motivation behind this research study is to design and develop a \{CLOUD\} Resource Broker (CLOUDRB) for efficiently managing cloud resources and completing jobs for scientific applications within a user-specified deadline. It is implemented and integrated with a Deadline-based Job Scheduling and Particle Swarm Optimization (PSO)-based Resource Allocation mechanism. Our proposed approach intends to achieve the objectives of minimizing both execution time and cost based on the defined fitness function. It is simulated by modeling the \{HPC\} jobs and Cloud resources using the Matlab programming environment. The simulation results prove the effectiveness of the proposed research work by minimizing the completion time, cost and job rejection ratio and maximizing the number of jobs completing their applications within a deadline and meeting the user’s satisfaction. The proposed work has been tested in our Eucalyptus-based cloud environments by submitting real-world \{HPC\} applications and observed the improvements in performance. },
  file         = {Somasundaram2014.pdf:Somasundaram2014 - CLOUDRB_ A framework for scheduling and managing High-Performance Computing (HPC) applications in science cloud.pdf:PDF},
  keywords     = {Cloud computing;High Performance Computing (HPC);\{CLOUD\} Resource Broker (CLOUDRB);Resource allocation;Job scheduling;Particle Swarm Optimization (PSO);Science cloud },
}

@InProceedings{Sridhar2015,
  Title                    = {Hybrid Particle Swarm Optimization scheduling for cloud computing},
  Author                   = {Sridhar, M. and Babu, G.R.M.},
  Booktitle                = {Advance Computing Conference (IACC), 2015 IEEE International},
  Year                     = {2015},
  Month                    = jun,
  Pages                    = {1196--1200},

  Abstract                 = {Cloud computing is revolutionizing on how information technology is used by organizations and individuals. Cloud computing provides dynamic services with virtualized resources over the Internet. It ensures facilities to develop, deploy, and manage applications `on the cloud' for end users entailing resources virtualization that maintains and manages itself. Scheduling is a task performed to get maximum profit to increase cloud computing work load efficiency. Its objective is using resources properly and managing load between resources with minimum execution time. High communication cost incurs in clouds prevent task schedulers from being applied in a large scale distributed environment. This study proposes a hybrid Particle Swarm Optimization (PSO) which performs better in execution ratio and average schedule length.},
  Doi                      = {10.1109/IADCC.2015.7154892},
  Keywords                 = {cloud computing;particle swarm optimisation;scheduling;virtualisation;Internet;PSO;average schedule length;cloud computing work load efficiency;communication cost;execution ratio;hybrid particle swarm optimization scheduling;large scale distributed environment;maximum profit;minimum execution time;resources virtualization;task schedulers;virtualized resources;Cloud computing;Conferences;Optimal scheduling;Schedules;Scheduling algorithms;Cloud computing;execution ratio and average schedule length;hybrid Particle Swarm Optimization (PSO);task scheduling}
}

@InProceedings{Stavrinides2015,
  author    = {Stavrinides, G.L. and Karatza, H.D.},
  title     = {A Cost-Effective and QoS-Aware Approach to Scheduling Real-Time Workflow Applications in PaaS and SaaS Clouds},
  booktitle = {Future Internet of Things and Cloud (FiCloud), 2015 3rd International Conference on},
  year      = {2015},
  month     = {Aug},
  pages     = {231-239},
  doi       = {10.1109/FiCloud.2015.93},
  abstract  = {The ever increasing popularity of cloud computing has relieved many consumers and businesses from the burden of acquiring, maintaining and monitoring expensive hardware and software infrastructure. In this paper, we focus on Platform as a Service (PaaS) and Software as a Service (SaaS) clouds, where users submit their workflow applications in order to be executed within strict timing constraints. It is assumed that the target cloud platform is based on a multi-tenant approach, where applications of different users may share the same virtual machines. We propose a list scheduling heuristic for the scheduling of real-time workflow applications in a heterogeneous PaaS (or SaaS) cloud that incorporates imprecise computations and bin packing techniques. Our scheduling approach has two objectives: (a) to guarantee that all applications will meet their deadline, providing high quality results and (b) to minimize the execution time of each workflow application and thus the cost charged to the user. The proposed approach is compared to a baseline list scheduling algorithm via simulation, for workflow applications with various communication to computation ratios. The simulation results show that the proposed scheduling strategy outperforms the baseline policy, providing promising results.},
  keywords  = {cloud computing;quality of service;scheduling;virtual machines;PaaS clouds;QoS-aware approach;SaaS clouds;baseline list scheduling algorithm;cost-effective approach;high quality results;multitenant approach;platform as a service;real-time workflow application scheduling;scheduling strategy;software as a service;strict timing constraints;virtual machines;workflow applications;Platform as a service;Processor scheduling;Quality of service;Real-time systems;Schedules;Software as a service;Real-time workflow scheduling;bin packing;cloud computing;execution cost;heterogeneous virtual machines;imprecise computations;quality of service;schedule gaps},
}

@Article{Su2013,
  author       = {Sen Su and Jian Li and Qingjia Huang and Xiao Huang and Kai Shuang and Jie Wang},
  title        = {Cost-efficient task scheduling for executing large programs in the cloud},
  journaltitle = {Parallel Computing},
  year         = {2013},
  volume       = {39},
  number       = {4-5},
  pages        = {177--188},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2013.03.002},
  abstract     = {Abstract Executing a large program using clouds is a promising approach, as this class of programs may be decomposed into multiple sequences of tasks that can be executed on multiple virtual machines (VMs) in a cloud. Such sequences of tasks can be represented as a directed acyclic graph (DAG), where nodes are tasks and edges are precedence constraints between tasks. Cloud users pay for what their programs actually use according to the pricing models of the cloud providers. Early task scheduling algorithms are focused on minimizing makespan, without mechanisms to reduce the monetary cost incurred in the setting of clouds. We present a cost-efficient task-scheduling algorithm using two heuristic strategies.The first strategy dynamically maps tasks to the most cost-efficient \{VMs\} based on the concept of Pareto dominance. The second strategy, a complement to the first strategy, reduces the monetary costs of non-critical tasks. We carry out extensive numerical experiments on large \{DAGs\} generated at random as well as on real applications. The simulation results show that our algorithm can substantially reduce monetary costs while producing makespan as good as the best known task-scheduling algorithm can provide. },
  keywords     = {Cloud computing,Cost-efficient scheduling,Parallel task scheduling,Directed acyclic graph },
}

@InProceedings{Szabo2012,
  author    = {Szabo, C. and Kroeger, T.},
  title     = {Evolving multi-objective strategies for task allocation of scientific workflows on public clouds},
  booktitle = {Evolutionary Computation (CEC), 2012 IEEE Congress on},
  year      = {2012},
  month     = jun,
  pages     = {1--8},
  doi       = {10.1109/CEC.2012.6256556},
  abstract  = {With the increase in deployment of scientific application on public and private clouds, the allocation of workflow tasks to specific cloud instances to reduce runtime and cost has emerged as an important challenge. The allocation of scientific workflows on public clouds can be described through a variety of perspectives and parameters and has been proved to be NP-complete. This paper presents an optimization framework for task allocation on public clouds. We present a solution that considers important parameters such as workflow runtime, communication overhead, and overall execution cost. Our multi-objective optimization framework builds on a simple and extensible cost model and uses a heuristic to determine the optimal number of cloud instances to be used. Using the Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) as an example, we show how our optimization heuristics lead to significantly better strategies than other state-of-the-art approaches. Specifically, our single-objective optimization is slightly better than a simple heuristic and a particle swarm optimization approach for small workflows, and achieves significant improvements for larger workflows. In a similar manner, our multi-objective optimization obtains similar results to our single-objective optimization for small-size workflows, and achieves up to 80% improvement for large-size workflows.},
  file      = {Szabo2012.pdf:Szabo2012 - Evolving multi-objective strategies for task allocation of scientific workflows on public clouds.pdf:PDF},
  keywords  = {cloud computing;computational complexity;natural sciences computing;particle swarm optimisation;resource allocation;workflow management software;Amazon elastic compute cloud;Amazon simple storage service;EC2;NP-complete problem;S3;communication overhead;multiobjective optimization framework;multiobjective strategy;overall execution cost;particle swarm optimization approach;private clouds;public clouds;scientific workflows;single-objective optimization;task allocation;workflow runtime;Bandwidth;Biological cells;Cloud computing;Computational modeling;Optimization;Resource management;Runtime},
}

@Article{Szabo2014,
  author       = {Szabo, Claudia and Sheng, QuanZ. and Kroeger, Trent and Zhang, Yihong and Yu, Jian},
  title        = {Science in the Cloud: Allocation and Execution of Data-Intensive Scientific Workflows},
  journaltitle = {J. Grid Computing},
  year         = {2014},
  language     = {English},
  volume       = {12},
  number       = {2},
  pages        = {245--264},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-013-9282-3},
  file         = {Szabo2014.pdf:Szabo2014 - Science in the Cloud_ Allocation and Execution of Data-Intensive Scientific Workflows.pdf:PDF},
  keywords     = {Data-intensive workflows; Cloud computing; Scheduling; Allocation; Evolutionary computation},
  publisher    = {Springer Netherlands},
}

@Article{Tchernykh2015,
  author       = {Tchernykh, Andrei and Lozano, Luz and Schwiegelshohn, Uwe and Bouvry, Pascal and Pecero, JohnatanE. and Nesmachnow, Sergio and Drozdov, AlexanderYu.},
  title        = {Online Bi-Objective Scheduling for IaaS Clouds Ensuring Quality of Service},
  journaltitle = {J. Grid Computing},
  year         = {2015},
  language     = {English},
  pages        = {1--18},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-015-9340-0},
  keywords     = {Cloud computing; Service level agreement; Energy efficiency; Multi-objective scheduling; IaaS; Provider income},
  publisher    = {Springer Netherlands},
}

@InProceedings{Thanh2015,
  author    = {Toan Phan Thanh and Loc Nguyen The and Cuong Nguyen Doan},
  title     = {A novel workflow scheduling algorithm in cloud environment},
  booktitle = {Information and Computer Science (NICS), 2015 2nd National Foundation for Science and Technology Development Conference on},
  year      = {2015},
  month     = {Sept},
  pages     = {125-129},
  doi       = {10.1109/NICS.2015.7302176},
  abstract  = {The key factor which rules the cloud's performance is the workflow scheduling, one of the well-known problems have proven to be NP-complete. Many algorithms in the literature have been targeting the workflow scheduling problem, however, handful efficient solutions have been proposed. This paper proposes a metaheuristic algorithm called PSOi which based on the Particle Swarm Optimization method. Our experiments which arranged by using the simulation tool CloudSim show that PSOi is superior to the general algorithms called Random and RoundRobin, moreover the deviation between the solution found by PSOi and the optimal solution is negligible.},
  keywords  = {cloud computing;computational complexity;particle swarm optimisation;CloudSim;NP-complete;PSOi;RoundRobin;cloud environment;general algorithms;metaheuristic algorithm;particle swarm optimization method;random;simulation tool;workflow scheduling algorithm;Algorithm design and analysis;Computer science;Particle swarm optimization;Scheduling;Scheduling algorithms;Servers;cloud computing;particle swarm optimization;workflow scheduling},
}

@Electronic{Topchiy2013,
  Title                    = {Testing Amazon EC2 network speed},
  Author                   = {Topchiy, Serhiy},
  Month                    = mar,
  Url                      = {http://goo.gl/9iHcl3},
  Year                     = {2013},

  Bdsk-url-1               = {http://goo.gl/9iHcl3}
}

@Article{Tsai2013,
  author       = {Jinn-Tsong Tsai and Jia-Cen Fang and Jyh-Horng Chou},
  title        = {Optimized task scheduling and resource allocation on cloud computing environment using improved differential evolution algorithm},
  journaltitle = {Computers \& Operations Research},
  year         = {2013},
  volume       = {40},
  number       = {12},
  pages        = {3045--3055},
  issn         = {0305-0548},
  doi          = {10.1016/j.cor.2013.06.012},
  abstract     = {AbstractPurpose The objective of this study is to optimize task scheduling and resource allocation using an improved differential evolution algorithm (IDEA) based on the proposed cost and time models on cloud computing environment. Methods The proposed \{IDEA\} combines the Taguchi method and a differential evolution algorithm (DEA). The \{DEA\} has a powerful global exploration capability on macro-space and uses fewer control parameters. The systematic reasoning ability of the Taguchi method is used to exploit the better individuals on micro-space to be potential offspring. Therefore, the proposed \{IDEA\} is well enhanced and balanced on exploration and exploitation. The proposed cost model includes the processing and receiving cost. In addition, the time model incorporates receiving, processing, and waiting time. The multi-objective optimization approach, which is the non-dominated sorting technique, not with normalized single-objective method, is applied to find the Pareto front of total cost and makespan. Results In the five-task five-resource problem, the mean coverage ratios C(IDEA, DEA) of 0.368 and C(IDEA, NSGA-II) of 0.3 are superior to the ratios C(DEA, IDEA) of 0.249 and C(NSGA-II, IDEA) of 0.288, respectively. In the ten-task ten-resource problem, the mean coverage ratios C(IDEA, DEA) of 0.506 and C(IDEA, NSGA-II) of 0.701 are superior to the ratios C(DEA, IDEA) of 0.286 and C(NSGA-II, IDEA) of 0.052, respectively. Wilcoxon matched-pairs signed-rank test confirms there is a significant difference between \{IDEA\} and the other methods. In summary, the above experimental results confirm that the \{IDEA\} outperforms both the \{DEA\} and NSGA-II in finding the better Pareto-optimal solutions. Conclusions In the study, the \{IDEA\} shows its effectiveness to optimize task scheduling and resource allocation compared with both the \{DEA\} and the NSGA-II. Moreover, for decision makers, the Gantt charts of task scheduling in terms of having smaller makespan, cost, and both can be selected to make their decision when conflicting objectives are present. },
  file         = {Tsai2013.pdf:Tsai2013 - Optimized task scheduling and resource allocation on cloud computing environment using improved differential evolution algorithm.pdf:PDF},
  keywords     = {Cloud computing;Differential evolution algorithm;Task scheduling;Cost and time models;Multi-objective approach },
}

@InProceedings{Vairavanathan2012,
  author    = {Vairavanathan, Emalayan and Al-Kiswany, Samer and Costa, Lauro Beltr\~{a}o and Zhang, Zhao and Katz, Daniel S. and Wilde, Michael and Ripeanu, Matei},
  title     = {A Workflow-Aware Storage System: An Opportunity Study},
  booktitle = {Proceedings of the 2012 12\textsuperscript{th} IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
  year      = {2012},
  series    = {CCGRID '12},
  publisher = {IEEE Computer Society},
  location  = {Washington, DC, USA},
  isbn      = {978-0-7695-4691-9},
  pages     = {326--334},
  doi       = {10.1109/CCGrid.2012.109},
  acmid     = {2310197},
  file      = {Vairavanathan2012.pdf:Vairavanathan2012 - A Workflow-Aware Storage System_ An Opportunity Study.pdf:PDF},
  keywords  = {Large-scale storage systems, workflow-aware storage systems, workflow optimizations},
  numpages  = {9},
}

@Article{Vecchiola2009a,
  author       = {Christian Vecchiola and Michael Kirley and Rajkumar Buyya},
  title        = {Multi-Objective Problem Solving With Offspring on Enterprise Clouds},
  journaltitle = {CoRR},
  year         = {2009},
  volume       = {abs/0903.1386},
  url          = {http://arxiv.org/abs/0903.1386},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-0903-1386},
  timestamp    = {Mon, 05 Dec 2011 18:05:37 +0100},
}

@InProceedings{Vecchiola2009,
  author    = {Vecchiola, C. and Pandey, S. and Buyya, R.},
  title     = {High-Performance Cloud Computing: A View of Scientific Applications},
  booktitle = {Pervasive Systems, Algorithms, and Networks (ISPAN), 2009 10\textsuperscript{th} International Symposium on},
  year      = {2009},
  month     = dec,
  pages     = {4--16},
  doi       = {10.1109/I-SPAN.2009.150},
  abstract  = {Scientific computing often requires the availability of a massive number of computers for performing large scale experiments. Traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. Cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. Compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. These resources can be released when they are no more needed. Such services are often offered within the context of a service level agreement (SLA), which ensure the desired quality of service (QoS). Aneka, an enterprise cloud computing solution, harnesses the power of compute resources by relying on private and public clouds and delivers to users the desired QoS. Its flexible and service based infrastructure supports multiple programming paradigms that make Aneka address a variety of different scenarios: from finance applications to computational science. As examples of scientific computing in the cloud, we present a preliminary case study on using Aneka for the classification of gene expression data and the execution of fMRI brain imaging workflow.},
  file      = {Vecchiola2009.pdf:Vecchiola2009 - High-Performance Cloud Computing_ A View of Scientific Applications.pdf:PDF},
  keywords  = {Internet;quality of service;ubiquitous computing;QoS;high-performance cloud computing;quality of service;scientific computing;service level agreement;Application software;Availability;Cloud computing;Computer applications;Context-aware services;Finance;Gene expression;Large-scale systems;Quality of service;Scientific computing;Cloud computing;Scientific computing;computational science;high-performance computing},
}

@InProceedings{Wan2012,
  author    = {Cong Wan and Cuirong Wang and Jianxun Pei},
  title     = {A QoS-awared scientific workflow scheduling schema in cloud computing},
  booktitle = {Information Science and Technology (ICIST), 2012 International Conference on},
  year      = {2012},
  month     = mar,
  pages     = {634--639},
  doi       = {10.1109/ICIST.2012.6221722},
  abstract  = {Now it is a trend to deploy the applications in the cloud computing environment. Most of scientific workflow (SWF) needs to process a large amount of data and requires parallel computing to reduce response time. SWF under grid computing environment have been already studied. As a business model, cloud computing is more scalable and cost-effective. In this paper, we propose a schedule schema that enable user to change the processing time by adjusting the budget and to change the price by adjusting the request response time in the SWF.},
  file      = {Wan2012.pdf:Wan2012 - A QoS-awared scientific workflow scheduling schema in cloud computing.pdf:PDF},
  keywords  = {cloud computing;natural sciences computing;parallel processing;quality of service;scheduling;QoS-awared scientific workflow scheduling schema;SWF;business model;cloud computing environment;parallel computing;Algorithm design and analysis;Cloud computing;Computational modeling;Processor scheduling;Schedules;Time factors},
}

@Article{Wieczorek2009,
  author       = {Marek Wieczorek and Andreas Hoheisel and Radu Prodan},
  title        = {Towards a general model of the multi-criteria workflow scheduling on the grid},
  journaltitle = {Future Generation Computer Systems},
  year         = {2009},
  volume       = {25},
  number       = {3},
  pages        = {237--256},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2008.09.002},
  abstract     = {Workflow scheduling on the Grid becomes more challenging when multiple scheduling criteria are considered. Existing studies provide different approaches to the multi-criteria Grid workflow scheduling problem, and address different variants of the problem. A profound understanding of the problem’s nature can be an important step towards more generic scheduling approaches. Based on the related work and on our own experience, we propose several novel taxonomies of the problem, considering five facets: workflow model, scheduling criteria, scheduling process, resource model, and task model. We make a survey of the existing related work, and classify it according to the proposed taxonomies, identifying the most common use cases and the areas that have not been sufficiently explored yet. },
  file         = {Wieczorek2009.pdf:Wieczorek2009 - Towards a general model of the multi-criteria workflow scheduling on the grid.pdf:PDF},
  keywords     = {Grid computing;Workflow;Multi-criteria scheduling;Taxonomy },
}

@InProceedings{Wieczorek2006,
  author    = {Wieczorek, M. and Siddiqui, M. and Villazon, A. and Prodan, R. and Fahringer, T.},
  title     = {Applying Advance Reservation to Increase Predictability of Workflow Execution on the Grid},
  booktitle = {e-Science and Grid Computing, 2006. e-Science '06. Second IEEE International Conference on},
  year      = {2006},
  month     = dec,
  pages     = {82--82},
  doi       = {10.1109/E-SCIENCE.2006.261166},
  abstract  = {In this paper we present an extension to devise and implement advance reservation as part of the scheduling and resource management services of the ASKALON Grid application development and runtime environment. The scheduling service has been enhanced to offer a list of resources that can execute a specific task and to negotiatewith the resource manager about resources capable of processing tasks in the shortest possible time. We introduce progressive reservation approach which tries to allocate resources based on a fair-share principle. Experiments are shown that demonstrate the effectiveness of our approach, and that reflect different QoS parameters including performance, predictability, resource usage and resource fairness.},
  file      = {Wieczorek2006.pdf:Wieczorek2006 - Applying Advance Reservation to Increase Predictability of Workflow Execution on the Grid.pdf:PDF},
  keywords  = {Application software;Computer applications;Computer architecture;Computer science;Economic forecasting;Grid computing;Processor scheduling;Resource management;Runtime environment;Scheduling algorithm},
}

@Article{Wu2015a,
  author       = {Wu, Fuhui and Wu, Qingbo and Tan, Yusong},
  title        = {Workflow scheduling in cloud: a survey},
  journaltitle = {J. Supercomputing},
  year         = {2015},
  language     = {English},
  pages        = {1--46},
  issn         = {0920-8542},
  doi          = {10.1007/s11227-015-1438-4},
  file         = {Wu2015a.pdf:Wu2015a - Workflow scheduling in cloud_ a survey.pdf:PDF},
  keywords     = {Cloud computing; Workflow scheduling; QoS constrained scheduling; Workflow-as-a-service; Robust scheduling; Hybrid environment; Data-intensive workflow scheduling},
  publisher    = {Springer US},
}

@Article{Wu2013,
  author       = {Wu, Zhangjun and Liu, Xiao and Ni, Zhiwei and Yuan, Dong and Yang, Yun},
  title        = {A market-oriented hierarchical scheduling strategy in cloud workflow systems},
  journaltitle = {J. Supercomputing},
  year         = {2013},
  language     = {English},
  volume       = {63},
  number       = {1},
  pages        = {256--293},
  issn         = {0920-8542},
  doi          = {10.1007/s11227-011-0578-4},
  file         = {Wu2013.pdf:Wu2013 - A market-oriented hierarchical scheduling strategy in cloud workflow systems.pdf:PDF},
  keywords     = {Cloud workflow system; Cloud computing; Workflow scheduling; Hierarchical scheduling; Metaheuristics},
  publisher    = {Springer US},
}

@InProceedings{Wu2010,
  author    = {Zhangjun Wu and Zhiwei Ni and Lichuan Gu and Xiao Liu},
  title     = {A Revised Discrete Particle Swarm Optimization for Cloud Workflow Scheduling},
  booktitle = {Computational Intelligence and Security (CIS), 2010 International Conference on},
  year      = {2010},
  month     = dec,
  pages     = {184--188},
  doi       = {10.1109/CIS.2010.46},
  abstract  = {A cloud workflow system is a type of platform service which facilitates the automation of distributed applications based on the novel cloud infrastructure. Compared with grid environment, data transfer is a big overhead for cloud workflows due to the market-oriented business model in the cloud environments. In this paper, a Revised Discrete Particle Swarm Optimization (RDPSO) is proposed to schedule applications among cloud services that takes both data transmission cost and computation cost into account. Experiment is conducted with a set of workflow applications by varying their data communication costs and computation costs according to a cloud price model. Comparison is made on make span and cost optimization ratio and the cost savings with RDPSO, the standard PSO and BRS (Best Resource Selection) algorithm. Experimental results show that the proposed RDPSO algorithm can achieve much more cost savings and better performance on make span and cost optimization.},
  file      = {Wu2010.pdf:Wu2010 - A Revised Discrete Particle Swarm Optimization for Cloud Workflow Scheduling.pdf:PDF},
  keywords  = {cloud computing;grid computing;particle swarm optimisation;scheduling;best resource selection algorithm;cloud price model;cloud workflow scheduling;computation cost;data transfer;data transmission cost;distributed applications;grid environment;market-oriented business model;revised discrete particle swarm optimization;cloud computing;discrete particle swarm optimization;workflow scheduling},
}

@Article{Xavier2013,
  author       = {Xavier, S and Lovesum, SPJ},
  title        = {A Survey of Various Workflow Scheduling Algorithms in Cloud Environment},
  journaltitle = {Int. J. Scientific and Research},
  year         = {2013},
  file         = {Xavier2013.pdf:Xavier2013 - A Survey of Various Workflow Scheduling Algorithms in Cloud Environment.pdf:PDF},
}

@InProceedings{Xie2014,
  author    = {Guoqi Xie and Renfa Li and Xiongren Xiao and Yuekun Chen},
  title     = {A High-Performance DAG Task Scheduling Algorithm for Heterogeneous Networked Embedded Systems},
  booktitle = {Advanced Information Networking and Applications (AINA), 2014 IEEE 28\textsuperscript{th} International Conference on},
  year      = {2014},
  month     = may,
  pages     = {1011--1016},
  doi       = {10.1109/AINA.2014.123},
  abstract  = {A high-performance scheduling for a DAG (Directed Acyclic Graph) task graph on heterogeneous networked embedded systems or parallel and distributed systems is to maximize concurrency and minimize inter-processor communication. Most of the algorithms using upward rank value for task prioritizing and earliest finish time for processor assignment. But both approaches ignored the heterogeneity of system and could not create accurate and efficient schedules. Yet no one has doubled about and recognized that. A fully heterogeneous task scheduling algorithm is proposed to address the above problems in this paper. The fundamentals of DAG model and corresponding algorithms are investigated. New concepts called Heterogeneous Upward Rank Value (HURV) and Heterogeneous Priority Rank Value (HPRV) are defined. An algorithm called Heterogeneous Select Value (HSV) is proposed in paper. Both benchmark and extensive experimental evaluation demonstrate the significant improvements in proposed algorithm.},
  file      = {Xie2014.pdf:Xie2014 - A High-Performance DAG Task Scheduling Algorithm for Heterogeneous Networked Embedded Systems.pdf:PDF},
  issn      = {1550-445X},
  keywords  = {embedded systems;parallel processing;scheduling;DAG model;directed acyclic graph;distributed systems;heterogeneous networked embedded systems;heterogeneous select value;heterogeneous task scheduling algorithm;heterogeneous upward rank value;high-performance DAG task scheduling algorithm;inter-processor communication;parallel systems;Automotive electronics;Complexity theory;Embedded systems;Schedules;Scheduling;Scheduling algorithms;DAG;heterogeneous networked embedded systems;heterogeneous select value;heterogeneous upward rank value},
}

@Article{Xu2013,
  author       = {Hong Xu and Baochun Li},
  title        = {Dynamic Cloud Pricing for Revenue Maximization},
  journaltitle = {IEEE Trans. Cloud Computing},
  year         = {2013},
  volume       = {1},
  number       = {2},
  month        = jul,
  pages        = {158--171},
  issn         = {2168-7161},
  doi          = {10.1109/TCC.2013.15},
  abstract     = {In cloud computing, a provider leases its computing resources in the form of virtual machines to users, and a price is charged for the period they are used. Though static pricing is the dominant pricing strategy in today's market, intuitively price ought to be dynamically updated to improve revenue. The fundamental challenge is to design an optimal dynamic pricing policy, with the presence of stochastic demand and perishable resources, so that the expected long-term revenue is maximized. In this paper, we make three contributions in addressing this question. First, we conduct an empirical study of the spot price history of Amazon, and find that surprisingly, the spot price is unlikely to be set according to market demand. This has important implications on understanding the current market, and motivates us to develop and analyze market-driven dynamic pricing mechanisms. Second, we adopt a revenue management framework from economics, and formulate the revenue maximization problem with dynamic pricing as a stochastic dynamic program. We characterize its optimality conditions, and prove important structural results. Finally, we extend to consider a nonhomogeneous demand model.},
  file         = {Xu2013.pdf:Xu2013 - Dynamic Cloud Pricing for Revenue Maximization.pdf:PDF},
  keywords     = {cloud computing;dynamic programming;pricing;stochastic programming;virtual machines;AmazonV spot price history;cloud computing;dynamic cloud pricing;market-driven dynamic pricing mechanisms;nonhomogeneous demand model;perishable resources;revenue management framework;revenue maximization;static pricing;stochastic demand;stochastic dynamic program;virtual machines;Analytical models;Cloud computing;Cost accounting;Numerical models;Pricing;Stochastic processes;Dynamic pricing;cloud computing;dynamic programming;public cloud;revenue maximization;spot market},
}

@InProceedings{Xu2009,
  author    = {Meng Xu and Lizhen Cui and Haiyang Wang and Yanbing Bi},
  title     = {A Multiple QoS Constrained Scheduling Strategy of Multiple Workflows for Cloud Computing},
  booktitle = {Parallel and Distributed Processing with Applications, 2009 IEEE International Symposium on},
  year      = {2009},
  month     = aug,
  pages     = {629--634},
  doi       = {10.1109/ISPA.2009.95},
  abstract  = {Cloud computing has gained popularity in recent times. As a cloud must provide services to many users at the same time and different users have different QoS requirements, the scheduling strategy should be developed for multiple workflows with different QoS requirements. In this paper, we introduce a multiple QoS constrained scheduling strategy of multi-workflows (MQMW) to address this problem. The strategy can schedule multiple workflows which are started at any time and the QoS requirements are taken into account. Experimentation shows that our strategy is able to increase the scheduling success rate significantly.},
  file      = {Xu2009.pdf:Xu2009 - A Multiple QoS Constrained Scheduling Strategy of Multiple Workflows for Cloud Computing.pdf:PDF},
  keywords  = {Internet;quality of service;scheduling;workflow management software;cloud computing;multiple QoS constrained scheduling strategy;multiple workflows;Algorithm design and analysis;Application software;Bismuth;Cloud computing;Costs;Distributed processing;Partitioning algorithms;Processor scheduling;Quality of service;Scheduling algorithm;Cloud Computing;Multiple QoS Requirements;Multiple Workflows;Scheduling},
}

@Article{Xue2012,
  author       = {Xue, S J and Wu, W},
  title        = {Scheduling Workflow in Cloud Computing Based on Hybrid Particle Swarm Algorithm},
  journaltitle = {TELKOMNIKA Indonesian Journal of Electrica},
  year         = {2012},
  doi          = {10.11591/telkomnika.v10i7.1452},
  file         = {Xue2012.pdf:Xue2012 - Scheduling Workflow in Cloud Computing Based on Hybrid Particle Swarm Algorithm.pdf:PDF},
}

@Article{Yassa,
  author       = {Yassa, S and Chelouah, R and Kadima, H},
  title        = {Multi-Objective Approach for Energy-Aware Workflow Scheduling in Cloud Computing Environments},
  journaltitle = {The Scientific World},
  file         = {:Yassa - Multi-Objective Approach for Energy-Aware Workflow Scheduling in Cloud Computing Environments.pdf:PDF;Yassa - Multi-Objective Approach for Energy-Aware Workflow Scheduling in Cloud Computing Environments.pdf:PDF/Yassa - Multi-Objective Approach for Energy-Aware Workflow Scheduling in Cloud Computing Environments.pdf:PDF},
}

@InProceedings{Yin2015,
  Title                    = {Joint Scheduling of Data and Computation in Geo-Distributed Cloud Systems},
  Author                   = {Lingyan Yin and Jizhou Sun and Laiping Zhao and Chenzhou Cui and Jian Xiao and Ce Yu},
  Booktitle                = {Cluster, Cloud and Grid Computing (CCGrid), 2015 15\textsuperscript{th} IEEE/ACM International Symposium on},
  Year                     = {2015},
  Month                    = may,
  Pages                    = {657--666},

  Abstract                 = {Recent trends show that cloud computing is growing to span more and more globally distributed data centers. For geo-distributed data centers, there is an increasing need for scheduling algorithms to place tasks across data centers, by jointly considering data and computation. This scheduling must deal with situations such as wide-area distributed data, data sharing, WAN bandwidth costs and data center capacity limits, while also minimizing completion time. However, this kind of scheduling problems is known to be NP-Hard. In this paper, inspired by real applications in astronomy field, we propose a two-phase scheduling algorithm that addresses these challenges. The mapping phase groups tasks considering the data-sharing relations, and dispatches groups to data centers by way of one-to-one correspondence. The reassigning phase balances the completion time across data centers according to relations between tasks and groups. We utilize the real China-Astronomy-Cloud model and typical applications to evaluate our proposal. Simulations show that our algorithm obtains up to 22% better completion time and effectively reduces the amount of data transfers compared with other similar scheduling algorithms.},
  Doi                      = {10.1109/CCGrid.2015.83},
  Keywords                 = {astronomy computing;cloud computing;computational complexity;computer centres;scheduling;China-astronomy-cloud model;NP-hard scheduling problems;WAN bandwidth costs;astronomy field;cloud computing;data center capacity limits;data-sharing relations;geo-distributed cloud systems;geo-distributed data centers;joint data-computation scheduling algorithm;mapping phase group tasks;two-phase scheduling algorithm;wide-area distributed data;Astronomy;Computational modeling;Data models;Data transfer;Distributed databases;Scheduling algorithms;cloud computing;data and computation intensive;geo-distributed data centers;scheduling algorithms}
}

@InProceedings{Yu2005,
  author    = {Jia Yu and Buyya, R. and Chen Khong Tham},
  title     = {Cost-based scheduling of scientific workflow applications on utility grids},
  booktitle = {e-Science and Grid Computing, 2005. First International Conference on},
  year      = {2005},
  month     = jul,
  pages     = {8 pp.-147},
  doi       = {10.1109/E-SCIENCE.2005.26},
  abstract  = {Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such "pay-per-use" grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a cost-based workflow scheduling algorithm that minimizes execution cost while meeting the deadline for delivering results. It can also adapt to the delays of service executions by rescheduling unexecuted tasks. We also attempt to optimally solve the task scheduling problem in branches with several sequential tasks by modeling the branch as a Markov decision process and using the value iteration method},
  file      = {:PDF/Yu2005a.pdf:PDF;Yu2005.pdf:Yu2005 - Cost-based scheduling of scientific workflow applications on utility grids.pdf:PDF;Yu2005.pdf:PDF/Yu2005.pdf:PDF},
  keywords  = {Markov processes;decision theory;grid computing;natural sciences computing;quality of service;scheduling;Markov decision process;cost-based scheduling;grid computing;grid technologies;pay-per-use grids;quality of service;scientific workflow applications;service execution delays;service provisioning;task scheduling;utility grids;value iteration method;Computer networks;Costs;Delay;Grid computing;Optimal scheduling;Processor scheduling;Quality of service;Scheduling algorithm;Time factors;Workflow management software},
}

@InProceedings{Yu2008,
  author    = {Zhifeng Yu and Weisong Shi},
  title     = {A Planner-Guided Scheduling Strategy for Multiple Workflow Applications},
  booktitle = {Parallel Processing - Workshops, 2008. ICPP-W '08. International Conference on},
  year      = {2008},
  month     = sep,
  pages     = {1--8},
  doi       = {10.1109/ICPP-W.2008.10},
  abstract  = {Workflow applications are gaining popularity in recent years because of the prevalence of cluster environments. Many algorithms have been developed since, however most static algorithms are designed in the problem domain of scheduling single workflow applications, thus not applicable to a common cluster environment where multiple workflow applications and other independent jobs compete for resources. Dynamic scheduling approaches can handle the mixed workload practically by nature but their performance has yet to optimize as they do not have a global view of workflow applications. Recent research efforts suggest merging multiple workflows into one workflow before execution, but fail to address an important issue that multiple workflow applications may be submitted at different times by different users. In this paper, we propose a planner-guided dynamic scheduling strategy for multiple workflow applications, leveraging job dependence information and execution time estimation.Our approach schedules individual jobs dynamically without requiring merging the workflow applications a priori. The simulation results show that the proposed algorithm significantly outperforms two other algorithms by 43.6% and 36.7% with respect to workflow makespan and turnaround time respectively, and it performs even better when the number of concurrent workflow applications increases and the resources are scarce.},
  file      = {Yu2008.pdf:Yu2008 - A Planner-Guided Scheduling Strategy for Multiple Workflow Applications.pdf:PDF},
  issn      = {1530-2016},
  keywords  = {graph theory;scheduling;dynamic scheduling strategy;execution time estimation;leveraging job dependence information;multiple workflow applications;planner-guided scheduling strategy;Algorithm design and analysis;Clustering algorithms;Dynamic scheduling;Heuristic algorithms;Job design;Merging;Parallel processing;Resource management;Scheduling algorithm;Time measurement;cluster;scheduling;workflow},
}

@Article{Zhan2015,
  author       = {Zhan, Zhi-Hui and Liu, Xiao-Fang and Gong, Yue-Jiao and Zhang, Jun and Chung, Henry Shu-Hung and Li, Yun},
  title        = {Cloud Computing Resource Scheduling and a Survey of Its Evolutionary Approaches},
  journaltitle = {ACM Computing Surv.},
  year         = {2015},
  volume       = {47},
  number       = {4},
  month        = jul,
  pages        = {63:1--63:33},
  issn         = {0360-0300},
  doi          = {10.1145/2788397},
  acmid        = {2788397},
  articleno    = {63},
  issue_date   = {July 2015},
  keywords     = {Cloud computing, ant colony optimization, evolutionary computation, genetic algorithm, particle swarm optimization, resource scheduling},
  location     = {New York, NY, USA},
  numpages     = {33},
  publisher    = {ACM},
}

@InProceedings{Zhao2013,
  author    = {Dongfang Zhao and Burlingame, K. and Debains, C. and Alvarez-Tabio, P. and Raicu, I.},
  title     = {Towards high-performance and cost-effective distributed storage systems with information dispersal algorithms},
  booktitle = {Cluster Computing (CLUSTER), 2013 IEEE International Conference on},
  year      = {2013},
  month     = sep,
  pages     = {1--5},
  doi       = {10.1109/CLUSTER.2013.6702655},
  abstract  = {Reliability is one of the most fundamental challenges for high performance computing (HPC) and cloud computing. Data replication is the de facto mechanism to achieve high reliability, even though it has been criticized for its high cost and low efficiency. Recent research showed promising results by switching the traditional data replication to a software-based RAID. In order to systematically study the effectiveness of this new method, we built two storage systems from the ground up: a POSIX-compliant distributed file system (FusionFS) and a distributed key-value store (IStore), both supporting information dispersal algorithms (IDA) for data redundancy. FusionFS is crafted to have excellent throughput and scalability for HPC, whereas IStore is architected mainly as a light-weight key-value storage in cloud computing. We evaluated both systems with a large number of parameter combinations. Results show that, for both HPC and cloud computing communities, IDA-based methods with current commodity hardware could outperform data replication in some cases, and would completely surpass data replication with the growing computational capacity through multi/many-core processors (e.g. Intel Xeon Phi, NVIDIA GPU).},
  file      = {Zhao2013.pdf:Zhao2013 - Towards high-performance and cost-effective distributed storage systems with information dispersal algorithms.pdf:PDF},
  keywords  = {cloud computing;multiprocessing systems;parallel processing;redundancy;storage management;FusionFS system;HPC;IDA;IStore;POSIX-compliant distributed file system;cloud computing;data replication;distributed key-value store;distributed storage systems;high performance computing;information dispersal algorithms;lightweight key-value storage;many-core processors;multicore processors;redundant array of independent disks;software-based RAID;Artificial neural networks;Encoding;Nickel;Redundancy;Switches;Writing},
}

@InCollection{Zhao2007,
  author    = {Zhao, Henan and Sakellariou, Rizos},
  title     = {Advance Reservation Policies for Workflows},
  booktitle = {Job Scheduling Strategies for Parallel Processing},
  year      = {2007},
  editor    = {Frachtenberg, Eitan and Schwiegelshohn, Uwe},
  language  = {English},
  volume    = {4376},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-71034-9},
  pages     = {47--67},
  doi       = {10.1007/978-3-540-71035-6_3},
  file      = {Zhao2007.pdf:Zhao2007 - Advance Reservation Policies for Workflows.pdf:PDF},
}

@InProceedings{Zhao2011,
  author    = {Yong Zhao and Xubo Fei and Raicu, I. and Shiyong Lu},
  title     = {Opportunities and Challenges in Running Scientific Workflows on the Cloud},
  booktitle = {Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC), 2011 International Conference on},
  year      = {2011},
  month     = oct,
  pages     = {455--462},
  doi       = {10.1109/CyberC.2011.80},
  abstract  = {Cloud computing is gaining tremendous momentum in both academia and industry. The application of Cloud computing, however, has mostly focused on Web applications and business applications; while the recognition of using Cloud computing to support large-scale workflows, especially data-intensive scientific workflows on the Cloud is still largely overlooked. We coin the term "Cloud Workflow", to refer to the specification, execution, provenance tracking of large-scale scientific workflows, as well as the management of data and computing resources to enable the execution of scientific workflows on the Cloud. In this paper, we analyze why there has been such a gap between the two technologies, and what it means to bring Cloud and workflow together; we then present the key challenges in running Cloud workflow, and discuss the research opportunities in realizing workflows on the Cloud.},
  file      = {Zhao2011.pdf:Zhao2011 - Opportunities and Challenges in Running Scientific Workflows on the Cloud.pdf:PDF},
  keywords  = {cloud computing;natural sciences computing;workflow management software;Web applications;business applications;cloud computing;cloud workflow;scientific workflow;Cloud computing;Computational modeling;Computer architecture;Engines;Monitoring;Programming;Scalability;Cloud computing;Cloud workflow;Data Intensive Computing;Scientific Workflow},
}

@InProceedings{Zhao2012,
  author    = {Yong Zhao and Youfu Li and Wenhong Tian and Ruini Xue},
  title     = {Scientific-Workflow-Management-as-a-Service in the Cloud},
  booktitle = {Cloud and Green Computing (CGC), 2012 Second International Conference on},
  year      = {2012},
  month     = nov,
  pages     = {97--104},
  doi       = {10.1109/CGC.2012.70},
  abstract  = {Scientific workflow management systems have been around for many years and provide essential support such as management of data and task dependencies, job scheduling and execution, provenance tracking, etc. to scientific computing. While we are entering into a “big data” era, it is necessary for scientific workflow systems to integrate with Cloud platforms so as to deal with the ever increasing data scale and analysis complexity. In this paper, we present our experience in offering the Swift scientific workflow management system as a service in the Cloud. Our solution integrates Swift with the OpenNebula Cloud platform, and supports workflow specification and submission, on-demand virtual cluster provisioning, high-throughput task scheduling and execution, and efficient and scalable Cloud resource management. We demonstrate the capability of the solution using a NASA MODIS image processing workflow.},
  file      = {Zhao2012.pdf:Zhao2012 - Scientific-Workflow-Management-as-a-Service in the Cloud.pdf:PDF},
  keywords  = {cloud computing;natural sciences computing;workflow management software;NASA MODIS image processing workflow;OpenNebula Cloud platform;Swift scientific workflow management system as a service;analysis complexity;big data era;data scale;high-throughput task execution;high-throughput task scheduling;on-demand virtual cluster provisioning;workflow specification;Cloud computing;Computer architecture;Customer relationship management;Monitoring;Processor scheduling;Resource management;Scalability;Cloud workflow;Scientific Workflow;Swift;Workflow-as-a-Service},
}

@Article{Zheng2013,
  author       = {Zheng, Wei and Sakellariou, Rizos},
  title        = {Budget-Deadline Constrained Workflow Planning for Admission Control},
  journaltitle = {J. Grid Computing},
  year         = {2013},
  language     = {English},
  volume       = {11},
  number       = {4},
  pages        = {633--651},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-013-9257-4},
  file         = {Zheng2013.pdf:Zheng2013 - Budget-Deadline Constrained Workflow Planning for Admission Control.pdf:PDF},
  keywords     = {Admission control; Bi-criteria DAG scheduling; SLA-based resource reservation; Workflow planning},
  publisher    = {Springer Netherlands},
}

@Article{Zitzler1999,
  author       = {Zitzler, E. and Thiele, L.},
  title        = {Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  journaltitle = {IEEE Trans. Evolutionary Computation},
  year         = {1999},
  volume       = {3},
  number       = {4},
  month        = nov,
  pages        = {257--271},
  issn         = {1089-778X},
  doi          = {10.1109/4235.797969},
  abstract     = {Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem},
  file         = {Zitzler1999.pdf:Zitzler1999 - Multiobjective evolutionary algorithms_ a comparative case study and the strength Pareto approach.pdf:PDF},
  keywords     = {evolutionary computation;knapsack problems;optimisation;Pareto dominance relationship;clustering procedure;conflicting objectives;continuously updated population;digital hardware-software multiprocessor system;extended 0/1 knapsack problem;multiobjective evolutionary algorithms;multiobjective optimization;nondominated solutions;population diversity;strength Pareto approach;Computer aided software engineering;Computer architecture;Cost function;Evolutionary computation;Hardware;Multiprocessing systems;Pareto optimization;Sampling methods;Software systems;Space exploration},
}

@Book{SoonOngandKayChenTan2009,
  title     = {Multi-Objective Memetic Algorithms},
  year      = {2009},
  editor    = {Chi-Keong Goh,Yew-Soon Ong,Kay Chen Tan},
  volume    = {171},
  series    = {Studies in Computational Intelligence},
  publisher = {Springer Berlin Heidelberg},
}

@Article{Simmhan2016,
  author       = {Simmhan, Yogesh and Ramakrishnan, Lavanya and Antoniu, Gabriel and Goble, Carole},
  title        = {Cloud Computing for Data-driven Science and Engineering},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2016},
  volume       = {28},
  number       = {4},
  month        = mar,
  pages        = {947--949},
  issn         = {1532-0626},
  doi          = {10.1002/cpe.3668},
  acmid        = {2915591},
  citedate     = {2016.03.23},
  issue_date   = {March 2016},
  numpages     = {3},
  publisher    = {John Wiley \& Sons Ltd.},
}

@Online{2016f,
  title        = {Compute Engine - IaaS | Google Cloud Engine},
  year         = {2016},
  url          = {https://cloud.google.com/compute/},
  organization = {Google},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{2016i,
  title     = {Cycle Computing | Better Answers. Faster.},
  year      = {2016},
  url       = {http://cyclecomputing.com/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016k,
  title     = {EGI site},
  year      = {2016},
  url       = {http://www.egi.eu/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016n,
  title     = {Internet History of 1970{s}},
  year      = {2016},
  url       = {http://www.computerhistory.org/internethistory/1970s/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  publisher = {Computer History Museum},
  timestamp = {2016.01.30},
}

@Online{2016o,
  title     = {Microsoft {HPC} Pack},
  year      = {2016},
  url       = {https://technet.microsoft.com/library/cc514029},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016p,
  title     = {pegasus-isi/WorkflowGenerator: Synthetic Workflow Generators},
  year      = {2016},
  url       = {https://github.com/pegasus-isi/WorkflowGenerator},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016q,
  title     = {Penguin Computing On-Demand: POD},
  year      = {2016},
  url       = {https://pod.penguincomputing.com/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016r,
  title     = {Pricing - Virtual Machines (VMs) | Microsoft Azure},
  year      = {2016},
  url       = {https://azure.microsoft.com/en-us/pricing/details/virtual-machines/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016u,
  title        = {Salesforce.com},
  year         = {2016},
  url          = {https://www.salesforce.com/},
  organization = {Salesforce.com, inc.},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{Jones2015,
  author   = {Bob Jones},
  title    = {Towards the European Open Science Cloud},
  year     = {2015},
  url      = {http://helix-nebula.eu/events/towards-the-european-open-science-cloud},
  month    = mar,
  citedate = {2016.04.30},
}

@Article{Bernstein2014,
  author       = {D. Bernstein},
  title        = {Containers and Cloud: From LXC to Docker to Kubernetes},
  journaltitle = {IEEE Cloud Computing},
  year         = {2014},
  volume       = {1},
  number       = {3},
  month        = sep,
  pages        = {81--84},
  issn         = {2325-6095},
  doi          = {10.1109/MCC.2014.51},
  abstract     = {This issue's "Cloud Tidbit" focuses on container technology and how it's emerging as an important part of the cloud computing infrastructure. It looks at Docker, an open source project that automates the faster deployment of Linux applications, and Kubernetes, an open source cluster manager for Docker containers.},
  citedate     = {2016.03.23},
  keywords     = {Cloud computing;Containers;Google;Home appliances;Linux;Runtime;Virtual machine monitors;cloud;containers;dockers;virtual machines},
  publisher    = {IEEE},
}

@InProceedings{Rostanski2014,
  author    = {M. Rostanski and K. Grochla and A. Seman},
  title     = {Evaluation of highly available and fault-tolerant middleware clustered architectures using RabbitMQ},
  booktitle = {Proceedings of the Federated Conference on Computer Science and Information Systems},
  year      = {2014},
  publisher = {IEEE},
  location  = {Warsaw},
  month     = sep,
  pages     = {879--884},
  doi       = {10.15439/2014F48},
  abstract  = {The paper presents a performance evaluation of message broker system, Rabbit MQ in high availability - enabling and redundant configurations. Rabbit MQ is a message queuing system realizing the middleware for distributed systems that implements the Advanced Message Queuing Protocol. The scalability and high availability design issues are discussed. Since HA and performance scalability requirements are in conflict, scenarios for using clustered RabbitMQ nodes and mirrored queues are presented. The results of performance measurements are reported.},
  citedate  = {2016.03.23},
  keywords  = {distributed processing;fault tolerance;middleware;queueing theory;RabbitMQ;advanced message queuing protocol;distributed system;fault-tolerant middleware clustered architecture;message broker system;message queuing system;Availability;Fault tolerance;Fault tolerant systems;Middleware;Publishing;Scalability;Servers},
}

@Article{VanDerWalt2014,
  author       = {Van Der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  title        = {scikit-image: image processing in Python},
  journaltitle = {PeerJ},
  year         = {2014},
  volume       = {2},
  pages        = {e453},
  doi          = {10.7717/peerj.453},
  citedate     = {2016.03.23},
  publisher    = {PeerJ Inc.},
}

@Article{Zhang2014b,
  author       = {Yongping Zhang and Gongxuan Zhang and Zhaomeng Zhu},
  title        = {The Application of Compressed Sensing in Internet of Things and Its Algorithm Acceleration},
  journaltitle = {Journal of Computational Information Systems},
  year         = {2014},
  volume       = {10},
  number       = {1},
  pages        = {119--126},
  citedate     = {2016.03.23},
  owner        = {tefx},
  publisher    = {Binary Information Press},
  timestamp    = {2016.01.30},
}

@Online{2013,
  title     = {Back to the Future: 1.21 petaFLOPSRPeak, 156,000-core CycleCloud {HPC} runs 264 years of Materials Science},
  year      = {2013},
  url       = {http://bit.ly/23ApHIe},
  month     = nov,
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Ryan2013,
  author       = {Mark D. Ryan},
  title        = {Cloud computing security: The scientific challenge, and a survey of solutions},
  journaltitle = {Journal of Systems and Software},
  year         = {2013},
  volume       = {86},
  number       = {9},
  pages        = {2263--2268},
  issn         = {0164-1212},
  doi          = {10.1016/j.jss.2012.12.025},
  abstract     = {We briefly survey issues in cloud computing security. The fact that data are shared with the cloud service provider is identified as the core scientific problem that separates cloud computing security from other topics in computing security. We survey three current research directions, and evaluate them in terms of a running software-as-a-service example. },
  citedate     = {2016.03.23},
  keywords     = {Cloud computing},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Book{Robert2013,
  author    = {Robert, Christian and Casella, George},
  title     = {{Monte} carlo statistical methods},
  year      = {2013},
  publisher = {Springer},
  location  = {New York},
  address   = {New York},
  citedate  = {2016.03.23},
}

@Article{Wolstencroft2013,
  author       = {Wolstencroft, Katherine and Haines, Robert and Fellows, Donal and Williams, Alan and Withers, David and Owen, Stuart and Soiland-Reyes, Stian and Dunlop, Ian and Nenadic, Aleksandra and Fisher, Paul and Bhagat, Jiten and Belhajjame, Khalid and Bacall, Finn and Hardisty, Alex and Nieva de la Hidalga, Abraham and Balcazar Vargas, Maria P. and Sufi, Shoaib and Goble, Carole},
  title        = {The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud},
  journaltitle = {Nucleic Acids Research},
  year         = {2013},
  volume       = {41},
  number       = {W1},
  pages        = {W557-W561},
  doi          = {10.1093/nar/gkt328},
  abstract     = {The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.},
  citedate     = {2016.03.23},
  publisher    = {Oxford University Press},
}

@Article{Chudoba2013,
  author       = {Chudoba, R and Sad{\'\i}lek, V and Rypl, R and Vo{\v{r}}echovsk{\`y}, M},
  title        = {Using Python for scientific computing: Efficient and flexible evaluation of the statistical characteristics of functions with multivariate random inputs},
  journaltitle = {Computer Physics Communications},
  year         = {2013},
  volume       = {184},
  number       = {2},
  pages        = {414--427},
  issn         = {0010-4655},
  doi          = {10.1016/j.cpc.2012.08.021},
  abstract     = {This paper examines the feasibility of high-level Python based utilities for numerically intensive applications via an example of a multidimensional integration for the evaluation of the statistical characteristics of a random variable. We discuss the approaches to the implementation of mathematically formulated incremental expressions using high-level scripting code and low-level compiled code. Due to the dynamic typing of the Python language, components of the algorithm can be easily coded in a generic way as algorithmic templates. Using the Enthought Development Suite they can be effectively assembled into a flexible computational framework that can be configured to execute the code for arbitrary combinations of integration schemes and versions of instantiated code. The paper describes the development cycle using a simple running example involving averaging of a random two-parametric function that includes discontinuity. This example is also used to compare the performance of the available algorithmic and executional features. The implemented package including further examples and the results of performance studies have been made available via the free repository [1] and \{CPCP\} library. Program summary Program title: spirrid Catalogue identifier: AENL_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AENL_v1_0.html Program obtainable from: \{CPC\} Program Library, Queen{\^{a}}s University, Belfast, N. Ireland Licensing provisions: Special licence provided by the author No. of lines in distributed program, including test data, etc.: 10722 No. of bytes in distributed program, including test data, etc.: 157099 Distribution format: tar.gz Programming language: Python and C. Computer: PC. Operating system: LINUX, UNIX, Windows. Classification: 4.13, 6.2. External routines: NumPy (http://numpy.scipy.org/), SciPy (http://www.scipy.com) Nature of problem: Evaluation of the statistical moments of a function of random variables. Solution method: Direct multidimensional integration. Running time: Depending on the number of random variables the time needed for the numerical estimation of the mean value of a function with a sufficiently low level of numerical error varies. For orientation, the time needed for two included examples: examples/fiber_tt_2p/fiber_tt_2p.py with 2 random variables: few milliseconds examples/fiber_po_8p/fiber_po_8p.py with 8 random variables: few seconds },
  citedate     = {2016.03.23},
  keywords     = {Python},
  publisher    = {Elsevier},
}

@InProceedings{Rao2012,
  author    = {Rao, B.B.P. and Saluia, P. and Sharma, N. and Mittal, A. and Sharma, S.V.},
  title     = {Cloud computing for Internet of Things amp; sensing based applications},
  booktitle = {Proceedings of the 6\textsuperscript{th} International Conference on Sensing Technology},
  year      = {2012},
  publisher = {IEEE},
  location  = {Kolkata, West Bangal, India.},
  month     = dec,
  pages     = {374--380},
  doi       = {10.1109/ICSensT.2012.6461705},
  abstract  = {Internet of Things (IoT) is a concept that envisions all objects around us as part of internet. IoT coverage is very wide and include variety of objects like smart phones, tablets, digital cameras, sensors, etc. Once all these devices are connected with each other, they enable more and more smart processes and services that support our basic needs, economies, environment and health. Such enormous number of devices connected to internet provides many kinds of services and produce huge amount of data and information. Cloud computing is a model for on-demand access to a shared pool of configurable resources (e.g. compute, networks, servers, storage, applications, services, and software) that can be easily provisioned as Infrastructure (IaaS), software and applications (SaaS). Cloud based platforms help to connect to the things (IaaS) around us so that we can access anything at any time and any place in a user friendly manner using customized portals and in built applications (SaaS). Hence, cloud acts as a front end to access Internet of Things. Applications that interact with devices like sensors have special requirements of massive storage to storage big data, huge computation power to enable the real time processing of the data, and high speed network to stream audio or video. In this paper, we describe how Internet of Things and Cloud computing can work together can address the Big Data issues. We also illustrate about Sensing as a service on cloud using few applications like Augmented Reality, Agriculture and Environment monitoring. Finally, we also propose a prototype model for providing sensing as a service on cloud.},
  citedate  = {2016.03.23},
  issn      = {2156-8065},
  keywords  = {Internet;audio streaming;cloud computing;computerised instrumentation;sensors;video streaming;IaaS;Internet of Things;IoT;SaaS;agriculture monitoring;audio streaming;augmented reality;cloud computing;configurable resource;data storage;digital camera;environment monitoring;high speed network;on-demand access;real time data processing;sensing based application;smart phone;tablet;video streaming;Agriculture;Cloud computing;Clouds;Computational modeling;Sensors;Servers;CDAC Scientific Cloud;CStaaS;Cloud Computing;Internet of Things;Sensor Networks},
}

@InCollection{Kranjc2012,
  author    = {Kranjc, Janez and Podpe{\v{c}}an, Vid and Lavra{\v{c}}, Nada},
  title     = {Clowdflows: a cloud based scientific workflow platform},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  year      = {2012},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  pages     = {816--819},
  doi       = {10.1007/978-3-642-33486-3_54},
  citedate  = {2016.03.23},
}

@InProceedings{Zhao2012a,
  author    = {Yong Zhao and Yanzhe Zhang and Wenhong Tian and Ruini Xue and Cui Lin},
  title     = {Designing and Deploying a Scientific Computing Cloud Platform},
  booktitle = {Proceedings of the 13\textsuperscript{th} ACM/IEEE International Conference on Grid Computing},
  year      = {2012},
  publisher = {IEEE},
  location  = {Beijing},
  month     = sep,
  pages     = {104--113},
  doi       = {10.1109/Grid.2012.12},
  abstract  = {Scientific applications are growing rapidly in both data scale and processing complexity due to advances in science instrumentation and network technologies, where Cloud computing as an emerging computing paradigm can offer unprecedented scalability and resources on demand, and is getting more and more adoption in the science community. We present our early effort in designing and building CloudDragon, a scientific computing Cloud platform based on OpenNebula. We take a structured approach that integrates client-side application specification and testing, service-based workflow submission and management, on-demand virtual cluster provisioning, high-throughput task scheduling and execution, and efficient and scalable Cloud resource management. We first analyze the integration efficiency of our approach in a cluster setting and then show a production deployment of the platform.},
  citedate  = {2016.03.23},
  issn      = {1550-5510},
  keywords  = {cloud computing;computational complexity;natural sciences computing;scheduling;CloudDragon;OpenNebula;client-side application specification;cloud computing;cluster setting;data scale;high-throughput task scheduling;network technologies;on-demand virtual cluster provisioning;processing complexity;science community;science instrumentation;scientific computing cloud platform;service-based workflow submission;Cloud computing;Clouds;Computer architecture;Monitoring;Resource management;Scientific computing;Virtual machining;Cloud Platform;Cloud Resource Management;Scientific Computing;Swift;Workflow},
}

@Article{Sefraoui2012,
  author       = {Sefraoui, Omar and Aissaoui, Mohammed and Eleuldj, Mohsine},
  title        = {OpenStack: toward an open-source solution for cloud computing},
  journaltitle = {International Journal of Computer Applications},
  year         = {2012},
  volume       = {55},
  number       = {3},
  pages        = {38--42},
  doi          = {10.5120/8738-2991},
  citedate     = {2016.03.23},
  publisher    = {Foundation of Computer Science},
}

@Article{Lin2012,
  author       = {Lin, Johnny Wei-Bing},
  title        = {Why python is the next wave in earth sciences computing},
  journaltitle = {Bulletin of the American Meteorological Society},
  year         = {2012},
  volume       = {93},
  number       = {12},
  pages        = {1823--1824},
  doi          = {10.1175/BAMS-D-12-00148.1},
  citedate     = {2016.03.23},
  publisher    = {American Meteorological Society},
}

@InProceedings{Sadashiv2011,
  author    = {Sadashiv, N. and Kumar, S.M.D.},
  title     = {Cluster, grid and cloud computing: A detailed comparison},
  booktitle = {Proceedings of the 6\textsuperscript{th} International Conference on Computer Science Education},
  year      = {2011},
  publisher = {IEEE},
  location  = {Singapore},
  month     = aug,
  pages     = {477--482},
  doi       = {10.1109/ICCSE.2011.6028683},
  abstract  = {Cloud computing is rapidly growing as an alternative to conventional computing. However, it is based on models like cluster computing, distributed computing, utility computing and grid computing in general. This paper presents an end-to-end comparison between Cluster Computing, Grid Computing and Cloud Computing, along with the challenges they face. This could help in better understanding these models and to know how they differ from its related concepts, all in one go. It also discusses the ongoing projects and different applications that use these computing models as a platform for execution. An insight into some of the tools which can be used in the three computing models to design and develop applications is given. This could help in bringing out the innovative ideas in the field and can be explored to the needs in the computing world.},
  citedate  = {2016.03.23},
  keywords  = {cloud computing;grid computing;workstation clusters;cloud computing;cluster computing;computing models;distributed computing;grid computing;utility computing;Cloud computing;Clouds;Computational modeling;Computers;Europe;Grid computing;Cloud Computing;Cluster Computing;Comparison;Computing Models;Grid Computing},
}

@InProceedings{Mundkur2011,
  author       = {Mundkur, Prashanth and Tuulos, Ville and Flatow, Jared},
  title        = {Disco: a computing platform for large-scale data analytics},
  booktitle    = {Proceedings of the 10\textsuperscript{th} ACM SIGPLAN workshop on Erlang},
  year         = {2011},
  organization = {ACM},
  publisher    = {ACM},
  location     = {New York},
  pages        = {84--89},
  doi          = {10.1145/2034654.2034670},
  citedate     = {2016.03.23},
}

@InProceedings{Voeckler2011,
  author    = {V\"{o}ckler, Jens-S\"{o}nke and Juve, Gideon and Deelman, Ewa and Rynge, Mats and Berriman, Bruce},
  title     = {Experiences Using Cloud Computing for a Scientific Workflow Application},
  booktitle = {Proceedings of the 2\textsuperscript{nd} International Workshop on Scientific Cloud Computing},
  year      = {2011},
  publisher = {ACM},
  location  = {San Jose, California, USA},
  isbn      = {978-1-4503-0699-7},
  pages     = {15--24},
  doi       = {10.1145/1996109.1996114},
  acmid     = {1996114},
  address   = {New York},
  citedate  = {2016.03.23},
  keywords  = {EC2, cross cloud computing, eucalyptus, experience, futuregrid, magellan, nimbus, pegasus, periodigrams, sky computing, workflow},
  numpages  = {10},
}

@Article{Wilde2011,
  author       = {Michael Wilde and Mihael Hategan and Justin M. Wozniak and Ben Clifford and Daniel S. Katz and Ian Foster},
  title        = {Swift: A language for distributed parallel scripting},
  journaltitle = {Parallel Computing},
  year         = {2011},
  volume       = {37},
  number       = {9},
  pages        = {633--652},
  note         = {Emerging Programming Paradigms for Large-Scale Scientific Computing},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2011.05.005},
  abstract     = {Scientists, engineers, and statisticians must execute domain-specific application programs many times on large collections of file-based data. This activity requires complex orchestration and data management as data is passed to, from, and among application invocations. Distributed and parallel computing resources can accelerate such processing, but their use further increases programming complexity. The Swift parallel scripting language reduces these complexities by making file system structures accessible via language constructs and by allowing ordinary application programs to be composed into powerful parallel scripts that can efficiently utilize parallel and distributed resources. We present Swift{\^{a}}s implicitly parallel and deterministic programming model, which applies external applications to file collections using a functional style that abstracts and simplifies distributed parallel execution. },
  citedate     = {2016.03.23},
  keywords     = {Swift},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.03.02},
}

@InProceedings{Fabian2011,
  author    = {Fabian, N. and Moreland, K. and Thompson, D. and Bauer, A.C. and Marion, P. and Geveci, B. and Rasquin, M. and Jansen, K.E.},
  title     = {The ParaView Coprocessing Library: A scalable, general purpose in situ visualization library},
  booktitle = {Proceedings of the IEEE Symposium on Large Data Analysis and Visualization},
  year      = {2011},
  publisher = {IEEE},
  location  = {Providence, Rhode Island},
  month     = oct,
  pages     = {89--96},
  doi       = {10.1109/LDAV.2011.6092322},
  abstract  = {As high performance computing approaches exascale, CPU capability far outpaces disk write speed, and in situ visualization becomes an essential part of an analyst's workflow. In this paper, we describe the ParaView Coprocessing Library, a framework for in situ visualization and analysis coprocessing. We describe how coprocessing algorithms (building on many from VTK) can be linked and executed directly from within a scientific simulation or other applications that need visualization and analysis. We also describe how the ParaView Coprocessing Library can write out partially processed, compressed, or extracted data readable by a traditional visualization application for interactive post-processing. Finally, we will demonstrate the library's scalability in a number of real-world scenarios.},
  citedate  = {2016.03.23},
  keywords  = {data analysis;data compression;data visualisation;feature extraction;multiprocessing systems;open systems;software libraries;CPU capability;ParaView coprocessing library;data compression;disk write speed;high performance computing;in situ visualization library;interactive post-processing;library scalability;readable data extraction;scientific simulation;Adaptation models;Data mining;Data models;Data visualization;Libraries;Pipelines;Sockets;coprocessing;in situ;scaling;simulation},
}

@Article{Afgan2010,
  author       = {Afgan, Enis and Baker, Dannon and Coraor, Nate and Chapman, Brad and Nekrutenko, Anton and Taylor, James},
  title        = {Galaxy CloudMan: delivering cloud compute clusters},
  journaltitle = {BMC Bioinformatics},
  year         = {2010},
  volume       = {11},
  number       = {12},
  pages        = {1--6},
  issn         = {1471-2105},
  doi          = {10.1186/1471-2105-11-S12-S4},
  abstract     = {Widespread adoption of high-throughput sequencing has greatly increased the scale and sophistication of computational infrastructure needed to perform genomic research. An alternative to building and maintaining local infrastructure is ``cloud computing'', which, in principle, offers on demand access to flexible computational infrastructure. However, cloud computing resources are not yet suitable for immediate ``as is'' use by experimental biologists.},
  citedate     = {2016.03.23},
  owner        = {tefx},
  publisher    = {Springer},
  timestamp    = {2016.03.04},
}

@InProceedings{Srirama2010,
  author    = {Srirama, Satish and Batrashev, Oleg and Vainikko, Eero},
  title     = {SciCloud: Scientific Computing on the Cloud},
  booktitle = {Proceedings of the 10\textsuperscript{th} IEEE/ACM International Conference on Cluster, Cloud and Grid Computing},
  year      = {2010},
  publisher = {IEEE},
  location  = {Washington},
  isbn      = {978-0-7695-4039-9},
  pages     = {579--580},
  doi       = {10.1109/CCGRID.2010.56},
  acmid     = {1845186},
  citedate  = {2016.03.23},
  keywords  = {Cloud computing, scientific computing, GRID, mobile web services, Mobile Host, Eucalyptus},
  numpages  = {2},
}

@InProceedings{Zaharia2010,
  author    = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  title     = {Spark: cluster computing with working sets},
  booktitle = {Proceedings of the 2\textsuperscript{nd} USENIX conference on Hot topics in cloud computing},
  year      = {2010},
  volume    = {10},
  publisher = {USENIX Association},
  location  = {Boston, MA},
  pages     = {10},
  url       = {http://static.usenix.org/legacy/events/hotcloud10/tech/full_papers/Zaharia.pdf},
  citedate  = {2016.03.23},
}

@InProceedings{Shvachko2010,
  author    = {Shvachko, K. and Hairong Kuang and Radia, S. and Chansler, R.},
  title     = {The Hadoop Distributed File System},
  booktitle = {Proceedings of the 26\textsuperscript{th} IEEE Symposium on Mass Storage Systems and Technologies},
  year      = {2010},
  publisher = {IEEE},
  location  = {Incline Villiage, Nevada},
  month     = may,
  pages     = {1--10},
  doi       = {10.1109/MSST.2010.5496972},
  abstract  = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
  citedate  = {2016.03.23},
  keywords  = {Internet;distributed databases;network operating systems;Hadoop distributed file system;Yahoo!;data storage;data stream;enterprise data;Bandwidth;Clustering algorithms;Computer architecture;Concurrent computing;Distributed computing;Facebook;File servers;File systems;Protection;Protocols;HDFS;Hadoop;distributed file system},
}

@InProceedings{Qian2009,
  author    = {Qian, Ling and Luo, Zhiguo and Du, Yujian and Guo, Leitao},
  title     = {Cloud Computing: An Overview},
  booktitle = {Proceedings of the 1\textsuperscript{st} International Conference on Cloud Computing},
  year      = {2009},
  publisher = {Springer},
  location  = {Beijing},
  isbn      = {978-3-642-10664-4},
  pages     = {626--631},
  doi       = {10.1007/978-3-642-10665-1_63},
  acmid     = {1695725},
  citedate  = {2016.03.23},
  keywords  = {Cloud Storage, Cloud computing, Virtualization},
  numpages  = {6},
}

@Article{Dikaiakos2009,
  author       = {Dikaiakos, M.D. and Katsaros, D. and Mehra, P. and Pallis, G. and Vakali, A.},
  title        = {Cloud Computing: Distributed Internet Computing for IT and Scientific Research},
  journaltitle = {IEEE Internet Computing},
  year         = {2009},
  volume       = {13},
  number       = {5},
  month        = sep,
  pages        = {10--13},
  issn         = {1089-7801},
  doi          = {10.1109/MIC.2009.103},
  abstract     = {Cloud computing is a disruptive technology with profound implications not only for Internet services but also for the IT sector as a whole. Its emergence promises to streamline the on-demand provisioning of software, hardware, and data as a service, achieving economies of scale in IT solutions' deployment and operation. This issue's articles tackle topics including architecture and management of cloud computing infrastructures, SaaS and IaaS applications, discovery of services and data in cloud computing infrastructures, and cross-platform interoperability. Still, several outstanding issues exist, particularly related to SLAs, security and privacy, and power efficiency. Other open issues include ownership, data transfer bottlenecks, performance unpredictability, reliability, and software licensing issues. Finally, hosted applications' business models must show a clear pathway to monetizing cloud computing. Several companies have already built Internet consumer services such as search, social networking, Web email, and online commerce that use cloud computing infrastructure. Above all, cloud computing's still unknown "killer application" will determine many of the challenges and the solutions we must develop to make this technology work in practice.},
  citedate     = {2016.03.23},
  keywords     = {Internet;electronic commerce;open systems;IT sector;Web email;business model;cloud computing;consumer services;cross-platform interoperability;data transfer;distributed Internet computing;online commerce;social networking;software licensing;software reliability;Application software;Business;Cloud computing;Computer architecture;Data security;Disaster management;Distributed computing;Economies of scale;Hardware;Web and internet services;Internet data centers;cloud computing;distributed systems;utility computing},
  publisher    = {IEEE},
}

@Article{Peterson2009,
  author       = {Peterson, Pearu},
  title        = {F2PY: a tool for connecting Fortran and Python programs},
  journaltitle = {International Journal of Computational Science and Engineering},
  year         = {2009},
  volume       = {4},
  number       = {4},
  pages        = {296--305},
  doi          = {10.1504/IJCSE.2009.029165},
  citedate     = {2016.03.23},
  publisher    = {Inderscience Publishers},
}

@Online{2009,
  title    = {Overview - Sphinx 1.4.6 Documentation},
  year     = {2009},
  url      = {http://www.sphinx-doc.org/en/stable/},
  citedate = {2016.04.30},
}

@InBook{Ludaescher2009,
  author    = {Lud{\"a}scher, Bertram and Altintas, Ilkay and Bowers, Shawn and Cummings, Julian and Critchlow, Terence and Deelman, Ewa and Roure, David D and Freire, Juliana and Goble, Carole and Jones, Matthew and others},
  title     = {Scientific process automation and workflow management},
  booktitle = {Scientific Data Management: Challenges, Existing Technology, and Deployment},
  year      = {2009},
  volume    = {230},
  publisher = {CRC Press},
  location  = {Boca Raton, FL},
  pages     = {476--508},
  citedate  = {2016.03.23},
}

@Article{Deelman2009,
  author       = {Deelman, Ewa and Gannon, Dennis and Shields, Matthew and Taylor, Ian},
  title        = {Workflows and e-Science: An overview of workflow system features and capabilities},
  journaltitle = {Future Generation Computer Systems},
  year         = {2009},
  volume       = {25},
  number       = {5},
  pages        = {528--540},
  doi          = {10.1016/j.future.2008.06.012},
  citedate     = {2016.03.23},
  publisher    = {Elsevier},
}

@InProceedings{Hoffa2008,
  author    = {Hoffa, C. and Mehta, G. and Freeman, T. and Deelman, E. and Keahey, K. and Berriman, B. and Good, J.},
  title     = {On the Use of Cloud Computing for Scientific Workflows},
  booktitle = {Proceedings of the 4\textsuperscript{th} IEEE International Conference on eScience},
  year      = {2008},
  publisher = {IEEE},
  location  = {Indianapolis, IN},
  month     = dec,
  pages     = {640--645},
  doi       = {10.1109/eScience.2008.167},
  abstract  = {This paper explores the use of cloud computing for scientific workflows, focusing on a widely used astronomy application-Montage. The approach is to evaluate from the point of view of a scientific workflow the tradeoffs between running in a local environment, if such is available, and running in a virtual environment via remote, wide-area network resource access. Our results show that for Montage, a workflow with short job runtimes, the virtual environment can provide good compute time performance but it can suffer from resource scheduling delays and widearea communications.},
  citedate  = {2016.03.23},
  keywords  = {astronomy computing;scheduling;scientific information systems;virtual reality;wide area networks;workflow management software;astronomy application-Montage;cloud computing;resource scheduling delays;scientific workflows;virtual environment;wide-area network resource access;widearea communications;Astronomy;Cloud computing;Costs;Grid computing;Internet;Laboratories;Power grids;Resource management;Runtime;Virtual environment;cloud computing;virtual machine;workflow},
}

@InProceedings{Hazelhurst2008,
  author    = {Hazelhurst, Scott},
  title     = {Scientific Computing Using Virtual High-performance Computing: A Case Study Using the Amazon Elastic Computing Cloud},
  booktitle = {Proceedings of the Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries: Riding the Wave of Technology},
  year      = {2008},
  publisher = {ACM},
  location  = {Wilderness, South Africa},
  isbn      = {978-1-60558-286-3},
  pages     = {94--103},
  doi       = {10.1145/1456659.1456671},
  acmid     = {1456671},
  address   = {New York},
  citedate  = {2016.03.23},
  keywords  = {Amazon elastic computing cloud, clusters, high-performance computing, virtualisation},
  numpages  = {10},
}

@Online{Certik2008,
  author   = {Certik, O and others},
  title    = {SymPy: Python library for symbolic mathematics},
  year     = {2008},
  url      = {http://www.sympy.org/en/index.html},
  citedate = {2016.04.30},
}

@Article{Hunter2007,
  author       = {Hunter, J.D.},
  title        = {Matplotlib: A {2D} Graphics Environment},
  journaltitle = {Computing in Science Engineering},
  year         = {2007},
  volume       = {9},
  number       = {3},
  month        = may,
  pages        = {90--95},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2007.55},
  abstract     = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems},
  citedate     = {2016.03.23},
  keywords     = {computer graphics;mathematics computing;object-oriented programming;software packages;2D graphics package;Matplotlib;Python;application development;interactive scripting;operating system;publication-quality image generation;user interface;Computer languages;Equations;Graphical user interfaces;Graphics;Image generation;Interpolation;Operating systems;Packaging;Programming profession;User interfaces;Python;application development;scientific programming;scripting languages},
  publisher    = {IEEE},
}

@Article{Oliphant2007,
  author       = {Oliphant, Travis E.},
  title        = {Python for Scientific Computing},
  journaltitle = {Computing in Science Engineering},
  year         = {2007},
  volume       = {9},
  number       = {3},
  month        = may,
  pages        = {10--20},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2007.58},
  abstract     = {Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
  citedate     = {2016.03.23},
  keywords     = {high level languages;Python;high-level language;scientific codes;scientific computing;steering language;Application software;Embedded software;High level languages;Internet;Libraries;Prototypes;Scientific computing;Software standards;Standards development;Writing;Python;computer languages;scientific computing;scientific programming},
  publisher    = {IEEE},
}

@InProceedings{VanRossum2007,
  author    = {Van Rossum, Guido and others},
  title     = {Python Programming Language},
  booktitle = {Proceedings of the USENIX Annual Technical Conference},
  year      = {2007},
  volume    = {41},
  publisher = {USENIX Association},
  location  = {Santa Clara, CA},
  citedate  = {2016.03.23},
}

@Online{Jones2007,
  author   = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
  title    = {SciPy: Open source scientific tools for Python},
  year     = {2007},
  url      = {http://www.scipy.org},
  citedate = {2016.04.30},
  pages    = {86},
  volume   = {73},
}

@Article{Oreilly2007,
  author       = {O'reilly, Tim},
  title        = {What is Web 2.0: Design patterns and business models for the next generation of software},
  journaltitle = {Communications \& Strategies},
  year         = {2007},
  volume       = {1},
  number       = {1},
  pages        = {17},
  url          = {http://ssrn.com/abstract=1008839},
  citedate     = {2016.03.23},
  publisher    = {O'Reilly Media},
}

@Article{Donoho2006,
  author       = {Donoho, D.L.},
  title        = {Compressed sensing},
  journaltitle = {IEEE Transactions on Information Theory},
  year         = {2006},
  volume       = {52},
  number       = {4},
  month        = apr,
  pages        = {1289--1306},
  issn         = {0018-9448},
  doi          = {10.1109/TIT.2006.871582},
  abstract     = {Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of "random" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that "most" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces},
  citedate     = {2016.03.23},
  keywords     = {convex programming;data compression;image coding;image reconstruction;image sampling;image sensors;sparse matrices;transform coding;Euclidean space;convex optimization;general linear functional measurement;image reconstruction;nonadaptive nonpixel sampling;sensing compression;signal processing;sparse representation;transform coding;Compressed sensing;Data mining;Digital images;Image coding;Image reconstruction;Pixel;Signal processing;Size measurement;Transform coding;Vectors;Adaptive sampling;Basis Pursuit;Gel'fand;Quotient-of-a-Subspace theorem;almost-spherical sections of Banach spaces;eigenvalues of random matrices;information-based complexity;integrated sensing and processing;minimum;optimal recovery;sparse solution of linear equations},
  publisher    = {IEEE},
}

@InProceedings{Candes2006,
  author       = {Cand{\`e}s, Emmanuel J and others},
  title        = {Compressive sampling},
  booktitle    = {Proceedings of the international congress of mathematicians},
  year         = {2006},
  volume       = {3},
  organization = {Madrid, Spain},
  publisher    = {European Mathematical Society},
  location     = {Madrid, Spain},
  pages        = {1433--1452},
  url          = {http://www.disp.duke.edu/~dbrady/courses/holography/lectures/CompressiveSampling.pdf},
  citedate     = {2016.03.23},
}

@Article{Ludaescher2006,
  author       = {Lud{\"a}scher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew B and Lee, Edward A and Tao, Jing and Zhao, Yang},
  title        = {Scientific workflow management and the Kepler system},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2006},
  volume       = {18},
  number       = {10},
  pages        = {1039--1065},
  doi          = {10.1002/cpe.994},
  citedate     = {2016.03.23},
  publisher    = {John Wiley \& Sons, Inc},
}

@Article{Hoheisel2006,
  author       = {Hoheisel, Andreas},
  title        = {User tools and languages for graph-based Grid workflows},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2006},
  volume       = {18},
  number       = {10},
  pages        = {1101--1113},
  doi          = {10.1002/cpe.1002},
  citedate     = {2016.03.23},
  publisher    = {John Wiley \& Sons, Inc},
}

@Article{Cai2005,
  author       = {Cai, Xing and Langtangen, Hans Petter and Moe, Halvard},
  title        = {On the performance of the Python programming language for serial and parallel scientific computations},
  journaltitle = {Scientific Programming},
  year         = {2005},
  volume       = {13},
  number       = {1},
  pages        = {31--56},
  doi          = {10.1155/2005/619804},
  citedate     = {2016.03.23},
  publisher    = {Hindawi Publishing Corporation},
}

@Article{Rosenblum2004,
  author       = {Rosenblum, Mendel},
  title        = {The Reincarnation of Virtual Machines},
  journaltitle = {ACM Queue},
  year         = {2004},
  volume       = {2},
  number       = {5},
  month        = jul,
  pages        = {34--40},
  issn         = {1542-7730},
  doi          = {10.1145/1016998.1017000},
  acmid        = {1017000},
  citedate     = {2016.03.23},
  issue_date   = {July/August 2004},
  numpages     = {7},
  publisher    = {ACM},
}

@Article{Cottom2003,
  author       = {Cottom, T.L.},
  title        = {Using SWIG to bind C++ to Python},
  journaltitle = {Computing in Science Engineering},
  year         = {2003},
  volume       = {5},
  number       = {2},
  month        = mar,
  pages        = {88--97},
  issn         = {1521-9615},
  doi          = {10.1109/MCISE.2003.1182968},
  abstract     = {An increasingly popular approach to scientific computing is to combine Python and compiled modules. Such an approach merges the high performance typically found in compiled routines with the interface of a flexible, scalable, and easy-to-learn interpreted language. Although using C to hand-code extensions to Python binds the latter to a given compiled asset in C++, programmers who used C++'s more advanced features (until recently) lacked the automated support available in Fortran and C. One tool for creating Python bindings to C the Simplified Wrapper and Interface Generator. SWIG-an open-source application used by a large and ever-expanding community-began as an effort to expose physics packages in a large parallel simulation code to interpreted languages. SWIG preprocesses C and C++ code and generates library bindings in several interpreted languages including Python, Pert, Tcl, and Java. Recent improvements to SWIG provide greater support for binding C++ code. SWIG now creates, for example, bindings for some of C++'s more advanced features such as templates and exceptions. This article explores how SWIG does this by examining a series of small C++ code examples.},
  citedate     = {2016.03.23},
  keywords     = {C++ language;application program interfaces;object-oriented programming;physics computing;C code preprocessing;C++ code preprocessing;C++/Python binding;Java;Perl;SWIG;Simplified Wrapper and Interface Generator;Tcl;exceptions;interpreted languages;large parallel simulation code;library bindings;open-source application;physics packages;templates;Libraries;Linux;Prototypes;Scientific computing},
  publisher    = {IEEE},
}

@TechReport{Frey2002,
  author    = {Frey, James},
  title     = {Condor DAGMan: Handling inter-job dependencies},
  year      = {2002},
  citedate  = {2016.03.23},
  publisher = {University of Wisconsin, Department of Computer Science},
}

@Online{Miller2002,
  author    = {Miller, Patrick},
  title     = {pyMPI --- An introduction to parallel Python using {MPI}},
  year      = {2002},
  url       = {http://uni.getrik.com/wp-content/uploads/2010/04/pyMPI.pdf},
  citedate  = {2016.03.10},
  publisher = {Livermore National Laboratories},
  volume    = {11},
}

@PhdThesis{Sarmenta2001,
  author      = {Sarmenta, Luis FG},
  title       = {Volunteer computing},
  institution = {Massachusetts Institute of Technology},
  year        = {2001},
  location    = {Cambridge, MA},
  citedate    = {2016.03.23},
}

@Book{Edwards2000,
  author    = {Edwards, W. Keith},
  title     = {Core JINI},
  year      = {2000},
  edition   = {2nd},
  publisher = {Prentice Hall PTR},
  location  = {Upper Saddle River, NJ},
  isbn      = {0130894087},
  address   = {Upper Saddle River, NJ},
  citedate  = {2016.03.23},
}

@Article{Hayes1998,
  author       = {Brian Hayes},
  title        = {Computing Science: Collective Wisdom},
  journaltitle = {American Scientist},
  year         = {1998},
  volume       = {86},
  number       = {2},
  pages        = {118--122},
  issn         = {0003-0996},
  doi          = {10.1511/1998.2.113},
  citedate     = {2016.03.23},
  publisher    = {Sigma Xi, The Scientific Research Society},
}

@InProceedings{Singh1996,
  author    = {Singh, Munindar P and Vouk, Mladen A},
  title     = {Scientific workflows: scientific computing meets transactional workflows},
  booktitle = {Proceedings of the NSF Workshop on Workflow and Process Automation in Information Systems: State-of-the-Art and Future Directions},
  year      = {1996},
  publisher = {ACM},
  location  = {New York},
  pages     = {28--34},
  citedate  = {2016.03.23},
}

@Article{Hudak1992,
  author       = {Hudak, Paul and Peyton Jones, Simon and Wadler, Philip and Boutel, Brian and Fairbairn, Jon and Fasel, Joseph and Guzm\'{a}n, Mar\'{\i}a M. and Hammond, Kevin and Hughes, John and Johnsson, Thomas and Kieburtz, Dick and Nikhil, Rishiyur and Partain, Will and Peterson, John},
  title        = {Report on the Programming Language Haskell: A Non-strict, Purely Functional Language Version 1.2},
  journaltitle = {ACM SIGPLAN Notices},
  year         = {1992},
  volume       = {27},
  number       = {5},
  month        = may,
  pages        = {1--164},
  issn         = {0362-1340},
  doi          = {10.1145/130697.130699},
  acmid        = {130699},
  citedate     = {2016.03.23},
  issue_date   = {May 1992},
  numpages     = {164},
  publisher    = {ACM},
}

@InProceedings{Wadler1992,
  author    = {Wadler, Philip},
  title     = {The Essence of Functional Programming},
  booktitle = {Proceedings of the 19\textsuperscript{th} ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  year      = {1992},
  publisher = {ACM},
  location  = {Albuquerque, New Mexico, USA},
  isbn      = {0-89791-453-8},
  pages     = {1--14},
  doi       = {10.1145/143165.143169},
  acmid     = {143169},
  address   = {New York},
  citedate  = {2016.03.23},
  numpages  = {14},
}

@Article{Strachey1959,
  author       = {Strachey, Christopher},
  title        = {Time sharing in large fast computers},
  journaltitle = {Communications of the ACM},
  year         = {1959},
  volume       = {2},
  number       = {7},
  pages        = {12--13},
  citedate     = {2016.03.23},
  organization = {ASSOC COMPUTING MACHINERY 1515 BROADWAY, NEW YORK, NY 10036},
  publisher    = {ACM},
}

@Online{2016j,
  title     = {EC2 Instance Pricing - Amazon Web Serices (AWS)},
  year      = {2016},
  url       = {http://aws.amazon.com/ec2/pricing/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016l,
  title        = {Elastic Compute Cloud (EC2) Cloud Server \& Hosting - AWS},
  year         = {2016},
  url          = {https://aws.amazon.com/ec2},
  organization = {Amazon},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Article{Zhu2016,
  author       = {Zhu, Z. and Zhang, G. and Li, M. and Liu, X.},
  title        = {Evolutionary Multi-Objective Workflow Scheduling in Cloud},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  year         = {2016},
  volume       = {27},
  number       = {5},
  pages        = {1344--1357},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2015.2446459},
  abstract     = {Cloud computing provides promising platforms for executing large applications with enormous computational resources to offer on demand. In a Cloud model, users are charged based on their usage of resources and the required Quality of Service (QoS) specifications. Although there are many existing workflow scheduling algorithms in traditional distributed or heterogeneous computing environments, they have difficulties in being directly applied to the Cloud environments since Cloud differs from traditional heterogeneous environments by its service-based resource managing method and pay-per-use pricing strategies. In this paper, we highlight such difficulties, and model the workflow scheduling problem which optimizes both makespan and cost as a Multi-objective Optimization Problem (MOP) for the Cloud environments. We propose an Evolutionary Multi-objective Optimization (EMO)-based algorithm to solve this workflow scheduling problem on an Infrastructure as a Service (IaaS) platform. Novel schemes for problemspecific encoding and population initialization, fitness evaluation and genetic operators are proposed in this algorithm. Extensive experiments on real world workflows and randomly generated workflows show that the schedules produced by our evolutionary algorithm present more stability on most of the workflows with the instance-based IaaS computing and pricing models. The results also show that our algorithm can achieve significantly better solutions than existing state-of-the-art QoS optimization scheduling algorithms in most cases. The conducted experiments are based on the on-demand instance types of Amazon EC2; however, the proposed algorithm are easy to be extended to the resources and pricing models of other IaaS services.},
  citedate     = {2016.03.23},
  keywords     = {Computational modeling;Encoding;Pricing;Processor scheduling;Quality of service;Schedules;Scheduling;Cloud computing;Infrastructure as a Service;evolutionary algorithm;multi-objective optimization;workflow scheduling},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Online{2016c,
  title        = {Free Microsoft Office Online, Word, Excel, Powerpoint},
  year         = {2016},
  url          = {https://products.office.com/en-US/office-online},
  organization = {Microsoft},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{2016e,
  title        = {Gmail},
  year         = {2016},
  url          = {https://mail.google.com},
  organization = {Google},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{2016g,
  title        = {Google Cloud Engine},
  year         = {2016},
  url          = {https://appengine.google.com/},
  organization = {Google},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{2016m,
  title        = {Great Internet Mersenne Prime Search - PrimeNet},
  year         = {2016},
  url          = {http://www.mersenne.org/},
  organization = {Mersenne Research, Inc.},
  citedate     = {2016.04.30},
  owner        = {tefx},
  timestamp    = {2016.03.04},
}

@Online{2016h,
  title     = {{HPC} On-Demand | Sabalcore | High Performance Cloud Computing - HPC Cloud},
  year      = {2016},
  url       = {http://www.sabalcore.com/services/hpc-on-demand/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Article{Arabnejad2016,
  author       = {Hamid Arabnejad and Jorge G. Barbosa and Radu Prodan},
  title        = {Low-time complexity budget-deadline constrained workflow scheduling on heterogeneous resources},
  journaltitle = {Future Generation Computer Systems},
  year         = {2016},
  volume       = {55},
  pages        = {29--40},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2015.07.021},
  abstract     = {Abstract The execution of scientific applications, under the utility computing model, is constrained to Quality of Service (QoS) parameters. Commonly, applications have time and cost constraints such that all tasks of an application need to be finished within a user-specified Deadline and Budget. Several algorithms have been proposed for multiple QoS workflow scheduling, but most of them use search-based strategies that generally have a high time complexity, making them less useful in realistic scenarios. In this paper, we present a heuristic scheduling algorithm with quadratic time complexity that considers two important constraints for QoS-based workflow scheduling, time and cost, named Deadline–Budget Constrained Scheduling (DBCS). From the deadline and budget defined by the user, the \{DBCS\} algorithm finds a feasible solution that accomplishes both constraints with a success rate similar to other state-of-the-art search-based algorithms in terms of the successful rate of feasible solutions, consuming in the worst case only approximately 4% of the time. The \{DBCS\} algorithm has a low-time complexity of O ( n 2 . p ) for n tasks and p processors.},
  citedate     = {2016.03.23},
  keywords     = {Quality of Service},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2015.10.10},
}

@Article{Abbott2016,
  author        = {Abbott, B. P. and Abbott, R. and Abbott, T. D. and Abernathy, M. R. and Acernese, F. and Ackley, K. and Adams, C. and Adams, T. and Addesso, P. and Adhikari, R. X. and Adya, V. B. and Affeldt, C. and Agathos, M. and Agatsuma, K. and Aggarwal, N. and Aguiar, O. D. and Aiello, L. and Ain, A. and Ajith, P. and Allen, B. and Allocca, A. and Altin, P. A. and Anderson, S. B. and Anderson, W. G. and Arai, K. and Arain, M. A. and Araya, M. C. and Arceneaux, C. C. and Areeda, J. S. and Arnaud, N. and Arun, K. G. and Ascenzi, S. and Ashton, G. and Ast, M. and Aston, S. M. and Astone, P. and Aufmuth, P. and Aulbert, C. and Babak, S. and Bacon, P. and Bader, M. K. M. and Baker, P. T. and Baldaccini, F. and Ballardin, G. and Ballmer, S. W. and Barayoga, J. C. and Barclay, S. E. and Barish, B. C. and Barker, D. and Barone, F. and Barr, B. and Barsotti, L. and Barsuglia, M. and Barta, D. and Bartlett, J. and Barton, M. A. and Bartos, I. and Bassiri, R. and Basti, A. and Batch, J. C. and Baune, C. and Bavigadda, V. and Bazzan, M. and Behnke, B. and Bejger, M. and Belczynski, C. and Bell, A. S. and Bell, C. J. and Berger, B. K. and Bergman, J. and Bergmann, G. and Berry, C. P. L. and Bersanetti, D. and Bertolini, A. and Betzwieser, J. and Bhagwat, S. and Bhandare, R. and Bilenko, I. A. and Billingsley, G. and Birch, J. and Birney, R. and Birnholtz, O. and Biscans, S. and Bisht, A. and Bitossi, M. and Biwer, C. and Bizouard, M. A. and Blackburn, J. K. and Blair, C. D. and Blair, D. G. and Blair, R. M. and Bloemen, S. and Bock, O. and Bodiya, T. P. and Boer, M. and Bogaert, G. and Bogan, C. and Bohe, A. and Bojtos, P. and Bond, C. and Bondu, F. and Bonnand, R. and Boom, B. A. and Bork, R. and Boschi, V. and Bose, S. and Bouffanais, Y. and Bozzi, A. and Bradaschia, C. and Brady, P. R. and Braginsky, V. B. and Branchesi, M. and Brau, J. E. and Briant, T. and Brillet, A. and Brinkmann, M. and Brisson, V. and Brockill, P. and Brooks, A. F. and Brown, D. A. and Brown, D. D. and Brown, N. M. and Buchanan, C. C. and Buikema, A. and Bulik, T. and Bulten, H. J. and Buonanno, A. and Buskulic, D. and Buy, C. and Byer, R. L. and Cabero, M. and Cadonati, L. and Cagnoli, G. and Cahillane, C. and Bustillo, J. Calder\'on and Callister, T. and Calloni, E. and Camp, J. B. and Cannon, K. C. and Cao, J. and Capano, C. D. and Capocasa, E. and Carbognani, F. and Caride, S. and Diaz, J. Casanueva and Casentini, C. and Caudill, S. and Cavagli\`a, M. and Cavalier, F. and Cavalieri, R. and Cella, G. and Cepeda, C. B. and Baiardi, L. Cerboni and Cerretani, G. and Cesarini, E. and Chakraborty, R. and Chalermsongsak, T. and Chamberlin, S. J. and Chan, M. and Chao, S. and Charlton, P. and Chassande-Mottin, E. and Chen, H. Y. and Chen, Y. and Cheng, C. and Chincarini, A. and Chiummo, A. and Cho, H. S. and Cho, M. and Chow, J. H. and Christensen, N. and Chu, Q. and Chua, S. and Chung, S. and Ciani, G. and Clara, F. and Clark, J. A. and Cleva, F. and Coccia, E. and Cohadon, P.-F. and Colla, A. and Collette, C. G. and Cominsky, L. and Constancio, M. and Conte, A. and Conti, L. and Cook, D. and Corbitt, T. R. and Cornish, N. and Corsi, A. and Cortese, S. and Costa, C. A. and Coughlin, M. W. and Coughlin, S. B. and Coulon, J.-P. and Countryman, S. T. and Couvares, P. and Cowan, E. E. and Coward, D. M. and Cowart, M. J. and Coyne, D. C. and Coyne, R. and Craig, K. and Creighton, J. D. E. and Creighton, T. D. and Cripe, J. and Crowder, S. G. and Cruise, A. M. and Cumming, A. and Cunningham, L. and Cuoco, E. and Canton, T. Dal and Danilishin, S. L. and D'Antonio, S. and Danzmann, K. and Darman, N. S. and Da Silva Costa, C. F. and Dattilo, V. and Dave, I. and Daveloza, H. P. and Davier, M. and Davies, G. S. and Daw, E. J. and Day, R. and De, S. and DeBra, D. and Debreczeni, G. and Degallaix, J. and De Laurentis, M. and Del\'eglise, S. and Del Pozzo, W. and Denker, T. and Dent, T. and Dereli, H. and Dergachev, V. and DeRosa, R. T. and De Rosa, R. and DeSalvo, R. and Dhurandhar, S. and D\'{\i}az, M. C. and Di Fiore, L. and Di Giovanni, M. and Di Lieto, A. and Di Pace, S. and Di Palma, I. and Di Virgilio, A. and Dojcinoski, G. and Dolique, V. and Donovan, F. and Dooley, K. L. and Doravari, S. and Douglas, R. and Downes, T. P. and Drago, M. and Drever, R. W. P. and Driggers, J. C. and Du, Z. and Ducrot, M. and Dwyer, S. E. and Edo, T. B. and Edwards, M. C. and Effler, A. and Eggenstein, H.-B. and Ehrens, P. and Eichholz, J. and Eikenberry, S. S. and Engels, W. and Essick, R. C. and Etzel, T. and Evans, M. and Evans, T. M. and Everett, R. and Factourovich, M. and Fafone, V. and Fair, H. and Fairhurst, S. and Fan, X. and Fang, Q. and Farinon, S. and Farr, B. and Farr, W. M. and Favata, M. and Fays, M. and Fehrmann, H. and Fejer, M. M. and Feldbaum, D. and Ferrante, I. and Ferreira, E. C. and Ferrini, F. and Fidecaro, F. and Finn, L. S. and Fiori, I. and Fiorucci, D. and Fisher, R. P. and Flaminio, R. and Fletcher, M. and Fong, H. and Fournier, J.-D. and Franco, S. and Frasca, S. and Frasconi, F. and Frede, M. and Frei, Z. and Freise, A. and Frey, R. and Frey, V. and Fricke, T. T. and Fritschel, P. and Frolov, V. V. and Fulda, P. and Fyffe, M. and Gabbard, H. A. G. and Gair, J. R. and Gammaitoni, L. and Gaonkar, S. G. and Garufi, F. and Gatto, A. and Gaur, G. and Gehrels, N. and Gemme, G. and Gendre, B. and Genin, E. and Gennai, A. and George, J. and Gergely, L. and Germain, V. and Ghosh, Abhirup and Ghosh, Archisman and Ghosh, S. and Giaime, J. A. and Giardina, K. D. and Giazotto, A. and Gill, K. and Glaefke, A. and Gleason, J. R. and Goetz, E. and Goetz, R. and Gondan, L. and Gonz\'alez, G. and Castro, J. M. Gonzalez and Gopakumar, A. and Gordon, N. A. and Gorodetsky, M. L. and Gossan, S. E. and Gosselin, M. and Gouaty, R. and Graef, C. and Graff, P. B. and Granata, M. and Grant, A. and Gras, S. and Gray, C. and Greco, G. and Green, A. C. and Greenhalgh, R. J. S. and Groot, P. and Grote, H. and Grunewald, S. and Guidi, G. M. and Guo, X. and Gupta, A. and Gupta, M. K. and Gushwa, K. E. and Gustafson, E. K. and Gustafson, R. and Hacker, J. J. and Hall, B. R. and Hall, E. D. and Hammond, G. and Haney, M. and Hanke, M. M. and Hanks, J. and Hanna, C. and Hannam, M. D. and Hanson, J. and Hardwick, T. and Harms, J. and Harry, G. M. and Harry, I. W. and Hart, M. J. and Hartman, M. T. and Haster, C.-J. and Haughian, K. and Healy, J. and Heefner, J. and Heidmann, A. and Heintze, M. C. and Heinzel, G. and Heitmann, H. and Hello, P. and Hemming, G. and Hendry, M. and Heng, I. S. and Hennig, J. and Heptonstall, A. W. and Heurs, M. and Hild, S. and Hoak, D. and Hodge, K. A. and Hofman, D. and Hollitt, S. E. and Holt, K. and Holz, D. E. and Hopkins, P. and Hosken, D. J. and Hough, J. and Houston, E. A. and Howell, E. J. and Hu, Y. M. and Huang, S. and Huerta, E. A. and Huet, D. and Hughey, B. and Husa, S. and Huttner, S. H. and Huynh-Dinh, T. and Idrisy, A. and Indik, N. and Ingram, D. R. and Inta, R. and Isa, H. N. and Isac, J.-M. and Isi, M. and Islas, G. and Isogai, T. and Iyer, B. R. and Izumi, K. and Jacobson, M. B. and Jacqmin, T. and Jang, H. and Jani, K. and Jaranowski, P. and Jawahar, S. and Jim\'enez-Forteza, F. and Johnson, W. W. and Johnson-McDaniel, N. K. and Jones, D. I. and Jones, R. and Jonker, R. J. G. and Ju, L. and Haris, K. and Kalaghatgi, C. V. and Kalogera, V. and Kandhasamy, S. and Kang, G. and Kanner, J. B. and Karki, S. and Kasprzack, M. and Katsavounidis, E. and Katzman, W. and Kaufer, S. and Kaur, T. and Kawabe, K. and Kawazoe, F. and K\'ef\'elian, F. and Kehl, M. S. and Keitel, D. and Kelley, D. B. and Kells, W. and Kennedy, R. and Keppel, D. G. and Key, J. S. and Khalaidovski, A. and Khalili, F. Y. and Khan, I. and Khan, S. and Khan, Z. and Khazanov, E. A. and Kijbunchoo, N. and Kim, C. and Kim, J. and Kim, K. and Kim, Nam-Gyu and Kim, Namjun and Kim, Y.-M. and King, E. J. and King, P. J. and Kinzel, D. L. and Kissel, J. S. and Kleybolte, L. and Klimenko, S. and Koehlenbeck, S. M. and Kokeyama, K. and Koley, S. and Kondrashov, V. and Kontos, A. and Koranda, S. and Korobko, M. and Korth, W. Z. and Kowalska, I. and Kozak, D. B. and Kringel, V. and Krishnan, B. and Kr\'olak, A. and Krueger, C. and Kuehn, G. and Kumar, P. and Kumar, R. and Kuo, L. and Kutynia, A. and Kwee, P. and Lackey, B. D. and Landry, M. and Lange, J. and Lantz, B. and Lasky, P. D. and Lazzarini, A. and Lazzaro, C. and Leaci, P. and Leavey, S. and Lebigot, E. O. and Lee, C. H. and Lee, H. K. and Lee, H. M. and Lee, K. and Lenon, A. and Leonardi, M. and Leong, J. R. and Leroy, N. and Letendre, N. and Levin, Y. and Levine, B. M. and Li, T. G. F. and Libson, A. and Littenberg, T. B. and Lockerbie, N. A. and Logue, J. and Lombardi, A. L. and London, L. T. and Lord, J. E. and Lorenzini, M. and Loriette, V. and Lormand, M. and Losurdo, G. and Lough, J. D. and Lousto, C. O. and Lovelace, G. and L\"uck, H. and Lundgren, A. P. and Luo, J. and Lynch, R. and Ma, Y. and MacDonald, T. and Machenschalk, B. and MacInnis, M. and Macleod, D. M. and Maga\~na-Sandoval, F. and Magee, R. M. and Mageswaran, M. and Majorana, E. and Maksimovic, I. and Malvezzi, V. and Man, N. and Mandel, I. and Mandic, V. and Mangano, V. and Mansell, G. L. and Manske, M. and Mantovani, M. and Marchesoni, F. and Marion, F. and M\'arka, S. and M\'arka, Z. and Markosyan, A. S. and Maros, E. and Martelli, F. and Martellini, L. and Martin, I. W. and Martin, R. M. and Martynov, D. V. and Marx, J. N. and Mason, K. and Masserot, A. and Massinger, T. J. and Masso-Reid, M. and Matichard, F. and Matone, L. and Mavalvala, N. and Mazumder, N. and Mazzolo, G. and McCarthy, R. and McClelland, D. E. and McCormick, S. and McGuire, S. C. and McIntyre, G. and McIver, J. and McManus, D. J. and McWilliams, S. T. and Meacher, D. and Meadors, G. D. and Meidam, J. and Melatos, A. and Mendell, G. and Mendoza-Gandara, D. and Mercer, R. A. and Merilh, E. and Merzougui, M. and Meshkov, S. and Messenger, C. and Messick, C. and Meyers, P. M. and Mezzani, F. and Miao, H. and Michel, C. and Middleton, H. and Mikhailov, E. E. and Milano, L. and Miller, J. and Millhouse, M. and Minenkov, Y. and Ming, J. and Mirshekari, S. and Mishra, C. and Mitra, S. and Mitrofanov, V. P. and Mitselmakher, G. and Mittleman, R. and Moggi, A. and Mohan, M. and Mohapatra, S. R. P. and Montani, M. and Moore, B. C. and Moore, C. J. and Moraru, D. and Moreno, G. and Morriss, S. R. and Mossavi, K. and Mours, B. and Mow-Lowry, C. M. and Mueller, C. L. and Mueller, G. and Muir, A. W. and Mukherjee, Arunava and Mukherjee, D. and Mukherjee, S. and Mukund, N. and Mullavey, A. and Munch, J. and Murphy, D. J. and Murray, P. G. and Mytidis, A. and Nardecchia, I. and Naticchioni, L. and Nayak, R. K. and Necula, V. and Nedkova, K. and Nelemans, G. and Neri, M. and Neunzert, A. and Newton, G. and Nguyen, T. T. and Nielsen, A. B. and Nissanke, S. and Nitz, A. and Nocera, F. and Nolting, D. and Normandin, M. E. N. and Nuttall, L. K. and Oberling, J. and Ochsner, E. and O'Dell, J. and Oelker, E. and Ogin, G. H. and Oh, J. J. and Oh, S. H. and Ohme, F. and Oliver, M. and Oppermann, P. and Oram, Richard J. and O'Reilly, B. and O'Shaughnessy, R. and Ott, C. D. and Ottaway, D. J. and Ottens, R. S. and Overmier, H. and Owen, B. J. and Pai, A. and Pai, S. A. and Palamos, J. R. and Palashov, O. and Palomba, C. and Pal-Singh, A. and Pan, H. and Pan, Y. and Pankow, C. and Pannarale, F. and Pant, B. C. and Paoletti, F. and Paoli, A. and Papa, M. A. and Paris, H. R. and Parker, W. and Pascucci, D. and Pasqualetti, A. and Passaquieti, R. and Passuello, D. and Patricelli, B. and Patrick, Z. and Pearlstone, B. L. and Pedraza, M. and Pedurand, R. and Pekowsky, L. and Pele, A. and Penn, S. and Perreca, A. and Pfeiffer, H. P. and Phelps, M. and Piccinni, O. and Pichot, M. and Pickenpack, M. and Piergiovanni, F. and Pierro, V. and Pillant, G. and Pinard, L. and Pinto, I. M. and Pitkin, M. and Poeld, J. H. and Poggiani, R. and Popolizio, P. and Post, A. and Powell, J. and Prasad, J. and Predoi, V. and Premachandra, S. S. and Prestegard, T. and Price, L. R. and Prijatelj, M. and Principe, M. and Privitera, S. and Prix, R. and Prodi, G. A. and Prokhorov, L. and Puncken, O. and Punturo, M. and Puppo, P. and P\"urrer, M. and Qi, H. and Qin, J. and Quetschke, V. and Quintero, E. A. and Quitzow-James, R. and Raab, F. J. and Rabeling, D. S. and Radkins, H. and Raffai, P. and Raja, S. and Rakhmanov, M. and Ramet, C. R. and Rapagnani, P. and Raymond, V. and Razzano, M. and Re, V. and Read, J. and Reed, C. M. and Regimbau, T. and Rei, L. and Reid, S. and Reitze, D. H. and Rew, H. and Reyes, S. D. and Ricci, F. and Riles, K. and Robertson, N. A. and Robie, R. and Robinet, F. and Rocchi, A. and Rolland, L. and Rollins, J. G. and Roma, V. J. and Romano, J. D. and Romano, R. and Romanov, G. and Romie, J. H. and Rosi\ifmmode \acute{n}\else \'{n}\fi{}ska, D. and Rowan, S. and R\"udiger, A. and Ruggi, P. and Ryan, K. and Sachdev, S. and Sadecki, T. and Sadeghian, L. and Salconi, L. and Saleem, M. and Salemi, F. and Samajdar, A. and Sammut, L. and Sampson, L. M. and Sanchez, E. J. and Sandberg, V. and Sandeen, B. and Sanders, G. H. and Sanders, J. R. and Sassolas, B. and Sathyaprakash, B. S. and Saulson, P. R. and Sauter, O. and Savage, R. L. and Sawadsky, A. and Schale, P. and Schilling, R. and Schmidt, J. and Schmidt, P. and Schnabel, R. and Schofield, R. M. S. and Sch\"onbeck, A. and Schreiber, E. and Schuette, D. and Schutz, B. F. and Scott, J. and Scott, S. M. and Sellers, D. and Sengupta, A. S. and Sentenac, D. and Sequino, V. and Sergeev, A. and Serna, G. and Setyawati, Y. and Sevigny, A. and Shaddock, D. A. and Shaffer, T. and Shah, S. and Shahriar, M. S. and Shaltev, M. and Shao, Z. and Shapiro, B. and Shawhan, P. and Sheperd, A. and Shoemaker, D. H. and Shoemaker, D. M. and Siellez, K. and Siemens, X. and Sigg, D. and Silva, A. D. and Simakov, D. and Singer, A. and Singer, L. P. and Singh, A. and Singh, R. and Singhal, A. and Sintes, A. M. and Slagmolen, B. J. J. and Smith, J. R. and Smith, M. R. and Smith, N. D. and Smith, R. J. E. and Son, E. J. and Sorazu, B. and Sorrentino, F. and Souradeep, T. and Srivastava, A. K. and Staley, A. and Steinke, M. and Steinlechner, J. and Steinlechner, S. and Steinmeyer, D. and Stephens, B. C. and Stevenson, S. P. and Stone, R. and Strain, K. A. and Straniero, N. and Stratta, G. and Strauss, N. A. and Strigin, S. and Sturani, R. and Stuver, A. L. and Summerscales, T. Z. and Sun, L. and Sutton, P. J. and Swinkels, B. L. and Szczepa\ifmmode \acute{n}\else \'{n}\fi{}czyk, M. J. and Tacca, M. and Talukder, D. and Tanner, D. B. and T\'apai, M. and Tarabrin, S. P. and Taracchini, A. and Taylor, R. and Theeg, T. and Thirugnanasambandam, M. P. and Thomas, E. G. and Thomas, M. and Thomas, P. and Thorne, K. A. and Thorne, K. S. and Thrane, E. and Tiwari, S. and Tiwari, V. and Tokmakov, K. V. and Tomlinson, C. and Tonelli, M. and Torres, C. V. and Torrie, C. I. and T\"oyr\"a, D. and Travasso, F. and Traylor, G. and Trifir\`o, D. and Tringali, M. C. and Trozzo, L. and Tse, M. and Turconi, M. and Tuyenbayev, D. and Ugolini, D. and Unnikrishnan, C. S. and Urban, A. L. and Usman, S. A. and Vahlbruch, H. and Vajente, G. and Valdes, G. and Vallisneri, M. and van Bakel, N. and van Beuzekom, M. and van den Brand, J. F. J. and Van Den Broeck, C. and Vander-Hyde, D. C. and van der Schaaf, L. and van Heijningen, J. V. and van Veggel, A. A. and Vardaro, M. and Vass, S. and Vas\'uth, M. and Vaulin, R. and Vecchio, A. and Vedovato, G. and Veitch, J. and Veitch, P. J. and Venkateswara, K. and Verkindt, D. and Vetrano, F. and Vicer\'e, A. and Vinciguerra, S. and Vine, D. J. and Vinet, J.-Y. and Vitale, S. and Vo, T. and Vocca, H. and Vorvick, C. and Voss, D. and Vousden, W. D. and Vyatchanin, S. P. and Wade, A. R. and Wade, L. E. and Wade, M. and Waldman, S. J. and Walker, M. and Wallace, L. and Walsh, S. and Wang, G. and Wang, H. and Wang, M. and Wang, X. and Wang, Y. and Ward, H. and Ward, R. L. and Warner, J. and Was, M. and Weaver, B. and Wei, L.-W. and Weinert, M. and Weinstein, A. J. and Weiss, R. and Welborn, T. and Wen, L. and We\ss{}els, P. and Westphal, T. and Wette, K. and Whelan, J. T. and Whitcomb, S. E. and White, D. J. and Whiting, B. F. and Wiesner, K. and Wilkinson, C. and Willems, P. A. and Williams, L. and Williams, R. D. and Williamson, A. R. and Willis, J. L. and Willke, B. and Wimmer, M. H. and Winkelmann, L. and Winkler, W. and Wipf, C. C. and Wiseman, A. G. and Wittel, H. and Woan, G. and Worden, J. and Wright, J. L. and Wu, G. and Yablon, J. and Yakushin, I. and Yam, W. and Yamamoto, H. and Yancey, C. C. and Yap, M. J. and Yu, H. and Yvert, M. and Zadro\ifmmode \dot{z}\else \.{z}\fi{}ny, A. and Zangrando, L. and Zanolin, M. and Zendri, J.-P. and Zevin, M. and Zhang, F. and Zhang, L. and Zhang, M. and Zhang, Y. and Zhao, C. and Zhou, M. and Zhou, Z. and Zhu, X. J. and Zucker, M. E. and Zuraw, S. E. and Zweizig, J.},
  title         = {Observation of Gravitational Waves from a Binary Black Hole Merger},
  journaltitle  = {Physical Review Letters},
  year          = {2016},
  volume        = {116},
  number        = {6},
  issue         = {6},
  month         = feb,
  pages         = {061102},
  doi           = {10.1103/PhysRevLett.116.061102},
  citedate      = {2016.03.23},
  collaboration = {LIGO Scientific Collaboration and Virgo Collaboration},
  numpages      = {16},
  publisher     = {American Physical Society},
}

@Online{2016s,
  title     = {PyPI - the Python Package Index: Python Package Index},
  year      = {2016},
  url       = {https://pypi.python.org/pypi},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016t,
  title     = {The Kepler Project --- kepler},
  year      = {2016},
  url       = {https://kepler-project.org/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016v,
  title     = {Web App Service | Microsoft Azure},
  year      = {2016},
  url       = {https://azure.microsoft.com/services/app-service/web/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Article{Merkel2014,
  author       = {Merkel, Dirk},
  title        = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
  journaltitle = {Linux Journal},
  year         = {2014},
  volume       = {2014},
  number       = {239},
  month        = mar,
  issn         = {1075-3583},
  url          = {http://www.linuxjournal.com/content/docker-lightweight-linux-containers-consistent-development-and-deployment},
  acmid        = {2600241},
  articleno    = {2},
  citedate     = {2016.03.23},
  issue_date   = {March 2014},
  publisher    = {Belltown Media},
}

@Article{Ananthakrishnan2014,
  author       = {Rachana Ananthakrishnan and Kyle Chard and Ian Foster and Steven Tuecke},
  title        = {Globus Platform-as-a-Service for Collaborative Science Applications},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2014},
  volume       = {27},
  number       = {2},
  issue        = {2},
  pages        = {290--305},
  doi          = {10.1002/cpe.3262},
  citedate     = {2016.03.23},
  keywords     = {globus, open source, software, software as a service, cloud, scientific software},
  owner        = {tefx},
  publisher    = {John Wiley \& Sons, Inc},
  timestamp    = {2016.03.04},
}

@Article{Durillo2014a,
  author       = {Durillo, JuanJ. and Prodan, Radu},
  title        = {Multi-objective workflow scheduling in Amazon EC2},
  journaltitle = {Cluster Computing},
  year         = {2014},
  language     = {English},
  volume       = {17},
  number       = {2},
  pages        = {169--189},
  issn         = {1386-7857},
  doi          = {10.1007/s10586-013-0325-0},
  citedate     = {2016.03.23},
  keywords     = {Workflow scheduling; Cloud; Multi-objective optimisation; List-based heuristics},
  owner        = {tefx},
  publisher    = {Springer},
  timestamp    = {2016.01.30},
}

@Book{Taylor2014,
  author    = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
  title     = {Workflows for e-Science: Scientific Workflows for Grids},
  year      = {2014},
  publisher = {Springer},
  location  = {London},
  isbn      = {1849966192, 9781849966191},
  address   = {London},
  citedate  = {2016.03.23},
}

@Article{Juve2013a,
  author       = {Juve, Gideon and Rynge, Mats and Deelman, Ewa and Vockler, Jens-S. and Berriman, G. Bruce},
  title        = {Comparing FutureGrid, Amazon EC2, and Open Science Grid for Scientific Workflows},
  journaltitle = {Computing in Science Engineering},
  year         = {2013},
  volume       = {15},
  number       = {4},
  pages        = {20--29},
  doi          = {10.1109/MCSE.2013.44},
  citedate     = {2016.03.23},
  file         = {:PDF/Juve2013.pdf:PDF},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Article{Abrishami2013,
  author       = {Saeid Abrishami and Mahmoud Naghibzadeh and Dick H.J. Epema},
  title        = {Deadline-constrained workflow scheduling algorithms for Infrastructure as a Service Clouds},
  journaltitle = {Future Generation Computer Systems},
  year         = {2013},
  volume       = {29},
  number       = {1},
  pages        = {158--169},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2012.05.004},
  abstract     = {The advent of Cloud computing as a new model of service provisioning in distributed systems encourages researchers to investigate its benefits and drawbacks on executing scientific applications such as workflows. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS requirements of the user as well as minimizing the cost of workflow execution. We have previously designed and analyzed a two-phase scheduling algorithm for utility Grids, called Partial Critical Paths (PCP), which aims to minimize the cost of workflow execution while meeting a user-defined deadline. However, we believe Clouds are different from utility Grids in three ways: on-demand resource provisioning, homogeneous networks, and the pay-as-you-go pricing model. In this paper, we adapt the \{PCP\} algorithm for the Cloud environment and propose two workflow scheduling algorithms: a one-phase algorithm which is called IaaS Cloud Partial Critical Paths (IC-PCP), and a two-phase algorithm which is called IaaS Cloud Partial Critical Paths with Deadline Distribution (IC-PCPD2). Both algorithms have a polynomial time complexity which make them suitable options for scheduling large workflows. The simulation results show that both algorithms have a promising performance, with IC-PCP performing better than IC-PCPD2 in most cases. },
  citedate     = {2016.03.23},
  keywords     = {Cloud computing;IaaS Clouds;Grid computing;Workflow scheduling;QoS-based scheduling },
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Article{Hiden2013,
  author       = {Hiden, Hugo and Woodman, Simon and Watson, Paul and Cala, Jacek},
  title        = {Developing cloud applications using the e-science central platform},
  journaltitle = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  year         = {2013},
  volume       = {371},
  number       = {1983},
  pages        = {20120085},
  doi          = {10.1098/rsta.2012.0085},
  citedate     = {2016.03.23},
  publisher    = {The Royal Society},
}

@Online{Mar2013,
  author   = {de la Mar, Jurry and Schirpke, Bernd and Casu, Francesco and Manunta, Michele},
  title    = {Helix Nebula --- The Science Cloud},
  year     = {2013},
  url      = {http://www.helix-nebula.eu/},
  month    = feb,
  citedate = {2016.02.15},
}

@InProceedings{Sobie2013,
  author    = {Sobie, Randall and Agarwal, Ashok and Gable, Ian and Leavett-Brown, Colin and Paterson, Michael and Taylor, Ryan and Charbonneau, Andre and Impey, Roger and Podiama, Wayne},
  title     = {HTC Scientific Computing in a Distributed Cloud Environment},
  booktitle = {Proceedings of the 4\textsuperscript{th} ACM Workshop on Scientific Cloud Computing},
  year      = {2013},
  publisher = {ACM},
  location  = {New York},
  isbn      = {978-1-4503-1979-9},
  pages     = {45--52},
  doi       = {10.1145/2465848.2465850},
  acmid     = {2465850},
  address   = {New York},
  citedate  = {2016.03.23},
  keywords  = {cloud computing},
  numpages  = {8},
}

@InProceedings{Malawski2012,
  author    = {Malawski, Maciej and Juve, Gideon and Deelman, Ewa and Nabrzyski, Jarek},
  title     = {Cost- and Deadline-constrained Provisioning for Scientific Workflow Ensembles in IaaS Clouds},
  booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
  year      = {2012},
  publisher = {IEEE},
  location  = {Salt Lake City, Utah},
  isbn      = {978-1-4673-0804-5},
  pages     = {221--2211},
  doi       = {10.1109/SC.2012.38},
  acmid     = {2389026},
  address   = {Los Alamitos, CA},
  articleno = {22},
  citedate  = {2016.03.23},
  file      = {Malawski2012.pdf:PDF/Malawski2012.pdf:PDF},
  numpages  = {11},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@InProceedings{Roloff2012,
  author    = {Roloff, E. and Birck, F. and Diener, M. and Carissimi, A. and Navaux, P.O.A.},
  title     = {Evaluating High Performance Computing on the Windows Azure Platform},
  booktitle = {Proceedings of the 5\textsuperscript{th} IEEE International Conference on Cloud Computing},
  year      = {2012},
  publisher = {IEEE},
  location  = {Honolulu, Hawaii},
  month     = jun,
  pages     = {803--810},
  doi       = {10.1109/CLOUD.2012.47},
  abstract  = {Using the Cloud Computing paradigm for High-Performance Computing (HPC) is currently a hot topic in the research community and the industry. The attractiveness of Cloud Computing for HPC is the capability to run large applications on powerful, scalable hardware without needing to actually own or maintain this hardware. Most current research focuses on running HPC applications on the Amazon Cloud Computing platform, which is relatively easy because it supports environments that are similar to existing HPC solutions, such as clusters and supercomputers. In this paper, we evaluate the possibility of using Microsoft Windows Azure as a platform for HPC applications. Since most HPC applications are based on the Unix programming model, their source code has to be ported to the Windows programming model in addition to porting it to the Azure platform. We outline the challenges we encountered during porting applications and their resolutions. Furthermore, we introduce a metric to measure the efficiency of Cloud Computing platforms in terms of performance and price. We compared the performance and efficiency of running these benchmarks on a real machine, an Amazon EC2 instance and a Windows Azure instance. Results show that the performance of Azure is close to the performance of running on real machines, and that it is a viable alternative for running HPC applications when compared to other Cloud Computing solutions.},
  citedate  = {2016.03.23},
  issn      = {2159-6182},
  keywords  = {Unix;cloud computing;Amazon cloud computing platform;HPC;Microsoft Windows Azure;Unix programming model;Windows Azure instance;Windows programming model;cloud computing paradigm;evaluating high performance computing;supercomputers;windows azure platform;Benchmark testing;Cloud computing;Computational modeling;Libraries;Measurement;Operating systems;Programming},
}

@Article{Wang2012,
  author       = {Lei Wang and Jianfeng Zhan and Weisong Shi and Yi Liang},
  title        = {In Cloud, Can Scientific Communities Benefit from the Economies of Scale?},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  year         = {2012},
  volume       = {23},
  number       = {2},
  month        = feb,
  pages        = {296--303},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2011.144},
  abstract     = {The basic idea behind cloud computing is that resource providers offer elastic resources to end users. In this paper, we intend to answer one key question to the success of cloud computing: in cloud, can small-to-medium scale scientific communities benefit from the economies of scale? Our research contributions are threefold: first, we propose an innovative public cloud usage model for small-to-medium scale scientific communities to utilize elastic resources on a public cloud site while maintaining their flexible system controls, i.e., create, activate, suspend, resume, deactivate, and destroy their high-level management entities-service management layers without knowing the details of management. Second, we design and implement an innovative system-DawningCloud, at the core of which are lightweight service management layers running on top of a common management service framework. The common management service framework of DawningCloud not only facilitates building lightweight service management layers for heterogeneous workloads, but also makes their management tasks simple. Third, we evaluate the systems comprehensively using both emulation and real experiments. We found that for four traces of two typical scientific workloads: High-Throughput Computing (HTC) and Many-Task Computing (MTC), DawningCloud saves the resource consumption maximally by 59.5 and 72.6 percent for HTC and MTC service providers, respectively, and saves the total resource consumption maximally by 54 percent for the resource provider with respect to the previous two public cloud solutions. To this end, we conclude that small-to-medium scale scientific communities indeed can benefit from the economies of scale of public clouds with the support of the enabling system.},
  citedate     = {2016.03.23},
  keywords     = {cloud computing;economies of scale;DawningCloud;cloud computing;common management service framework;economies of scale;elastic resource utilization;flexible system controls;high-level management entities;high-throughput computing;innovative public cloud usage model;many-task computing;service management layers;small-to-medium scale scientific communities;Cloud computing;Dynamic scheduling;Economies of scale;Investments;Resource management;Cloud;and high-throughput computing.;economies of scale;many-task computing;scientific communities},
  publisher    = {IEEE},
}

@InProceedings{Zaharia2012,
  author    = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  title     = {Resilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing},
  booktitle = {Proceedings of the 9\textsuperscript{th} USENIX Conference on Networked Systems Design and Implementation},
  year      = {2012},
  publisher = {USENIX Association},
  location  = {San Jose, CA},
  pages     = {2--2},
  url       = {http://dl.acm.org/citation.cfm?id=2228301},
  acmid     = {2228301},
  address   = {Berkeley},
  citedate  = {2016.03.23},
  numpages  = {1},
}

@InProceedings{Fan2012,
  author    = {Pei Fan and Zhenbang Chen and Ji Wang and Zibin Zheng and Lyu, M.R.},
  title     = {Topology-Aware Deployment of Scientific Applications in Cloud Computing},
  booktitle = {Proceedings of the 5\textsuperscript{th} IEEE International Conference on Cloud Computing},
  year      = {2012},
  publisher = {IEEE},
  location  = {Honolulu, Hawaii},
  month     = jun,
  pages     = {319--326},
  doi       = {10.1109/CLOUD.2012.70},
  abstract  = {Nowadays, more and more scientific applications are moving to cloud computing. The optimal deployment of scientific applications is critical for providing good services to users. Scientific applications are usually topology-aware applications. Therefore, considering the topology of a scientific application during the development will benefit the performance of the application. However, it is challenging to automatically discover and make use of the communication pattern of a scientific application while deploying the application on cloud. To attack this challenge, in this paper, we propose a framework to discover the communication topology of a scientific application by pre-execution and multi-scale graph clustering, based on which the deployment can be optimized. Comprehensive experiments are conducted by employing a well-known MPI benchmark and comparing the performance of our method with those of other methods. The experimental results show the effectiveness of our topology-aware deployment method.},
  citedate  = {2016.03.23},
  issn      = {2159-6182},
  keywords  = {application program interfaces;cloud computing;graph theory;message passing;natural sciences computing;pattern clustering;MPI benchmark;cloud computing;communication pattern;communication topology;multiscale graph clustering;preexecution graph clustering;scientific applications;topology-aware deployment method;Benchmark testing;Cloud computing;Clustering algorithms;Clustering methods;Partitioning algorithms;Throughput;Topology;Topology-aware;cloud computing;communication topology;deployment;scientific applications},
}

@Article{Behnel2011,
  author       = {Behnel, S. and Bradshaw, R. and Citro, C. and Dalcin, L. and Seljebotn, D.S. and Smith, K.},
  title        = {Cython: The Best of Both Worlds},
  journaltitle = {Computing in Science Engineering},
  year         = {2011},
  volume       = {13},
  number       = {2},
  month        = mar,
  pages        = {31--39},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2010.118},
  abstract     = {Cython is a Python language extension that allows explicit type declarations and is compiled directly to C. As such, it addresses Python's large overhead for numerical loops and the difficulty of efficiently using existing C and Fortran code, which Cython can interact with natively.},
  citedate     = {2016.03.23},
  keywords     = {C language;numerical analysis;Cython language;Fortran code;Python language extension;numerical loops;programming language;Cython;Python;numerics;scientific computing},
  publisher    = {IEEE},
}

@Article{Ramachandran2011,
  author       = {Ramachandran, P. and Varoquaux, G.},
  title        = {Mayavi: {3D} Visualization of Scientific Data},
  journaltitle = {Computing in Science Engineering},
  year         = {2011},
  volume       = {13},
  number       = {2},
  month        = mar,
  pages        = {40--51},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2011.35},
  abstract     = {Mayavi is a general purpose, open source 3D scientific visualization package that is tightly integrated with the rich ecosystem of Python scientific packages. Mayavi provides a continuum of tools for developing scientific applications, ranging from interactive and script-based data visualization in Python to full-blown custom end-user applications.},
  citedate     = {2016.03.23},
  keywords     = {data visualisation;natural sciences computing;public domain software;3D scientific data visualization;Mayavi;Python scientific packages;open source 3D scientific visualization package;Data visualization;Image color analysis;Science - general;Scientific computing;Three dimensional displays;Python;Visualization;scientific computing;software engineering},
  publisher    = {IEEE},
}

@Article{Dalcin2011,
  author       = {Lisandro D. Dalcin and Rodrigo R. Paz and Pablo A. Kler and Alejandro Cosimo},
  title        = {Parallel distributed computing using Python},
  journaltitle = {Advances in Water Resources},
  year         = {2011},
  volume       = {34},
  number       = {9},
  pages        = {1124--1139},
  note         = {New Computational Methods and Software Tools},
  issn         = {0309-1708},
  doi          = {10.1016/j.advwatres.2011.04.013},
  abstract     = {This work presents two software components aimed to relieve the costs of accessing high-performance parallel computing resources within a Python programming environment: \{MPI\} for Python and \{PETSc\} for Python. \{MPI\} for Python is a general-purpose Python package that provides bindings for the Message Passing Interface (MPI) standard using any back-end \{MPI\} implementation. Its facilities allow parallel Python programs to easily exploit multiple processors using the message passing paradigm. \{PETSc\} for Python provides access to the Portable, Extensible Toolkit for Scientific Computation (PETSc) libraries. Its facilities allow sequential and parallel Python applications to exploit state of the art algorithms and data structures readily available in \{PETSc\} for the solution of large-scale problems in science and engineering. \{MPI\} for Python and \{PETSc\} for Python are fully integrated to PETSc-FEM, an \{MPI\} and \{PETSc\} based parallel, multiphysics, finite elements code developed at \{CIMEC\} laboratory. This software infrastructure supports research activities related to simulation of fluid flows with applications ranging from the design of microfluidic devices for biochemical analysis to modeling of large-scale stream/aquifer interactions. },
  citedate     = {2016.03.23},
  keywords     = {Python},
  publisher    = {Elsevier},
}

@Article{Millman2011,
  author       = {Millman, K. Jarrod and Aivazis, Michael},
  title        = {Python for Scientists and Engineers},
  journaltitle = {Computing in Science Engineering},
  year         = {2011},
  volume       = {13},
  number       = {2},
  month        = mar,
  pages        = {9--12},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2011.36},
  abstract     = {Python has arguably become the de facto standard for exploratory, interactive, and computation-driven scientific research. This issue discusses Python's advantages for scientific research and presents several of the core Python libraries and tools used in scientific research.},
  citedate     = {2016.03.23},
  keywords     = {Computer languages;Numerical models;Programming;Scientific computing;Special issues and sections;Programming languages;Python;Python libraries;Python tools;Scientific computing;interactive research},
  publisher    = {IEEE},
}

@Article{Pérez2011,
  author       = {Pérez, F. and Granger, B.E. and Hunter, J.D.},
  title        = {Python: An Ecosystem for Scientific Computing},
  journaltitle = {Computing in Science Engineering},
  year         = {2011},
  volume       = {13},
  number       = {2},
  month        = mar,
  pages        = {13--21},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2010.119},
  abstract     = {As the relationship between research and computing evolves, new tools are required to not only treat numerical problems, but also to solve various problems that involve large datasets in different formats, new algorithms, and computational systems such as databases and Internet servers. Python can help develop these computational research tools by providing a balance of clarity and flexibility without sacrificing performance.},
  citedate     = {2016.03.23},
  keywords     = {object-oriented languages;Python;computational research tool;large dataset;numerical problem;scientific computing;Programming environments;Python;arrays;data structure;high-level languages;language classifications;object-oriented languages;programming languages;scientific computing;software engineering},
  publisher    = {IEEE},
}

@Article{Pedregosa2011,
  author       = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
  title        = {Scikit-learn: Machine Learning in Python},
  journaltitle = {Journal of Machine Learning Research},
  year         = {2011},
  volume       = {12},
  month        = nov,
  pages        = {2825--2830},
  issn         = {1532-4435},
  url          = {http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
  acmid        = {2078195},
  citedate     = {2016.03.23},
  issue_date   = {2/1/2011},
  numpages     = {6},
  publisher    = {Microtome Publishing},
}

@Article{Walt2011,
  author       = {van der Walt, S. and Colbert, S.C. and Varoquaux, G.},
  title        = {The NumPy Array: A Structure for Efficient Numerical Computation},
  journaltitle = {Computing in Science Engineering},
  year         = {2011},
  volume       = {13},
  number       = {2},
  month        = mar,
  pages        = {22--30},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2011.37},
  abstract     = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
  citedate     = {2016.03.23},
  keywords     = {data structures;high level languages;mathematics computing;numerical analysis;Python programming language;high level language;numerical computation;numerical data;numpy array;Arrays;Computational efficiency;Finite element methods;Numerical analysis;Performance evaluation;Resource management;Vector quantization;NumPy;Python;numerical computations;programming libraries;scientific programming},
  publisher    = {IEEE},
}

@InCollection{Ishibuchi2010,
  author    = {Ishibuchi, Hisao and Hitotsuyanagi, Yasuhiro and Tsukamoto, Noritaka and Nojima, Yusuke},
  title     = {Many-Objective Test Problems to Visually Examine the Behavior of Multiobjective Evolution in a Decision Space},
  booktitle = {Parallel Problem Solving from Nature (PPSN XI)},
  year      = {2010},
  language  = {English},
  volume    = {6239},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  isbn      = {978-3-642-15870-4},
  pages     = {91--100},
  doi       = {10.1007/978-3-642-15871-1_10},
  citedate  = {2016.03.23},
  keywords  = {Evolutionary multiobjective optimization (EMO); many-objective optimization; multiobjective optimization problems; test problems},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Nilsen2010,
  author       = {Jon K Nilsen and Xing Cai and Bj{\~{A}}{\c{}}rn H{\~{A}}{\c{}}yland and Hans Petter Langtangen},
  title        = {Simplifying the parallelization of scientific codes by a function-centric approach in Python},
  journaltitle = {Computational Science \& Discovery},
  year         = {2010},
  volume       = {3},
  number       = {1},
  pages        = {015003},
  doi          = {10.1088/1749-4699/3/1/015003},
  abstract     = {The purpose of this paper is to show how existing scientific software can be parallelized using a separate thin layer of Python code where all parallelization-specific tasks are implemented. We provide specific examples of such a Python code layer, which can act as templates for parallelizing a wide set of serial scientific codes. The use of Python for parallelization is motivated by the fact that the language is well suited for reusing existing serial codes programmed in other languages. The extreme flexibility of Python with regard to handling functions makes it very easy to wrap up decomposed computational tasks of a serial scientific application as Python functions. Many parallelization-specific components can be implemented as generic Python functions, which may take as input those wrapped functions that perform concrete computational tasks. The overall programming effort needed by this parallelization approach is limited, and the resulting parallel Python scripts have a compact and clean structure. The usefulness of the parallelization approach is exemplified by three different classes of application in natural and social sciences.},
  citedate     = {2016.03.23},
  publisher    = {IOP Publishing},
}

@InProceedings{Zhang2010,
  author    = {Shuai Zhang and Xuebin Chen and Shufen Zhang and Xiuzhen Huo},
  title     = {The comparison between cloud computing and grid computing},
  booktitle = {Proceedings of the International Conference on Computer Application and System Modeling},
  year      = {2010},
  volume    = {11},
  publisher = {IEEE},
  location  = {Taiyuan, China},
  month     = oct,
  pages     = {V11-72--V11-75},
  doi       = {10.1109/ICCASM.2010.5623257},
  abstract  = {It is a great idea to make many normal computers together to get a super computer, and this computer can do a lot of things. This is the concept of cloud computing. Cloud computing is an emerging model of business computing. And it is becoming a development trend. This article compares cloud computing and grid computing. Internet has connected all the computers in the world. Grid computing has been put forward under this background. Its core concept is to complete computing based on compute grid, in it every computer will devote power. In recent years a new concept cloud computing has been put forward, it can connect millions of computers to a super cloud. This article also introduces the application field the merit of cloud computing, such as, it do not need user's high level equipment, so it reduces the user's cost. It provides secure and dependable data storage center, so user needn't do the awful things such storing data and killing virus, this kind of task can be done by professionals. It can realize data share through different equipments. The users need not know how the cloud runs. In this paper I describe the concept of cloud computing and grid computing and compare them.},
  citedate  = {2016.03.23},
  keywords  = {Internet;grid computing;Internet;cloud computing;grid computing;Cloud computing;Clouds;Computers;Grid computing;Servers;Software;Cloud Computing;Distributed computer;Grid computing},
}

@Online{Venkat2009,
  author    = {Venkat, Girish},
  title     = {Loudcloud: Early light on cloud computing},
  year      = {2009},
  url       = {http://news.cnet.com/8301-1001_3-10202058-92.html},
  month     = mar,
  citedate  = {2016.04.30},
  owner     = {tefx},
  publisher = {CNET},
  timestamp = {2016.01.30},
}

@Online{Helsley2009,
  author   = {Helsley, Matt},
  title    = {LXC: Linux container tools},
  year     = {2009},
  url      = {http://www.ibm.com/developerworks/library/l-lxc-containers/index.html},
  citedate = {2016.04.30},
}

@Article{Talukder2009,
  author       = {Talukder, A. K. M. Khaled Ahsan and Kirley, Michael and Buyya, Rajkumar},
  title        = {Multiobjective differential evolution for scheduling workflow applications on global Grids},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2009},
  volume       = {21},
  number       = {13},
  month        = sep,
  pages        = {1742--1756},
  issn         = {1532-0634},
  doi          = {10.1002/cpe.1417},
  citedate     = {2016.03.23},
  owner        = {tefx},
  publisher    = {John Wiley \& Sons, Inc},
  timestamp    = {2016.01.30},
}

@InProceedings{Keahey2009,
  author    = {Keahey, Kate},
  title     = {Nimbus: open source infrastructure-as-a-service cloud computing software},
  booktitle = {Proceedings of the Workshop on adapting applications and computing services to multi-core and virtualization},
  year      = {2009},
  publisher = {CERN},
  location  = {Switzerland},
  url       = {http://www.nimbusproject.org/files/nimbus-cern_June2009.pdf},
  citedate  = {2016.03.23},
}

@InProceedings{Hagberg2008,
  author    = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
  title     = {Exploring Network Structure, Dynamics, and Function using NetworkX},
  booktitle = {Proceedings of the 7th Python in Science Conference},
  year      = {2008},
  editor    = {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman},
  publisher = {SciPy},
  location  = {Pasadena, CA},
  pages     = {11--15},
  url       = {http://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/LA-UR-08-05495},
  citedate  = {2016.03.23},
}

@Article{Dean2008,
  author       = {Dean, Jeffrey and Ghemawat, Sanjay},
  title        = {MapReduce: simplified data processing on large clusters},
  journaltitle = {Communications of the ACM},
  year         = {2008},
  volume       = {51},
  number       = {1},
  pages        = {107--113},
  doi          = {10.1145/1327452.1327492},
  citedate     = {2016.03.23},
  publisher    = {ACM},
}

@InProceedings{Skomoroch2008,
  author    = {Skomoroch, Pete},
  title     = {{MPI} cluster programming with Python and Amazon EC2},
  booktitle = {Proceedings of the 6\textsuperscript{th} Annual Python Community Conference},
  year      = {2008},
  publisher = {Python Software Foundation},
  location  = {Chicago},
  url       = {http://datawrangling.s3.amazonaws.com/elasticwulf_pycon_talk.pdf},
  citedate  = {2016.03.23},
}

@Article{Dalcin2008,
  author       = {Dalc{\'\i}n, Lisandro and Paz, Rodrigo and Storti, Mario and D’El{\'\i}a, Jorge},
  title        = {{MPI} for Python: Performance improvements and {MPI}-2 extensions},
  journaltitle = {Journal of Parallel and Distributed Computing},
  year         = {2008},
  volume       = {68},
  number       = {5},
  pages        = {655--662},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2007.09.005},
  abstract     = {MPI for Python provides bindings of the message passing interface (MPI) standard for the Python programming language and allows any Python program to exploit multiple processors. In its first release, \{MPI\} for Python was constructed on top of the MPI-1 specification defining an object-oriented interface that closely followed the MPI-2 C ++ bindings, and provided support for communications of general Python objects. In the latest release, this package is improved to enable direct blocking/non-blocking communication of numeric arrays, and to support almost all MPI-2 features. Improvements in communication performance have been tested in a Beowulf class cluster. Results showed a negligible overhead in comparison to compiled C code. \{MPI\} for Python is open source and available for download on the web (http://mpi4py.scipy.org/).},
  citedate     = {2016.03.23},
  keywords     = {Message passing},
  publisher    = {Elsevier},
}

@InCollection{Barker2008,
  author    = {Barker, Adam and Van Hemert, Jano},
  title     = {Scientific workflow: a survey and research directions},
  booktitle = {Parallel Processing and Applied Mathematics},
  year      = {2008},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  pages     = {746--753},
  doi       = {10.1007/978-3-540-68111-3_78},
  citedate  = {2016.03.23},
}

@Article{Pérez2007,
  author       = {Pérez, F. and Granger, B.E.},
  title        = {IPython: A System for Interactive Scientific Computing},
  journaltitle = {Computing in Science Engineering},
  year         = {2007},
  volume       = {9},
  number       = {3},
  month        = may,
  pages        = {21--29},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2007.53},
  abstract     = {Python offers basic facilities for interactive work and a comprehensive library on top of which more sophisticated systems can be built. The IPython project provides on enhanced interactive environment that includes, among other features, support for data visualization and facilities for distributed and parallel computation},
  citedate     = {2016.03.23},
  keywords     = {data visualisation;natural sciences computing;object-oriented languages;object-oriented programming;parallel programming;software libraries;IPython;comprehensive library;data visualization;distributed computation;interactive scientific computing;parallel computation;Data analysis;Data visualization;Hardware;Libraries;Parallel processing;Production;Scientific computing;Spine;Supercomputers;Testing;Python;computer languages;scientific computing;scientific programming},
  publisher    = {IEEE},
}

@InProceedings{Zhao2007a,
  author    = {Y. Zhao and M. Hategan and B. Clifford and I. Foster and G. von Laszewski and V. Nefedova and I. Raicu and T. Stef-Praun and M. Wilde},
  title     = {Swift: Fast, Reliable, Loosely Coupled Parallel Computation},
  booktitle = {Proceedings of the IEEE Congress on Services},
  year      = {2007},
  publisher = {IEEE},
  location  = {Salt Lake City},
  month     = jul,
  pages     = {199--206},
  doi       = {10.1109/SERVICES.2007.63},
  abstract  = {We present Swift, a system that combines a novel scripting language called SwiftScript with a powerful runtime system based on CoG Karajan, Falkon, and Globus to allow for the concise specification, and reliable and efficient execution, of large loosely coupled computations. Swift adopts and adapts ideas first explored in the GriPhyN virtual data system, improving on that system in many regards. We describe the SwiftScript language and its use of XDTM to describe the logical structure of complex file system structures. We also present the Swift runtime system and its use of CoG Karajan, Falkon, and Globus services to dispatch and manage the execution of many tasks in parallel and grid environments. We describe application experiences and performance experiments that quantify the cost of Swift operations.},
  citedate  = {2016.03.23},
  keywords  = {formal specification;grid computing;software reliability;GriPhyN virtual data system;Swift system;SwiftScript language;complex file system structures;runtime system;scripting language;Computer science;Concurrent computing;Data systems;Distributed computing;File systems;High performance computing;Laboratories;Magnetic analysis;Mathematics;Power system reliability},
}

@Article{Guan2006,
  author       = {Guan, Zhijie and Hernandez, Francisco and Bangalore, Purushotham and Gray, Jeff and Skjellum, Anthony and Velusamy, Vijay and Liu, Yin},
  title        = {Grid-Flow: a Grid-enabled scientific workflow system with a Petri-net-based interface},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2006},
  volume       = {18},
  number       = {10},
  pages        = {1115--1140},
  doi          = {10.1002/cpe.988},
  citedate     = {2016.03.23},
  publisher    = {John Wiley \& Sons, Inc},
}

@Article{Konak2006,
  author       = {Abdullah Konak and David W. Coit and Alice E. Smith},
  title        = {Multi-objective optimization using genetic algorithms: A tutorial},
  journaltitle = {Reliability Engineering \& System Safety},
  year         = {2006},
  volume       = {91},
  number       = {9},
  pages        = {992--1007},
  note         = {Special Issue---Genetic Algorithms and ReliabilitySpecial Issue - Genetic Algorithms and Reliability},
  issn         = {0951-8320},
  doi          = {10.1016/j.ress.2005.11.018},
  abstract     = {Multi-objective formulations are realistic models for many complex engineering optimization problems. In many real-life problems, objectives under consideration conflict with each other, and optimizing a particular solution with respect to a single objective can result in unacceptable results with respect to the other objectives. A reasonable solution to a multi-objective problem is to investigate a set of solutions, each of which satisfies the objectives at an acceptable level without being dominated by any other solution. In this paper, an overview and tutorial is presented describing genetic algorithms (GA) developed specifically for problems with multiple objectives. They differ primarily from traditional \{GA\} by using specialized fitness functions and introducing methods to promote solution diversity. },
  citedate     = {2016.03.23},
  publisher    = {Elsevier},
}

@Article{Field2006,
  author       = {Field, Dawn and Tiwari, Bela and Booth, Tim and Houten, Stewart and Swan, Dan and Bertrand, Nicolas and Thurston, Milo and others},
  title        = {Open software for biologists: from famine to feast},
  journaltitle = {Nature biotechnology},
  year         = {2006},
  volume       = {24},
  number       = {7},
  pages        = {801--804},
  doi          = {10.1038/nbt0706-801},
  citedate     = {2016.03.23},
  publisher    = {Nature Publishing Group},
}

@Article{Thain2005,
  author       = {Thain, Douglas and Tannenbaum, Todd and Livny, Miron},
  title        = {Distributed computing in practice: the Condor experience},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  year         = {2005},
  volume       = {17},
  number       = {2-4},
  pages        = {323--356},
  issn         = {1532-0634},
  doi          = {10.1002/cpe.938},
  abstract     = {Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational Grid. In this paper, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course travelled by research ideas as they grow into production systems. Copyright {\copyright} 2005 John Wiley & Sons, Ltd.},
  citedate     = {2016.03.23},
  keywords     = {Condor, Grid, history, community, planning, scheduling, split execution},
  publisher    = {John Wiley \& Sons, Ltd.},
}

@InProceedings{Amin2004,
  author    = {Amin, Kaizar and Von Laszewski, Gregor and Hategan, Mihael and Zaluzec, Nestor J and Hampton, Shawn and Rossi, Albert},
  title     = {GridAnt: A client-controllable grid workflow system},
  booktitle = {Proceedings of the 37\textsuperscript{th} Annual Hawaii International Conference on System Sciences},
  year      = {2004},
  publisher = {IEEE},
  location  = {Big Island, Hawaii},
  pages     = {10--pp},
  doi       = {10.1109/HICSS.2004.1265491},
  citedate  = {2016.03.23},
}

@InProceedings{Altintas2004,
  author    = {I. Altintas and C. Berkley and E. Jaeger and M. Jones and B. Ludascher and S. Mock},
  title     = {Kepler: an extensible system for design and execution of scientific workflows},
  booktitle = {Proceedings of the 16\textsuperscript{th} International Conference on Scientific and Statistical Database Management},
  year      = {2004},
  publisher = {IEEE},
  location  = {Santorini Island, Greece},
  month     = jun,
  pages     = {423--424},
  doi       = {10.1109/SSDM.2004.1311241},
  abstract  = {Most scientists conduct analyses and run models in several different software and hardware environments, mentally coordinating the export and import of data from one environment to another. The Kepler scientific workflow system provides domain scientists with an easy-to-use yet powerful system for capturing scientific workflows (SWFs). SWFs are a formalization of the ad-hoc process that a scientist may go through to get from raw data to publishable results. Kepler attempts to streamline the workflow creation and execution process so that scientists can design, execute, monitor, re-run, and communicate analytical procedures repeatedly with minimal effort. Kepler is unique in that it seamlessly combines high-level workflow design with execution and runtime interaction, access to local and remote data, and local and remote service invocation. SWFs are superficially similar to business process workflows but have several challenges not present in the business workflow scenario. For example, they often operate on large, complex and heterogeneous data, can be computationally intensive and produce complex derived data products that may be archived for use in reparameterized runs or other workflows. Moreover, unlike business workflows, SWFs are often dataflow-oriented as witnessed by a number of recent academic systems (e.g., DiscoveryNet, Taverna and Triana) and commercial systems (Scitegic/Pipeline-Pilot, Inforsense). In a sense, SWFs are often closer to signal-processing and data streaming applications than they are to control-oriented business workflow applications.},
  citedate  = {2016.03.23},
  issn      = {1099-3371},
  keywords  = {data flow computing;data handling;database management systems;scientific information systems;workflow management software;DiscoveryNet;Inforsense;Kepler scientific workflow system;Scitegic/Pipeline-Pilot;Taverna;Triana;complex data;data access;data export;data import;data products;data streaming;extensible system;hardware environment;heterogeneous data;high-level workflow design;large data;runtime interaction;scientific analysis;scientific workflow design;scientific workflow execution;service invocation;software environment;workflow creation;Biological system modeling;Business;Java;Plugs;Power system modeling;Prototypes;Runtime;Supercomputers;Web services;Yarn},
}

@Book{Schroeder2004,
  author    = {Schroeder, Will J and Lorensen, Bill and Martin, Ken},
  title     = {The visualization toolkit},
  year      = {2004},
  publisher = {Kitware},
  location  = {New York},
  address   = {New York},
  citedate  = {2016.03.23},
}

@InProceedings{Papazoglou2003,
  author    = {Papazoglou, M.P.},
  title     = {Service-oriented computing: concepts, characteristics and directions},
  booktitle = {Proceedings of the 4\textsuperscript{th} International Conference on Web Information Systems Engineering},
  year      = {2003},
  publisher = {IEEE},
  location  = {Roma},
  month     = dec,
  pages     = {3--12},
  doi       = {10.1109/WISE.2003.1254461},
  abstract  = {Service-oriented computing (SOC) is the computing paradigm that utilizes services as fundamental elements for developing applications/solutions. To build the service model, SOC relies on the service oriented architecture (SOA), which is a way of reorganizing software applications and infrastructure into a set of interacting services. However, the basic SOA does not address overarching concerns such as management, service orchestration, service transaction management and coordination, security, and other concerns that apply to all components in a service architecture. In this paper we introduce an extended service oriented architecture that provides separate tiers for composing and coordinating services and for managing services in an open marketplace by employing grid services.},
  citedate  = {2016.03.23},
  keywords  = {Internet;distributed processing;object-oriented programming;software architecture;application development;computing paradigm;grid services;interacting services;open marketplace;service architecture;service coordination;service management;service orchestration;service oriented architecture;service security;service transaction coordination;service transaction management;service-oriented computing;software applications;software infrastructure;solutions development;Application software;Distributed computing;Grid computing;Handheld computers;Management information systems;Personal digital assistants;Protocols;Service oriented architecture;Standards organizations;Web and internet services},
}

@Article{Anderson2002,
  author       = {Anderson, David P. and Cobb, Jeff and Korpela, Eric and Lebofsky, Matt and Werthimer, Dan},
  title        = {SETI@Home: An Experiment in Public-resource Computing},
  journaltitle = {Communications of the ACM},
  year         = {2002},
  volume       = {45},
  number       = {11},
  month        = nov,
  pages        = {56--61},
  issn         = {0001-0782},
  doi          = {10.1145/581571.581573},
  acmid        = {581573},
  citedate     = {2016.03.23},
  issue_date   = {November 2002},
  numpages     = {6},
  publisher    = {ACM},
}

@Article{Willke2002,
  author       = {Willke, Benno and Aufmuth, Peter and Aulbert, Carsten and Babak, Stanislav and Balasubramanian, Ramachandran and Barr, BW and Berukoff, S and Bose, Sukanta and Cagnoli, G and Casey, Morag M and others},
  title        = {The GEO 600 gravitational wave detector},
  journaltitle = {Classical and Quantum Gravity},
  year         = {2002},
  volume       = {19},
  number       = {7},
  pages        = {1377},
  doi          = {10.1088/0264-9381/19/7/321},
  citedate     = {2016.03.23},
  publisher    = {IOP Publishing},
}

@InBook{Blankenberg2001,
  author    = {Blankenberg, Daniel and Kuster, Gregory Von and Coraor, Nathaniel and Ananda, Guruprasad and Lazarus, Ross and Mangan, Mary and Nekrutenko, Anton and Taylor, James},
  title     = {Galaxy: A Web-Based Genome Analysis Tool for Experimentalists},
  booktitle = {Current Protocols in Molecular Biology},
  year      = {2001},
  publisher = {John Wiley \& Sons, Inc.},
  location  = {Hoboken, NJ},
  isbn      = {9780471142720},
  doi       = {10.1002/0471142727.mb1910s89},
  abstract  = {High-throughput data production has revolutionized molecular biology. However, massive increases in data generation capacity require analysis approaches that are more sophisticated, and often very computationally intensive. Thus, making sense of high-throughput data requires informatics support. Galaxy (http://galaxyproject.org) is a software system that provides this support through a framework that gives experimentalists simple interfaces to powerful tools, while automatically managing the computational details. Galaxy is distributed both as a publicly available Web service, which provides tools for the analysis of genomic, comparative genomic, and functional genomic data, or a downloadable package that can be deployed in individual laboratories. Either way, it allows experimentalists without informatics or programming expertise to perform complex large-scale analysis with just a Web browser. Curr. Protoc. Mol. Biol. 89:19.10.1-19.10.21. {\^{A}}{\copyright} 2010 by John Wiley & Sons, Inc.},
  citedate  = {2016.03.23},
  keywords  = {Galaxy, analysis, bioinformatics, workflow, algorithm, pipeline, genomics, SNPs},
  owner     = {tefx},
  timestamp = {2016.03.02},
}

@InCollection{Corne2000,
  author    = {Corne, DavidW. and Knowles, JoshuaD. and Oates, MartinJ.},
  title     = {The Pareto Envelope-Based Selection Algorithm for Multiobjective Optimization},
  booktitle = {Parallel Problem Solving from Nature (PPSN VI)},
  year      = {2000},
  editor    = {Schoenauer, Marc and Deb, Kalyanmoy and Rudolph, GÃ¼nther and Yao, Xin and Lutton, Evelyne and Merelo, JuanJulian and Schwefel, Hans-Paul},
  language  = {English},
  volume    = {1917},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  isbn      = {978-3-540-41056-0},
  pages     = {839--848},
  doi       = {10.1007/3-540-45356-3_82},
  citedate  = {2016.03.23},
  owner     = {tefx},
  timestamp = {2016.01.05},
}

@Article{Judy1995,
  author       = {Judy, EO and others},
  title        = {The Role of ARPA in the Development of the ARPANET, 1961-1972},
  journaltitle = {IEEE Annals of the History of Computing},
  year         = {1995},
  volume       = {17},
  number       = {4},
  month        = {Winter},
  pages        = {76--81},
  issn         = {1058-6180},
  doi          = {10.1109/85.477437},
  abstract     = {The use of computer networks is growing rapidly throughout our society. Current network technology has its roots in the U.S. Department of Defense, specifically in the Advanced Research Projects Agency and the ARPANET computer network, but designed for other than explicitly military objectives. This paper describes ARPA's motivations for developing the network and how ARPA and computer science researchers built the first wide-area packet-switching network},
  citedate     = {2016.03.23},
  keywords     = {military communication;military computing;packet switching;wide area networks;ARPA;ARPANET;Advanced Research Projects Agency;Department of Defense;computer networks;computer science research;military computer network;wide-area packet-switching network;ARPANET;Buildings;Command and control systems;Computer networks;Computer vision;Contracts;Hardware;Information processing;Programming profession;Time sharing computer systems},
  publisher    = {IEEE},
}

@InProceedings{Pati1993,
  author    = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
  title     = {Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition},
  booktitle = {Proceedings of the 27\textsuperscript{th} Asilomar Conference on Signals, Systems and Computers},
  year      = {1993},
  volume    = {1},
  publisher = {IEEE},
  location  = {Pacific Grove, CA},
  month     = nov,
  pages     = {40--44},
  doi       = {10.1109/ACSSC.1993.342465},
  abstract  = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
  citedate  = {2016.03.23},
  issn      = {1058-6393},
  keywords  = {approximation theory;convergence of numerical methods;recursive estimation;signal representation;wavelet transforms;affine wavelet frames;backward orthogonality;convergence;matching pursuit algorithm;orthogonal matching pursuit;overcomplete dictionaries;recursive algorithm;recursive function approximation;signal representation;wavelet decomposition;Convergence;Dictionaries;Educational institutions;Function approximation;Information systems;Iterative algorithms;Laboratories;Matching pursuit algorithms;Pursuit algorithms;Zinc},
}

@InProceedings{Anderson1990,
  author    = {Anderson, E. and Bai, Z. and Dongarra, J. and Greenbaum, A. and McKenney, A. and Du Croz, J. and Hammerling, S. and Demmel, J. and Bischof, C. and Sorensen, D.},
  title     = {LAPACK: A Portable Linear Algebra Library for High-performance Computers},
  booktitle = {Proceedings of the ACM/IEEE Conference on Supercomputing},
  year      = {1990},
  series    = {Supercomputing '90},
  publisher = {IEEE},
  location  = {New York, New York, USA},
  isbn      = {0-89791-412-0},
  pages     = {2--11},
  doi       = {10.1109/SUPERC.1990.129995},
  acmid     = {110385},
  address   = {Los Alamitos, CA},
  citedate  = {2016.03.23},
  numpages  = {10},
}

@Article{Hughes1989,
  author       = {Hughes, J.},
  title        = {Why Functional Programming Matters},
  journaltitle = {The Computer Journal},
  year         = {1989},
  volume       = {32},
  number       = {2},
  pages        = {98--107},
  doi          = {10.1093/comjnl/32.2.98},
  abstract     = {As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised. Functional languages push those limits back. In this paper we show that two features of functional languages in particular, higher-order functions and lazy evaluation, can contribute greatly to modularity. As examples, we manipulate lists and trees, program several numerical algorithms, and implement the alpha-beta heuristics (an Artificial Intelligence algorithm used in game-playing programs). Since modularity is the key to successful programming, functional languages are vitally important to the real world.},
  citedate     = {2016.03.23},
  publisher    = {Oxford University Press},
}

@Online{Swarztrauber1985,
  author   = {Swarztrauber, PN},
  title    = {FFTPACK, a Package of FORTRAN Subprograms for the Fast {Fourier} Transform of Periodic and Other Symmetric Sequences},
  year     = {1985},
  url      = {http://netlib.org/fftpack},
  citedate = {2016.04.30},
}

@Article{Hochman1969,
  author       = {Hochman, Harold M and Rodgers, James D},
  title        = {Pareto optimal redistribution},
  journaltitle = {The American Economic Review},
  year         = {1969},
  volume       = {59},
  number       = {4},
  pages        = {542--557},
  url          = {http://www.jstor.org/stable/1813216},
  citedate     = {2016.03.23},
  publisher    = {JSTOR},
}

@Article{Ishibuchi2015,
  author       = {Hisao Ishibuchi and Naoya Akedo and Yusuke Nojima},
  title        = {Behavior of Multiobjective Evolutionary Algorithms on Many-Objective Knapsack Problems},
  journaltitle = {{IEEE} Transactions on Evolutionary Computation},
  year         = {2015},
  volume       = {19},
  number       = {2},
  month        = {apr},
  pages        = {264--283},
  doi          = {10.1109/tevc.2014.2315442},
  file         = {:Ishibuchi2015 - Behavior of Multiobjective Evolutionary Algorithms on Many-Objective Knapsack Problems.pdf:PDF},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Li2009,
  author       = {Hui Li and Qingfu Zhang},
  title        = {Multiobjective Optimization Problems With Complicated Pareto Sets, MOEA/D and NSGA-II},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  year         = {2009},
  volume       = {13},
  number       = {2},
  month        = apr,
  pages        = {284--302},
  issn         = {1089-778X},
  doi          = {10.1109/TEVC.2008.925798},
  abstract     = {Partly due to lack of test problems, the impact of the Pareto set (PS) shapes on the performance of evolutionary algorithms has not yet attracted much attention. This paper introduces a general class of continuous multiobjective optimization test instances with arbitrary prescribed PS shapes, which could be used for studying the ability of multiobjective evolutionary algorithms for dealing with complicated PS shapes. It also proposes a new version of MOEA/D based on differential evolution (DE), i.e., MOEA/D-DE, and compares the proposed algorithm with NSGA-II with the same reproduction operators on the test instances introduced in this paper. The experimental results indicate that MOEA/D could significantly outperform NSGA-II on these test instances. It suggests that decomposition based multiobjective evolutionary algorithms are very promising in dealing with complicated PS shapes.},
  citedate     = {2016.03.23},
  file         = {:PDF/Li2009.pdf:PDF},
  keywords     = {Pareto optimisation;evolutionary computation;set theory;MOEA/D;NSGA-II;Pareto sets;differential evolution;evolutionary algorithms;multiobjective optimization problems;Aggregation;Pareto optimality;decomposition;differential evolution;evolutionary algorithms;multiobjective optimization;test problems},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Article{Li2014b,
  author       = {Miqing Li and Shengxiang Yang and Xiaohui Liu},
  title        = {Shift-Based Density Estimation for Pareto-Based Algorithms in Many-Objective Optimization},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  year         = {2014},
  volume       = {18},
  number       = {3},
  month        = jun,
  pages        = {348--365},
  issn         = {1089-778X},
  doi          = {10.1109/TEVC.2013.2262178},
  abstract     = {It is commonly accepted that Pareto-based evolutionary multiobjective optimization (EMO) algorithms encounter difficulties in dealing with many-objective problems. In these algorithms, the ineffectiveness of the Pareto dominance relation for a high-dimensional space leads diversity maintenance mechanisms to play the leading role during the evolutionary process, while the preference of diversity maintenance mechanisms for individuals in sparse regions results in the final solutions distributed widely over the objective space but distant from the desired Pareto front. Intuitively, there are two ways to address this problem: 1) modifying the Pareto dominance relation and 2) modifying the diversity maintenance mechanism in the algorithm. In this paper, we focus on the latter and propose a shift-based density estimation (SDE) strategy. The aim of our study is to develop a general modification of density estimation in order to make Pareto-based algorithms suitable for many-objective optimization. In contrast to traditional density estimation that only involves the distribution of individuals in the population, SDE covers both the distribution and convergence information of individuals. The application of SDE in three popular Pareto-based algorithms demonstrates its usefulness in handling many-objective problems. Moreover, an extensive comparison with five state-of-the-art EMO algorithms reveals its competitiveness in balancing convergence and diversity of solutions. These findings not only show that SDE is a good alternative to tackle many-objective problems, but also present a general extension of Pareto-based algorithms in many-objective optimization.},
  citedate     = {2016.03.23},
  file         = {:PDF/Li2014b.pdf:PDF},
  keywords     = {Pareto optimisation;evolutionary computation;EMO algorithms;Pareto dominance relation;Pareto front;Pareto-based evolutionary multiobjective optimization algorithm;SDE strategy;diversity maintenance mechanisms;high-dimensional space;many-objective optimization;shift-based density estimation;sparse regions;Convergence;Evolutionary multiobjective optimization;convergence;diversity;evolutionary multiobjective optimization;many-objective optimization;manyobjective optimization;shift-based density estimation},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@InProceedings{Yu2007,
  author    = {Yu, Jia and Kirley, Michael and Buyya, Rajkumar},
  title     = {Multi-objective Planning for Workflow Execution on Grids},
  booktitle = {Proceedings of the 8\textsuperscript{th} IEEE/ACM International Conference on Grid Computing},
  year      = {2007},
  publisher = {IEEE},
  location  = {Washington},
  isbn      = {978-1-4244-1559-5},
  pages     = {10--17},
  doi       = {10.1109/GRID.2007.4354110},
  acmid     = {1513496},
  citedate  = {2016.03.23},
  file      = {:PDF/Yu2007.pdf:PDF},
  numpages  = {8},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Zhang2007,
  author       = {Qingfu Zhang and Hui Li},
  title        = {MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  year         = {2007},
  volume       = {11},
  number       = {6},
  month        = dec,
  pages        = {712--731},
  issn         = {1089-778X},
  doi          = {10.1109/TEVC.2007.892759},
  abstract     = {Decomposition is a basic strategy in traditional multiobjective optimization. However, it has not yet been widely used in multiobjective evolutionary optimization. This paper proposes a multiobjective evolutionary algorithm based on decomposition (MOEA/D). It decomposes a multiobjective optimization problem into a number of scalar optimization subproblems and optimizes them simultaneously. Each subproblem is optimized by only using information from its several neighboring subproblems, which makes MOEA/D have lower computational complexity at each generation than MOGLS and nondominated sorting genetic algorithm II (NSGA-II). Experimental results have demonstrated that MOEA/D with simple decomposition methods outperforms or performs similarly to MOGLS and NSGA-II on multiobjective 0-1 knapsack problems and continuous multiobjective optimization problems. It has been shown that MOEA/D using objective normalization can deal with disparately-scaled objectives, and MOEA/D with an advanced decomposition method can generate a set of very evenly distributed solutions for 3-objective test instances. The ability of MOEA/D with small population, the scalability and sensitivity of MOEA/D have also been experimentally investigated in this paper.},
  citedate     = {2016.03.23},
  file         = {:PDF/Zhang2007.pdf:PDF},
  keywords     = {computational complexity;genetic algorithms;computational complexity;decomposition;genetic algorithm;knapsack problem;multiobjective evolutionary algorithm;scalar optimization subproblem;Computational complexity;Pareto optimality;decomposition;evolutionary algorithm;multiobjective optimization},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@InProceedings{Zhang2011,
  author    = {Fan Zhang and Cao, Junwei and Kai Hwang and Cheng Wu},
  title     = {Ordinal Optimized Scheduling of Scientific Workflows in Elastic Compute Clouds},
  booktitle = {Proceedings of the 3\textsuperscript{rd} IEEE International Conference on Cloud Computing Technology and Science},
  year      = {2011},
  publisher = {IEEE},
  location  = {Athens, Greece},
  month     = nov,
  pages     = {9--17},
  doi       = {10.1109/CloudCom.2011.12},
  abstract  = {Elastic compute clouds are best represented by the virtual clusters in Amazon EC2 or in IBM RC2. This paper proposes a simulation based approach to scheduling scientific workflows onto elastic clouds. Scheduling multitask workflows in virtual clusters is a NP-hard problem. Excessive simulations in months of time may be needed to produce the optimal schedule using Monte Carlo simulations. To reduce this scheduling overhead is necessary in real-time cloud computing. We present a new workflow scheduling method based on iterative ordinal optimization (IOO). This new method outperforms the Monte Carlo and Blind-Pick methods to yield higher performance against rapid workflow variations. For example, to execute 20,000 tasks on 128 virtual machines for gravitational wave analysis, an ordinal optimized schedule can be generated in a few minutes, which is O(103)~O(104) faster than using Monte Carlo simulations. The ordinal optimized schedule results in higher throughput with lower memory demand. The cloud experimental results being reported verified our theoretical findings on the relative performance of three workflow scheduling methods studied in this paper.},
  citedate  = {2016.03.23},
  file      = {:PDF/Zhang2011.pdf:PDF},
  keywords  = {Monte Carlo methods;cloud computing;computational complexity;optimisation;scheduling;virtual machines;virtual reality;Amazon EC2;Blind-Pick method;IBM RC2;Monte Carlo simulation;NP-hard problem;elastic compute clouds;gravitational wave analysis;iterative ordinal optimization;memory demand;multitask workflow scheduling method;optimal scheduling;ordinal optimized scheduling;real time cloud computing;scientific workflow scheduling;simulation based approach;virtual cluster;virtual machine;Computational modeling;Monte Carlo methods;Optimal scheduling;Processor scheduling;Schedules;Servers;Cloud computing;ordinal optimization;virtual clustering;workflow scheduling},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Zhang2014a,
  author       = {Fan Zhang and Junwei Cao and Keqin Li and Samee U. Khan and Kai Hwang},
  title        = {Multi-objective scheduling of many tasks in cloud platforms},
  journaltitle = {Future Generation Computer Systems},
  year         = {2014},
  volume       = {37},
  number       = {0},
  pages        = {309--320},
  note         = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2013.09.006},
  abstract     = {Abstract The scheduling of a many-task workflow in a distributed computing platform is a well known NP-hard problem. The problem is even more complex and challenging when the virtualized clusters are used to execute a large number of tasks in a cloud computing platform. The difficulty lies in satisfying multiple objectives that may be of conflicting nature. For instance, it is difficult to minimize the makespan of many tasks, while reducing the resource cost and preserving the fault tolerance and/or the quality of service (QoS) at the same time. These conflicting requirements and goals are difficult to optimize due to the unknown runtime conditions, such as the availability of the resources and random workload distributions. Instead of taking a very long time to generate an optimal schedule, we propose a new method to generate suboptimal or sufficiently good schedules for smooth multitask workflows on cloud platforms. Our new multi-objective scheduling (MOS) scheme is specially tailored for clouds and based on the ordinal optimization (OO) method that was originally developed by the automation community for the design optimization of very complex dynamic systems. We extend the \{OO\} scheme to meet the special demands from cloud platforms that apply to virtual clusters of servers from multiple data centers. We prove the suboptimality through mathematical analysis. The major advantage of our \{MOS\} method lies in the significantly reduced scheduling overhead time and yet a close to optimal performance. Extensive experiments were carried out on virtual clusters with 16 to 128 virtual machines. The multitasking workflow is obtained from a real scientific \{LIGO\} workload for earth gravitational wave analysis. The experimental results show that our proposed algorithm rapidly and effectively generates a small set of semi-optimal scheduling solutions. On a 128-node virtual cluster, the method results in a thousand times of reduction in the search time for semi-optimal workflow schedules compared with the use of the Monte Carlo and the Blind Pick methods for the same purpose. },
  citedate     = {2016.03.23},
  file         = {:PDF/Zhang2014.pdf:PDF},
  keywords     = {Cloud computing;Many-task computing;Ordinal optimization;Performance evaluation;Virtual machines;Workflow scheduling },
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Article{Zheng2013a,
  author       = {Zheng, Wei and Sakellariou, Rizos},
  title        = {Budget-Deadline Constrained Workflow Planning for Admission Control},
  journaltitle = {Journal of Grid Computing},
  year         = {2013},
  language     = {English},
  volume       = {11},
  number       = {4},
  pages        = {633--651},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-013-9257-4},
  citedate     = {2016.03.23},
  file         = {:PDF/Zheng2013.pdf:PDF},
  keywords     = {Admission control; Bi-criteria DAG scheduling; SLA-based resource reservation; Workflow planning},
  owner        = {tefx},
  publisher    = {Springer},
  timestamp    = {2016.01.30},
}

@Article{Arabnejad2014,
  author       = {Arabnejad, Hamid and Barbosa, JorgeG.},
  title        = {A Budget Constrained Scheduling Algorithm for Workflow Applications},
  journaltitle = {Journal of Grid Computing},
  year         = {2014},
  language     = {English},
  volume       = {12},
  number       = {4},
  pages        = {665--679},
  issn         = {1570-7873},
  doi          = {10.1007/s10723-014-9294-7},
  citedate     = {2016.03.23},
  file         = {Arabnejad2014.pdf:Arabnejad2014 - A Budget Constrained Scheduling Algorithm for Workflow Applications.pdf:PDF},
  keywords     = {Utility computing; Deadline; Quality of Service; Planning Success Rate},
  owner        = {tefx},
  publisher    = {Springer},
  timestamp    = {2016.01.30},
}

@InCollection{Guo2013,
  author    = {Guo, Jian and Qian, Kun and Zhu, Zhaomeng and Zhang, Gongxuan and Xu, Huijie},
  title     = {A Cloud Computing System for Snore Signals Processing},
  booktitle = {Advanced Parallel Processing Technologies},
  year      = {2013},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  pages     = {359--366},
  doi       = {10.1007/978-3-642-45293-2_27},
  citedate  = {2016.03.23},
}

@InProceedings{Zhu2012,
  author    = {Mengxia Zhu and Qishi Wu and Yang Zhao},
  title     = {A cost-effective scheduling algorithm for scientific workflows in clouds},
  booktitle = {Performance Computing and Communications Conference (IPCCC), 2012 IEEE 31\textsuperscript{st} International},
  year      = {2012},
  publisher = {IEEE},
  location  = {Austin, Texas},
  month     = dec,
  pages     = {256--265},
  doi       = {10.1109/PCCC.2012.6407766},
  abstract  = {Cloud computing enables the delivery of computing, software, storage, and data access through web browsers as a metered service. In addition to commercial applications, an increasing number of large-scale workflow-based scientific applications are being supported by cloud computing. In order to meet the rapidly growing and dynamic computing demands of scientific users, the cloud service provider needs to employ efficient and cost-effective job schedulers to guarantee workflow completion time as well as improve resource utilization for high throughput. Based on rigorous cost models, we formulate a delay-constrained optimization problem to maximize resource utilization and propose a two-step workflow scheduling algorithm to minimize the cloud overhead within a user-specified execution time bound. The extensive simulation results illustrate that our approach consistently achieves lower computing overhead or higher resource utilization than existing methods within the execution time bound. Our approach also significantly reduces the total execution time by strategically selecting appropriate mapping nodes for prioritized modules.},
  citedate  = {2016.03.23},
  file      = {Zhu2012.pdf:Zhu2012 - A cost-effective scheduling algorithm for scientific workflows in clouds.pdf:PDF},
  issn      = {1097-2641},
  keywords  = {cloud computing;online front-ends;optimisation;resource allocation;scheduling;scientific information systems;Web browsers;cloud computing;cloud overhead minimization;cloud service provider;computing delivery;cost-effective job scheduling algorithm;data access;delay-constrained optimization problem;dynamic computing demands;large-scale workflow-based scientific applications;metered service;resource utilization improvement;scientific users;software delivery;total execution time reduction;two-step workflow scheduling algorithm;user-specified execution time bound;workflow completion time;Cloud computing;Computational modeling;Delay;Resource management;Scheduling algorithms;Virtual machining;Scientific workflow;cloud computing;workflow scheduling},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Deb2002,
  author       = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  title        = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  year         = {2002},
  volume       = {6},
  number       = {2},
  month        = apr,
  pages        = {182--197},
  issn         = {1089-778X},
  doi          = {10.1109/4235.996017},
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
  citedate     = {2016.03.23},
  file         = {Deb2002.pdf:Deb2002 - A fast and elitist multiobjective genetic algorithm_ NSGA-II.pdf:PDF},
  keywords     = {Pareto distribution;computational complexity;constraint theory;convergence;genetic algorithms;operations research;simulation;sorting;NSGA-II;Nondominated Sorting Genetic Algorithm II;Pareto-archived evolution strategy;Pareto-optimal front;algorithm performance;computational complexity;constrained multi-objective problems;constraint handling;convergence;dominance definition;fast elitist multi-objective genetic algorithm;mating pool;multi-criterion decision making;multi-objective evolutionary algorithm;multi-objective optimization;nondominated sharing;nonlinear problem;objectives;parent/offspring population combination;population size;selection operator;simulation;solution fitness;solution spread;strength-Pareto evolutionary algorithm;Associate members;Computational complexity;Computational modeling;Constraint optimization;Decision making;Diversity reception;Evolutionary computation;Genetic algorithms;Sorting;Testing},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@InCollection{Alt2005,
  author    = {Alt, Martin and Hoheisel, Andreas and Pohl, Hans-Werner and Gorlatch, Sergei},
  title     = {A grid workflow language using high-level petri nets},
  booktitle = {Parallel Processing and Applied Mathematics},
  year      = {2005},
  publisher = {Springer},
  location  = {Berlin Heidelberg},
  pages     = {715--722},
  doi       = {10.1007/11752578_86},
  citedate  = {2016.03.23},
}

@Article{Jorissen2012,
  author       = {K. Jorissen and F.D. Vila and J.J. Rehr},
  title        = {A high performance scientific cloud computing environment for materials simulations},
  journaltitle = {Computer Physics Communications},
  year         = {2012},
  volume       = {183},
  number       = {9},
  pages        = {1911--1919},
  issn         = {0010-4655},
  doi          = {10.1016/j.cpc.2012.04.010},
  abstract     = {We describe the development of a scientific cloud computing (SCC) platform that offers high performance computation capability. The platform consists of a scientific virtual machine prototype containing a \{UNIX\} operating system and several materials science codes, together with essential interface tools (an \{SCC\} toolset) that offers functionality comparable to local compute clusters. In particular, our \{SCC\} toolset provides automatic creation of virtual clusters for parallel computing, including tools for execution and monitoring performance, as well as efficient I/O utilities that enable seamless connections to and from the cloud. Our \{SCC\} platform is optimized for the Amazon Elastic Compute Cloud (EC2). We present benchmarks for prototypical scientific applications and demonstrate performance comparable to local compute clusters. To facilitate code execution and provide user-friendly access, we have also integrated cloud computing capability in a JAVA-based GUI. Our \{SCC\} platform may be an alternative to traditional \{HPC\} resources for materials science or quantum chemistry applications. },
  citedate     = {2016.03.23},
  keywords     = {Cloud computing},
  publisher    = {Elsevier},
}

@InProceedings{Fard2012,
  author    = {Fard, Hamid Mohammadi and Prodan, Radu and Barrionuevo, Juan Jose Durillo and Fahringer, Thomas},
  title     = {A Multi-objective Approach for Workflow Scheduling in Heterogeneous Environments},
  booktitle = {Proceedings of the 12\textsuperscript{th} IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  year      = {2012},
  series    = {CCGRID '12},
  publisher = {IEEE},
  location  = {Washington},
  isbn      = {978-0-7695-4691-9},
  pages     = {300--309},
  doi       = {10.1109/CCGrid.2012.114},
  acmid     = {2310188},
  citedate  = {2016.03.23},
  file      = {Fard2012.pdf:Fard2012 - A Multi-objective Approach for Workflow Scheduling in Heterogeneous Environments.pdf:PDF},
  keywords  = {computing systems, workflow scheduling, multi-objective optimization, Grids and Clouds},
  numpages  = {10},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@InProceedings{Udomkasemsub2012,
  author    = {Udomkasemsub, O. and Li Xiaorong and Achalakul, T.},
  title     = {A multiple-objective workflow scheduling framework for cloud data analytics},
  booktitle = {Computer Science and Software Engineering (JCSSE), 2012 International Joint Conference on},
  year      = {2012},
  publisher = {IEEE},
  location  = {Bangkok, Thailand},
  month     = may,
  pages     = {391--398},
  doi       = {10.1109/JCSSE.2012.6261985},
  abstract  = {One of the most important characteristics of a cloud system is elasticity in resources provisioning. Cloud fabric often composes of massive and heterogeneous types of resources allowing the sciences and engineering applications in many domains to collaboratively utilize the infrastructure. As the cloud systems are designed for a large number of users, a large volume of data, and various types of applications, efficient task management is needed for cloud data analytics. One of the popular methods used in task management is to represent a set of tasks with a workflow diagram, which can capture task decomposition, communication between subtasks, and cost of computation and communication. In this paper, we proposed a workflow scheduling framework that can efficiently schedule series workflows with multiple objectives onto a cloud system. Our designed framework uses a meta-heuristics method, called Artificial Bee Colony (ABC), to create an optimized scheduling plan. The framework allows multiple constraints and objectives to be set. Conflicts among objectives can also be resolved using Pareto-based technique. A series of experiments are then conducted to investigate the performance in comparison to the algorithms often used in cloud scheduling. Results show that our proposed method is able to reduce 57% cost and 50% scheduling time within a similar makespan of HEFT/LOSS for a typical scientific workflow like Chimera-2.},
  citedate  = {2016.03.23},
  file      = {Udomkasemsub2012.pdf:Udomkasemsub2012 - A multiple-objective workflow scheduling framework for cloud data analytics.pdf:PDF},
  keywords  = {Pareto optimisation;cloud computing;data analysis;resource allocation;scheduling;workflow management software;ABC;Chimera-2;HEFT/LOSS makespan;Pareto-based technique;artificial bee colony;cloud data analytics;cloud fabric;cloud scheduling;computation cost reduction;heterogeneous resources;meta-heuristics method;multiple-objective workflow scheduling framework;resource provisioning process;scheduling plan optimization;scheduling time reduction;scientific workflow;subtask communication cost reduction;task decomposition;task management;workflow diagram;Algorithm design and analysis;Equations;Genetic algorithms;Optimization;Processor scheduling;Schedules;Scheduling;Artificial Bee Colony;cloud computing;multiple-objective optimization;workflow scheduling},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@InProceedings{Li2003,
  author    = {Li, Xiaodong},
  title     = {A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective Optimization},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  year      = {2003},
  editor    = {Cant{\'u}-Paz, Erick and Foster, James A. and Deb, Kalyanmoy and Davis, Lawrence David and Roy, Rajkumar and O'Reilly, Una-May and Beyer, Hans-Georg and Standish, Russell and Kendall, Graham and Wilson, Stewart and Harman, Mark and Wegener, Joachim and Dasgupta, Dipankar and Potter, Mitch A. and Schultz, Alan C. and Dowsland, Kathryn A. and Jonoska, Natasha and Miller, Julian},
  language  = {English},
  volume    = {2723},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  location  = {Chicago},
  month     = jul,
  isbn      = {978-3-540-45105-1},
  pages     = {37--48},
  doi       = {10.1007/3-540-45105-6_4},
  citedate  = {2016.03.23},
  file      = {Li2003.pdf:Li2003 - A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective Optimization.pdf:PDF},
}

@InProceedings{Wen2012,
  author    = {Yiping Wen and Zhigang Chen and Tiemin Chen and Jianxun Liu and Guosheng Kang},
  title     = {A Particle Swarm Optimization Algorithm for Batch Processing Workflow Scheduling},
  booktitle = {Cloud and Green Computing (CGC), 2012 Second International Conference on},
  year      = {2012},
  publisher = {IEEE},
  location  = {Xiangtan, China},
  month     = nov,
  pages     = {645--649},
  doi       = {10.1109/CGC.2012.67},
  abstract  = {Aiming at shortcomings in existing scheduling methods for batch processing workflow, this paper attempt to investigate and solve the optimization problem for grouping and scheduling multiple activity instances in batch processing workflow. A multiple objective optimal model of problem with constraints is presented firstly. Then, a discrete particle swarm optimization algorithm is proposed to produce a set of optimal Pareto solutions by optimizing the two objective functions simultaneously. The result of simulation experiment shows the effectiveness of this algorithm.},
  citedate  = {2016.03.23},
  file      = {Wen2012.pdf:Wen2012 - A Particle Swarm Optimization Algorithm for Batch Processing Workflow Scheduling.pdf:PDF},
  keywords  = {Pareto optimisation;batch production systems;optimised production technology;particle swarm optimisation;scheduling;set theory;batch processing workflow scheduling;discrete particle swarm optimization algorithm;multiple activity instance grouping;multiple activity instance scheduling;multiple objective optimal model;objective function optimization;optimal Pareto solutions;Argon;Batch production systems;Dynamic scheduling;Heuristic algorithms;Job shop scheduling;Optimization;Particle swarm optimization;batch processing workflow;particle swarm optimization;scheduling},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@InProceedings{Tudoran2012,
  author    = {Tudoran, Radu and Costan, Alexandru and Antoniu, Gabriel and Boug{\'e}, Luc},
  title     = {A Performance Evaluation of Azure and Nimbus Clouds for Scientific Applications},
  booktitle = {Proceedings of the 2\textsuperscript{nd} International Workshop on Cloud Computing Platforms},
  year      = {2012},
  publisher = {ACM},
  location  = {Bern, Switzerland},
  isbn      = {978-1-4503-1161-8},
  pages     = {41--46},
  doi       = {10.1145/2168697.2168701},
  acmid     = {2168701},
  address   = {New York},
  articleno = {4},
  citedate  = {2016.03.23},
  numpages  = {6},
}

@Article{Lin2009,
  author       = {Lin, Cui and Lu, Shiyong and Fei, Xubo and Chebotko, Artem and Pai, Darshan and Lai, Zhaoqiang and Fotouhi, Farshad and Hua, Jing},
  title        = {A reference architecture for scientific workflow management systems and the VIEW SOA solution},
  journaltitle = {IEEE Transactions on Services Computing},
  year         = {2009},
  volume       = {2},
  number       = {1},
  pages        = {79--92},
  doi          = {10.1109/TSC.2009.4},
  citedate     = {2016.03.23},
  publisher    = {IEEE},
}

@Article{Liu2015,
  author       = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
  title        = {A survey of data-intensive scientific workflow management},
  journaltitle = {Journal of Grid Computing},
  year         = {2015},
  volume       = {13},
  number       = {4},
  pages        = {457--493},
  doi          = {10.1007/s10723-015-9329-8},
  citedate     = {2016.03.23},
  publisher    = {Springer},
}

@Article{Yu2005a,
  author       = {Yu, Jia and Buyya, Rajkumar},
  title        = {A Taxonomy of Scientific Workflow Systems for Grid Computing},
  journaltitle = {ACM SIGMOD Record},
  year         = {2005},
  volume       = {34},
  number       = {3},
  month        = sep,
  pages        = {44--49},
  issn         = {0163-5808},
  doi          = {10.1145/1084805.1084814},
  acmid        = {1084814},
  citedate     = {2016.03.23},
  file         = {Yu2005a.pdf:Yu2005a - A Taxonomy of Scientific Workflow Systems for Grid Computing.pdf:PDF;Yu2005a.pdf:PDF/Yu2005a.pdf:PDF},
  issue_date   = {September 2005},
  keywords     = {grid computing, scientific workflows, taxonomy},
  location     = {New York, NY, USA},
  numpages     = {6},
  publisher    = {ACM},
}

@Article{Fard2013,
  author       = {Fard, H.M. and Prodan, R. and Fahringer, T.},
  title        = {A Truthful Dynamic Workflow Scheduling Mechanism for Commercial Multicloud Environments},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  year         = {2013},
  volume       = {24},
  number       = {6},
  month        = jun,
  pages        = {1203--1212},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2012.257},
  abstract     = {The ultimate goal of cloud providers by providing resources is increasing their revenues. This goal leads to a selfish behavior that negatively affects the users of a commercial multicloud environment. In this paper, we introduce a pricing model and a truthful mechanism for scheduling single tasks considering two objectives: monetary cost and completion time. With respect to the social cost of the mechanism, i.e., minimizing the completion time and monetary cost, we extend the mechanism for dynamic scheduling of scientific workflows. We theoretically analyze the truthfulness and the efficiency of the mechanism and present extensive experimental results showing significant impact of the selfish behavior of the cloud providers on the efficiency of the whole system. The experiments conducted using real-world and synthetic workflow applications demonstrate that our solutions dominate in most cases the Pareto-optimal solutions estimated by two classical multiobjective evolutionary algorithms.},
  citedate     = {2016.03.23},
  file         = {Fard2013a.pdf:Fard2013 - A Truthful Dynamic Workflow Scheduling Mechanism for Commercial Multicloud Environments.pdf:PDF},
  keywords     = {Pareto optimisation;cloud computing;evolutionary computation;natural sciences computing;pricing;scheduling;workflow management software;Pareto-optimal solutions;classical multiobjective evolutionary algorithms;commercial multicloud environments;completion time minimization;monetary cost;pricing model;real-world workflow applications;scientific workflows;selfish behavior;single task scheduling;social cost;synthetic workflow applications;truthful dynamic workflow scheduling mechanism;Dynamic scheduling;Game theory;Games;Heuristic algorithms;Optimization;Processor scheduling;Workflow scheduling;game theory;multicloud environment;reverse auction;truthful mechanism},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Article{Armbrust2010,
  author       = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
  title        = {A View of Cloud Computing},
  journaltitle = {Communications of the ACM},
  year         = {2010},
  volume       = {53},
  number       = {4},
  month        = apr,
  pages        = {50--58},
  issn         = {0001-0782},
  doi          = {10.1145/1721654.1721672},
  acmid        = {1721672},
  citedate     = {2016.03.23},
  file         = {Armbrust2010.pdf:Armbrust2010 - A View of Cloud Computing.pdf:PDF},
  issue_date   = {April 2010},
  location     = {New York, NY, USA},
  numpages     = {9},
  owner        = {tefx},
  publisher    = {ACM},
  timestamp    = {2016.01.30},
}

@Article{Srirama2012,
  author       = {Satish Narayana Srirama and Pelle Jakovits and Eero Vainikko},
  title        = {Adapting scientific computing problems to clouds using MapReduce},
  journaltitle = {Future Generation Computer Systems},
  year         = {2012},
  volume       = {28},
  number       = {1},
  pages        = {184--192},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2011.05.025},
  abstract     = {Cloud computing, with its promise of virtually infinite resources, seems to suit well in solving resource greedy scientific computing problems. To study this, we established a scientific computing cloud (SciCloud) project and environment on our internal clusters. The main goal of the project is to study the scope of establishing private clouds at the universities. With these clouds, students and researchers can efficiently use the already existing resources of university computer networks, in solving computationally intensive scientific, mathematical, and academic problems. However, to be able to run the scientific computing applications on the cloud infrastructure, the applications must be reduced to frameworks that can successfully exploit the cloud resources, like the MapReduce framework. This paper summarizes the challenges associated with reducing iterative algorithms to the MapReduce model. Algorithms used by scientific computing are divided into different classes by how they can be adapted to the MapReduce model; examples from each such class are reduced to the MapReduce model and their performance is measured and analyzed. The study mainly focuses on the Hadoop MapReduce framework but also compares it to an alternative MapReduce framework called Twister, which is specifically designed for iterative algorithms. The analysis shows that Hadoop MapReduce has significant trouble with iterative problems while it suits well for embarrassingly parallel problems, and that Twister can handle iterative problems much more efficiently. This work shows how to adapt algorithms from each class into the MapReduce model, what affects the efficiency and scalability of algorithms in each class and allows us to judge which framework is more efficient for each of them, by mapping the advantages and disadvantages of the two frameworks. This study is of significant importance for scientific computing as it often uses complex iterative methods to solve critical problems and adapting such methods to cloud computing frameworks is not a trivial task. },
  citedate     = {2016.03.23},
  keywords     = {Scientific computing},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Article{Zhang2014,
  author       = {Yongping Zhang and Gongxuan Zhang and Zhaomeng Zhu},
  title        = {Algorithm Acceleration of Compressed Sensing with Cloud},
  journaltitle = {Informattion Technology Journal},
  year         = {2014},
  volume       = {13},
  number       = {2},
  pages        = {269--277},
  doi          = {10.3923/itj.2014.269.277},
  citedate     = {2016.03.23},
  owner        = {tefx},
  publisher    = {Asian Network for Scientific Information},
  timestamp    = {2016.01.30},
}

@Article{Malawski2015,
  author       = {Maciej Malawski and Gideon Juve and Ewa Deelman and Jarek Nabrzyski},
  title        = {Algorithms for cost- and deadline-constrained provisioning for scientific workflow ensembles in IaaS clouds},
  journaltitle = {Future Generation Computer Systems},
  year         = {2015},
  volume       = {48},
  pages        = {1--18},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2015.01.004},
  abstract     = {Abstract Large-scale applications expressed as scientific workflows are often grouped into ensembles of inter-related workflows. In this paper, we address a new and important problem concerning the efficient management of such ensembles under budget and deadline constraints on Infrastructure as a Service (IaaS) clouds. IaaS clouds are characterized by on-demand resource provisioning capabilities and a pay-per-use model. We discuss, develop, and assess novel algorithms based on static and dynamic strategies for both task scheduling and resource provisioning. We perform the evaluation via simulation using a set of scientific workflow ensembles with a broad range of budget and deadline parameters, taking into account task granularity, uncertainties in task runtime estimations, provisioning delays, and failures. We find that the key factor determining the performance of an algorithm is its ability to decide which workflows in an ensemble to admit or reject for execution. Our results show that an admission procedure based on workflow structure and estimates of task runtimes can significantly improve the quality of solutions. },
  citedate     = {2016.03.23},
  file         = {Malawski2015.pdf:Malawski2015 - Algorithms for cost- and deadline-constrained provisioning for scientific workflow ensembles in IaaS clouds.pdf:PDF},
  keywords     = {Scientific workflows},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Online{2016,
  title     = {Amazon EC2 FAQs - Amazon Web Service},
  year      = {2016},
  url       = {http://aws.amazon.com/ec2/faqs/#hardsware-information},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{Liu2014,
  author     = {Liu, Huan},
  title      = {Amazon EC2 grows 62\% in 2 years},
  year       = {2014},
  url        = {https://huanliu.wordpress.com/2014/02/26/amazon-ec2-grows-62-in-2-years/},
  month      = feb,
  bdsk-url-1 = {http://goo.gl/FgkxoR},
  citedate   = {2016.04.30},
  owner      = {tefx},
  timestamp  = {2015.03.18},
}

@Online{Amazon2016,
  author    = {Amazon},
  title     = {Amazon EFS - Cloud File Storage},
  year      = {2016},
  url       = {http://aws.amazon.com/efs/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016a,
  title     = {Amazon Web Service (AWS) - Cloud Computing Service},
  year      = {2016},
  url       = {http://aws.amazon.com/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{Rosoff2015,
  author    = {Martt Rosoff},
  title     = {Amazon's true brilliance shone this week in a tale of three clouds.},
  year      = {2015},
  url       = {http://www.businessinsider.com/amazon-built-an-enterprise-business-out-of-nothing-2015-4},
  month     = apr,
  citedate  = {2016.04.30},
  owner     = {tefx},
  publisher = {Business Insider},
  timestamp = {2015.05.18},
}

@Article{Chen2009,
  author       = {Wei-Neng Chen and Jun Zhang},
  title        = {An Ant Colony Optimization Approach to a Grid Workflow Scheduling Problem With Various {QoS} Requirements},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews},
  year         = {2009},
  volume       = {39},
  number       = {1},
  month        = jan,
  pages        = {29--43},
  issn         = {1094-6977},
  doi          = {10.1109/TSMCC.2008.2001722},
  abstract     = {Grid computing is increasingly considered as a promising next-generation computational platform that supports wide-area parallel and distributed computing. In grid environments, applications are always regarded as workflows. The problem of scheduling workflows in terms of certain quality of service (QoS) requirements is challenging and it significantly influences the performance of grids. By now, there have been some algorithms for grid workflow scheduling, but most of them can only tackle the problems with a single QoS parameter or with small-scale workflows. In this frame, this paper aims at proposing an ant colony optimization (ACO) algorithm to schedule large-scale workflows with various QoS parameters. This algorithm enables users to specify their QoS preferences as well as define the minimum QoS thresholds for a certain application. The objective of this algorithm is to find a solution that meets all QoS constraints and optimizes the user-preferred QoS parameter. Based on the characteristics of workflow scheduling, we design seven new heuristics for the ACO approach and propose an adaptive scheme that allows artificial ants to select heuristics based on pheromone values. Experiments are done in ten workflow applications with at most 120 tasks, and the results demonstrate the effectiveness of the proposed algorithm.},
  citedate     = {2016.03.23},
  file         = {Chen2009.pdf:Chen2009 - An Ant Colony Optimization Approach to a Grid Workflow Scheduling Problem With Various QoS Requirements.pdf:PDF},
  keywords     = {grid computing;optimisation;quality of service;scheduling;QoS;ant colony optimization;grid computing;grid workflow scheduling problem;quality of service;wide-area distributed computing;wide-area parallel computing;Ant colony optimization (ACO);grid computing;workflow scheduling},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Article{Heroux2005,
  author       = {Heroux, Michael A. and Bartlett, Roscoe A. and Howle, Vicki E. and Hoekstra, Robert J. and Hu, Jonathan J. and Kolda, Tamara G. and Lehoucq, Richard B. and Long, Kevin R. and Pawlowski, Roger P. and Phipps, Eric T. and Salinger, Andrew G. and Thornquist, Heidi K. and Tuminaro, Ray S. and Willenbring, James M. and Williams, Alan and Stanley, Kendall S.},
  title        = {An Overview of the Trilinos Project},
  journaltitle = {ACM Transactions on Mathematical Software},
  year         = {2005},
  volume       = {31},
  number       = {3},
  month        = sep,
  pages        = {397--423},
  issn         = {0098-3500},
  doi          = {10.1145/1089014.1089021},
  acmid        = {1089021},
  citedate     = {2016.03.23},
  issue_date   = {September 2005},
  keywords     = {Software Quality Engineering, Software framework, interfaces},
  numpages     = {27},
  publisher    = {ACM},
}

@InProceedings{Garcia2013,
  author    = {Garc{\i}a, {\'A}lvaro L{\'o}pez and del Castillo, Enol Fern{\'a}ndez},
  title     = {Analysis of Scientific Cloud Computing requirements},
  booktitle = {Proceedings of the 7\textsuperscript{th} IBERIAN Grid Infrastructure Conference},
  year      = {2013},
  publisher = {Editorial Universitat Politècnica de València},
  location  = {Madrid, Spain},
  pages     = {147},
  url       = {http://arxiv.org/abs/1309.6109},
  citedate  = {2016.03.23},
}

@InProceedings{Fahringer2005,
  author    = {Fahringer, T. and Prodan, R. and Duan, Rubing and Nerieri, F. and Podlipnig, S. and Qin, Jun and Siddiqui, M. and Truong, Hong-Linh and Villazon, A. and Wieczorek, M.},
  title     = {ASKALON: A Grid Application Development and Computing Environment},
  booktitle = {Proceedings of the 6\textsuperscript{th} IEEE/ACM International Workshop on Grid Computing},
  year      = {2005},
  series    = {GRID '05},
  publisher = {IEEE},
  location  = {Washington},
  isbn      = {0-7803-9492-5},
  pages     = {122--131},
  doi       = {10.1109/GRID.2005.1542733},
  acmid     = {1253487},
  citedate  = {2016.03.23},
  file      = {Fahringer2005.pdf:Fahringer2005 - ASKALON_ A Grid Application Development and Computing Environment.pdf:PDF},
  numpages  = {10},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Online{2016b,
  title     = {AWS | High Performance Computing - HPC Cloud Computing},
  year      = {2016},
  url       = {https://aws.amazon.com/hpc/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Online{2016d,
  title     = {Big Compute: {HPC} \& Batch | Microsoft Azure},
  year      = {2016},
  url       = {https://azure.microsoft.com/en-us/solutions/big-compute/},
  citedate  = {2016.04.30},
  owner     = {tefx},
  timestamp = {2016.03.04},
}

@Article{Marx2013,
  author       = {Marx, Vivien},
  title        = {Biology: The big challenges of big data},
  journaltitle = {Nature},
  year         = {2013},
  volume       = {498},
  number       = {7453},
  pages        = {255--260},
  doi          = {10.1038/498255a},
  citedate     = {2016.03.23},
  publisher    = {Nature Publishing Group},
}

@InProceedings{Fard2013a,
  author    = {Fard, H.M. and Fahringer, T. and Prodan, R.},
  title     = {Budget-Constrained Resource Provisioning for Scientific Applications in Clouds},
  booktitle = {Proceedings of the 5\textsuperscript{th} IEEE International Conference on Cloud Computing Technology and Science},
  year      = {2013},
  volume    = {1},
  publisher = {IEEE},
  location  = {Santa Clara, CA},
  month     = dec,
  pages     = {315--322},
  doi       = {10.1109/CloudCom.2013.48},
  abstract  = {Public commercial clouds emerged as new and attractive resource provisioning option for scientific computing. This new alternative raises new challenges for users of such clouds, since optimizing the completion time of scientific applications might substantially increase the monetary cost of leasing cloud resources. In this paper, we first propose a set of basic rescheduling operations covering a broad set of scenarios for reducing the costs of running scientific workflows in clouds. Based on them, we design two heuristic scheduling algorithms. The first algorithm aims at reducing the cost of resource provisioning while still attaining the optimal make span. The second algorithm further reduces the costs to meet a budget constraint with a small increase in the make span. The experiments conducted using real-world and synthetic workflow applications demonstrate important benefits compared to related state-of-the-art approaches.},
  citedate  = {2016.03.23},
  file      = {Fard2013.pdf:Fard2013a - Budget-Constrained Resource Provisioning for Scientific Applications in Clouds.pdf:PDF},
  keywords  = {budgeting;cloud computing;optimisation;resource allocation;scheduling;budget constraint;budget-constrained resource provisioning;completion time optimization;heuristic scheduling algorithms;monetary cost;optimal makespan;public commercial cloud resources;real-world workflow applications;rescheduling operations;resource provisioning cost reduction;scientific computing;scientific workflows;synthetic workflow applications;Algorithm design and analysis;Booting;Cloud computing;Delays;Optimization;Schedules;Scheduling;cloud computing;makespan;monetary cost;resource provisioning;scheduling;scientific workflows},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Electronic{Google2014,
  author     = {Google},
  title      = {Google Compute Engine Pricing},
  year       = {2014},
  url        = {http://goo.gl/fKQwzb},
  bdsk-url-1 = {http://goo.gl/fKQwzb},
  citedate   = {2016.04.30},
  owner      = {tefx},
  timestamp  = {2016.03.04},
}

@Article{Garg2014,
  author       = {Garg, Ritu and Singh, AwadheshKumar},
  title        = {Multi-objective workflow grid scheduling using $\varepsilon$-fuzzy dominance sort based discrete particle swarm optimization},
  journaltitle = {J. Supercomputing},
  year         = {2014},
  language     = {English},
  volume       = {68},
  number       = {2},
  pages        = {709--732},
  issn         = {0920-8542},
  doi          = {10.1007/s11227-013-1059-8},
  citedate     = {2016.03.23},
  file         = {Garg2014.pdf:Garg2014 - Multi-objective workflow grid scheduling using $_varepsilon$-fuzzy dominance sort based discrete particle swarm optimization.pdf:PDF},
  keywords     = {Multi-objective optimization; DAG; Grid computing; Particle swarm optimization; Fuzzy dominance},
  owner        = {tefx},
  publisher    = {Springer US},
  timestamp    = {2016.01.30},
}

@InProceedings{Durillo2013,
  author    = {Durillo, J.J. and Nae, V. and Prodan, R.},
  title     = {Multi-objective Workflow Scheduling: An Analysis of the Energy Efficiency and Makespan Tradeoff},
  booktitle = {Cluster, Cloud and Grid Computing (CCGrid), 2013 13\textsuperscript{th} IEEE/ACM International Symposium on},
  year      = {2013},
  publisher = {IEEE},
  location  = {Delft, the Netherlands},
  month     = may,
  pages     = {203--210},
  doi       = {10.1109/CCGrid.2013.62},
  abstract  = {While in the past scheduling algorithms were almost exclusively targeted at optimizing applications' make span, today they must simultaneously optimise several goals. Among these goals, energy efficiency is receiving increasing attention for environmental and financial reasons. In contrast to related work that optimises energy consumption as a single objective function, we reformulate in this paper the problem as a bi-objective optimisation by considering both make span and energy as goals. We study the potential benefits of using a Pareto-based workflow scheduling algorithm called MOHEFT using realistic energy consumption and performance models for task executions. We analyse the tradeoff solutions computed by MOHET for different workflows (different in shapes and sizes) in different execution scenarios (different resources in terms of energy consumption). The obtained results show that our bi-objective approach found in some cases schedules that reduce the energy consumption up to 85% with only 3.3% of make span concessions.},
  citedate  = {2016.03.23},
  file      = {Durillo2013.pdf:Durillo2013 - Multi-objective Workflow Scheduling_ An Analysis of the Energy Efficiency and Makespan Tradeoff.pdf:PDF},
  keywords  = {Pareto optimisation;environmental factors;natural sciences computing;power aware computing;scheduling;MOHEFT;Pareto-based workflow scheduling algorithm;application makespan optimization;biobjective optimisation;energy efficiency;environmental reasons;financial reasons;makespan concessions;makespan tradeoff;multiobjective workflow scheduling;Computational modeling;Energy consumption;Energy measurement;Multicore processing;Optimization;Schedules;Shape},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@Article{Durillo2015,
  author       = {Juan J. Durillo and Radu Prodan and Jorge G. Barbosa},
  title        = {Pareto tradeoff scheduling of workflows on federated commercial Clouds},
  journaltitle = {Simulation Modelling Practice and Theory},
  year         = {2015},
  volume       = {58},
  pages        = {95--111},
  issn         = {1569-190X},
  doi          = {10.1016/j.simpat.2015.07.001},
  abstract     = {Abstract As distributed computing infrastructures become nowadays ever more complex and heterogeneous, scientists are confronted with multiple competing goals such as makespan in high-performance computing and economic cost in Clouds. Existing approaches typically aim at finding a single tradeoff solution by aggregating or constraining the objectives in an a-priory fashion, which negatively impacts the quality of the solutions. In contrast, Pareto-based approaches aiming to approximate the complete set of (nearly-) optimal tradeoff solutions have been scarcely studied. In this paper, we extend the popular Heterogeneous Earliest Finish Time (HEFT) workflow scheduling heuristic for dealing with multiple conflicting objectives and approximating the Pareto frontier optimal schedules. We evaluate our new algorithm for performance and cost tradeoff optimisation of synthetic and real-world applications in Distributed Computing Infrastructures (DCIs) and federated Clouds and compare it with a state-of-the-art meta-heuristic from the multi-objective optimisation community.},
  citedate     = {2016.03.23},
  keywords     = {Scheduling},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Article{Deelman2005,
  author       = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G Bruce and Good, John and others},
  title        = {Pegasus: A framework for mapping complex scientific workflows onto distributed systems},
  journaltitle = {Scientific Programming},
  year         = {2005},
  volume       = {13},
  number       = {3},
  pages        = {219--237},
  doi          = {10.1155/2005/128026},
  citedate     = {2016.03.23},
  file         = {Deelman2005.pdf:Deelman2005 - Pegasus_ A framework for mapping complex scientific workflows onto distributed systems.pdf:PDF},
  owner        = {tefx},
  publisher    = {Hindawi Publishing Corporation},
  timestamp    = {2016.01.30},
}

@Article{Iosup2011,
  author       = {Iosup, A. and Ostermann, S. and Yigitbasi, M.N. and Prodan, R. and Fahringer, T. and Epema, D.H.J.},
  title        = {Performance Analysis of Cloud Computing Services for Many-Tasks Scientific Computing},
  journaltitle = {IEEE Trans. Parallel and Distributed Systems},
  year         = {2011},
  volume       = {22},
  number       = {6},
  month        = jun,
  pages        = {931--945},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2011.66},
  abstract     = {Cloud computing is an emerging commercial infrastructure paradigm that promises to eliminate the need for maintaining expensive computing facilities by companies and institutes alike. Through the use of virtualization and resource time sharing, clouds serve with a single set of physical resources a large user base with different needs. Thus, clouds have the potential to provide to their owners the benefits of an economy of scale and, at the same time, become an alternative for scientists to clusters, grids, and parallel production environments. However, the current commercial clouds have been built to support web and small database workloads, which are very different from typical scientific computing workloads. Moreover, the use of virtualization and resource time sharing may introduce significant performance penalties for the demanding scientific computing workloads. In this work, we analyze the performance of cloud computing services for scientific computing workloads. We quantify the presence in real scientific computing workloads of Many-Task Computing (MTC) users, that is, of users who employ loosely coupled applications comprising many tasks to achieve their scientific goals. Then, we perform an empirical evaluation of the performance of four commercial cloud computing services including Amazon EC2, which is currently the largest commercial cloud. Last, we compare through trace-based simulation the performance characteristics and cost models of clouds and other scientific computing platforms, for general and MTC-based scientific computing workloads. Our results indicate that the current clouds need an order of magnitude in performance improvement to be useful to the scientific community, and show which improvements should be considered first to address this discrepancy between offer and demand.},
  citedate     = {2016.03.23},
  file         = {Iosup2011.pdf:Iosup2011 - Performance Analysis of Cloud Computing Services for Many-Tasks Scientific Computing.pdf:PDF},
  keywords     = {cloud computing;software performance evaluation;task analysis;Amazon EC2;cloud computing services;clouds cost model;loosely coupled application;many tasks scientific computing;performance analysis;real scientific computing workload;trace based simulation;Cloud computing;Computational modeling;Kernel;Performance evaluation;Production;Supercomputers;Distributed systems;distributed applications;metrics/measurement;performance evaluation;performance measures.},
  publisher    = {IEEE},
}

@Article{Topcuoglu2002,
  author       = {Topcuoglu, H. and Hariri, S. and Min-You Wu},
  title        = {Performance-effective and low-complexity task scheduling for heterogeneous computing},
  journaltitle = {IEEE Trans. Parallel and Distributed Systems},
  year         = {2002},
  volume       = {13},
  number       = {3},
  month        = mar,
  pages        = {260--274},
  issn         = {1045-9219},
  doi          = {10.1109/71.993206},
  abstract     = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics},
  citedate     = {2016.03.23},
  file         = {Topcuoglu2002.pdf:Topcuoglu2002 - Performance-effective and low-complexity task scheduling for heterogeneous computing.pdf:PDF},
  keywords     = {directed graphs;processor scheduling;workstation clusters;Critical-Path-on-a-Processor algorithm;DAG scheduling;Heterogeneous Earliest-Finish-Time algorithm;application scheduling problem;heterogeneous computing environments;list scheduling;parametric graph generator;scheduling costs;task graphs;time metrics;weighted directed acyclic graphs;Processor scheduling},
  owner        = {tefx},
  publisher    = {IEEE},
  timestamp    = {2016.01.30},
}

@Online{Balay2001,
  author   = {Balay, Satish and Buschelman, Kris and Gropp, William D and Kaushik, Dinesh and Knepley, Matthew G and McInnes, Lois Curfman and Smith, Barry F and Zhang, Hong},
  title    = {Portable, Extensible Toolkit for Scientific Computation},
  year     = {2001},
  url      = {http://www.mcs.anl.gov/petsc},
  citedate = {2016.04.30},
}

@Article{Kianpisheh2015,
  author       = {Somayeh Kianpisheh and Nasrolah Moghadam Charkari and Mehdi Kargahi},
  title        = {Reliability-driven scheduling of time/cost-constrained grid workflows},
  journaltitle = {Future Generation Computer Systems},
  year         = {2015},
  volume       = {55},
  pages        = {1--16},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2015.07.014},
  abstract     = {Abstract Workflow scheduling in Grids and Clouds is a NP-Hard problem. Constrained workflow scheduling, arisen in recent years, provides the description of the user requirements through defining constraints on factors like makespan and cost. This paper proposes a scheduling algorithm to maximize the workflow execution reliability while respecting the user-defined deadline and budget. We have used ant colony system to minimize an aggregation of reliability and constraints violation. Three novel heuristics have been proposed which are adaptively selected by ants. Two of them are employed to find feasible schedules and the other is used to enhance the reliability. Two methods have been investigated for time and cost considerations in the resource selection. One of them assigns equal importance to the time and cost factors, and the other weighs them according to the tightness of satisfaction of the corresponding constraints. Simulation results demonstrate the effectiveness of the proposed algorithm in finding feasible schedules with high reliability. As it is shown, as an additional achievement, the Grid profit loss has been decreased. },
  citedate     = {2016.03.23},
  keywords     = {Workflow scheduling},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2016.01.30},
}

@Article{Wu2015,
  author       = {Wu, H. and Hua, X. and Li, Z. and Ren, S.},
  title        = {Resource and Instance Hour Minimization for Deadline Constrained DAG Applications Using Computer Clouds},
  journaltitle = {IEEE Trans. Parallel and Distributed Systems},
  year         = {2015},
  volume       = {PP},
  number       = {99},
  month        = mar,
  pages        = {1--1},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2015.2411257},
  abstract     = {n this paper, we address the resource and virtual machine instance hour minimization problem for directed-acyclic-graph-based deadline constrained applications deployed on computer clouds. The allocated resources and instance hours on computer clouds must: (1) guarantee the satisfaction of a deadline constrained application’s end-to-end deadline; (2) ensure that the number of virtual machine (VM) instances allocated to the application is minimized; (3) under the allocated number of VM instances, determine application execution schedule that minimizes the application’s makespan; and (4) under the decided application execution schedule, determine a VM operation schedule, i.e., when a VM should be turned on or off, that minimizes total VM instance hours needed to execute the application. We first give lower and upper bounds for the number of VM instances needed to guarantee the satisfaction of a deadline constrained application’s end-to-end deadline. Based on the bounds, we develop a heuristic algorithm called minimal slack time and minimal distance (MSMD) algorithm that finds the minimum number of VM instances needed to guarantee the application’s deadline and schedules tasks on the allocated VM instances so that the application’s makespan is minimized. Once the application execution schedule and the number of VM instances needed are determined, the proposed VM instance hour minimization (IHM) algorithm is applied to further reduce the instance hours needed by VMs to complete the application’s execution. Our experimental results show that the MSMD algorithm can guarantee applications’ end-to-end deadlines with less resources than the HEFT [32], MOHEFT [16], DBUS [9], QoS-base [40] and Auto-Scaling [25] heuristic scheduling algorithms in the literature. Furthermore, under allocated resources, the MSMD algorithm can, on average, reduce an application’s makespan by 3.4% of its deadline. In addition, with th- IHM algorithm we can effectively reduce the application’s execution instance hours compared with when IHM is not applied.},
  citedate     = {2016.03.23},
  file         = {Wu2015.pdf:Wu2015 - Resource and Instance Hour Minimization for Deadline Constrained DAG Applications Using Computer Clouds.pdf:PDF},
  keywords     = {Cloud computing;Computers;Heuristic algorithms;Minimization;Processor scheduling;Schedules;Virtual machining;Cloud;Cost minimization;Instance Hour Minimization;MSMD;Makespan minimization;Real-time;Resource Minimization;Scheduling},
  publisher    = {IEEE},
}

@InProceedings{Juve2009,
  author    = {Juve, G. and Deelman, E. and Vahi, K. and Mehta, G. and Berriman, B. and Berman, B.P. and Maechling, P.},
  title     = {Scientific workflow applications on Amazon EC2},
  booktitle = {E-Science Workshops, 2009 5\textsuperscript{th} IEEE International Conference on},
  year      = {2009},
  publisher = {IEEE},
  location  = {Oxford},
  month     = dec,
  pages     = {59--66},
  doi       = {10.1109/ESCIW.2009.5408002},
  abstract  = {The proliferation of commercial cloud computing providers has generated significant interest in the scientific computing community. Much recent research has attempted to determine the benefits and drawbacks of cloud computing for scientific applications. Although clouds have many attractive features, such as virtualization, on-demand provisioning, and Â¿pay as you goÂ¿ usage-based pricing, it is not clear whether they are able to deliver the performance required for scientific applications at a reasonable price. In this paper we examine the performance and cost of clouds from the perspective of scientific workflow applications. We use three characteristic workflows to compare the performance of a commercial cloud with that of a typical HPC system, and we analyze the various costs associated with running those workflows in the cloud. We find that the performance of clouds is not unreasonable given the hardware resources provided, and that performance comparable to HPC systems can be achieved given similar resources. We also find that the cost of running workflows on a commercial cloud can be reduced by storing data in the cloud rather than transferring it from outside.},
  citedate  = {2016.03.23},
  keywords  = {Internet;ubiquitous computing;Amazon EC2;HPC system;commercial cloud computing;scientific workflow application;Application software;Application virtualization;Cloud computing;Costs;Grid computing;Open source software;Performance analysis;Processor scheduling;Resource management;Scientific computing},
  owner     = {tefx},
  timestamp = {2016.01.30},
}

@TechReport{Zitzler2001,
  author      = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar and Zitzler, Eckart and Zitzler, Eckart and Thiele, Lothar and Thiele, Lothar},
  title       = {SPEA2: Improving the strength Pareto evolutionary algorithm},
  institution = {Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH), Institut f{\"u}r Technische Informatik und Kommunikationsnetze (TIK)},
  year        = {2001},
  citedate    = {2016.03.23},
  file        = {Zitzler2001.pdf:Zitzler2001 - SPEA2_ Improving the strength Pareto evolutionary algorithm.pdf:PDF},
  owner       = {tefx},
  publisher   = {{\ldots} Engineering and Networks Laboratory (TIK)},
  timestamp   = {2015.05.13},
}

@Article{Bochenina2016,
  author       = {Klavdiya Bochenina and Nikolay Butakov and Alexander Boukhanovsky},
  title        = {Static scheduling of multiple workflows with soft deadlines in non-dedicated heterogeneous environments},
  journaltitle = {Future Generation Computer Systems},
  year         = {2016},
  volume       = {55},
  pages        = {51--61},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2015.08.009},
  abstract     = {Abstract Typical patterns of using scientific workflows include their periodical executions using a fixed set of computational resources. Using the statistics from multiple runs, one can accurately estimate task execution and communication times to apply static scheduling algorithms. Several workflows with known estimates could be combined into a set to improve the resulting schedule. In this paper, we consider the mapping of multiple workflows to partially available heterogeneous resources. The problem is how to fill free time windows with tasks from different workflows, taking into account users’ requirements of the urgency of the results of calculations. To estimate quality of schedules for several workflows with various soft deadlines, we introduce the unified metric incorporating levels of meeting constraints and fairness of resource distribution. The main goal of the work was to develop a set of algorithms implementing different scheduling strategies for multiple workflows with soft deadlines in a non-dedicated environment, and to perform a comparative analysis of these strategies. We study how time restrictions (given by resource providers and users) influence the quality of schedules, and which scheme of grouping and ordering the tasks is the most effective for the batched scheduling of non-urgent workflows. Experiments with several types of synthetic and domain-specific sets of multiple workflows show that: (i) the use of information about time windows and deadlines leads to the significant increase of the quality of static schedules, (ii) the clustering-based scheduling scheme outperforms task-based and workflow-based schemes. This was confirmed by an evaluation of studied algorithms on a basis of the \{CLAVIRE\} workflow management platform. },
  citedate     = {2016.03.23},
  keywords     = {Workflow scheduling},
  owner        = {tefx},
  publisher    = {Elsevier},
  timestamp    = {2015.10.10},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectory:/home/tefx/Desktop/Papers/workflow/PDF;}

@Comment{jabref-meta: fileDirectory-tefx-tefx-XPS:/home/tefx/Desktop/Papers/workflow/PDF;}
